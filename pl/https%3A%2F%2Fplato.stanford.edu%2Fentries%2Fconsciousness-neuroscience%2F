<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html lang="en"> <!-- <![endif]-->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
The Neuroscience of Consciousness (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="The Neuroscience of Consciousness" />
<meta property="citation_author" content="Wu, Wayne" />
<meta property="citation_author" content="Morales, Jorge" />
<meta property="citation_publication_date" content="2018/10/09" />
<meta name="DC.title" content="The Neuroscience of Consciousness" />
<meta name="DC.creator" content="Wu, Wayne" />
<meta name="DC.creator" content="Morales, Jorge" />
<meta name="DCTERMS.issued" content="2018-10-09" />
<meta name="DCTERMS.modified" content="2024-04-03" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP home page" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
                    <li role="menuitem"><a href="../../new.html">What's New</a></li>
                    <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li role="menuitem"><a href="../../published.html">Chronological</a></li>
                    <li role="menuitem"><a href="../../archives/">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
                    <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
                    <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
                    <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
                    <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
                    <li role="menuitem"><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
                    <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
                    <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="../../search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit" aria-label="search"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/consciousness-neuroscience/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=consciousness-neuroscience">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>The Neuroscience of Consciousness</h1><div id="pubinfo"><em>First published Tue Oct 9, 2018; substantive revision Wed Apr 3, 2024</em></div>

<div id="preamble">

<p>
Conscious experience in humans depends on brain activity, so
neuroscience will contribute to explaining consciousness. What would
it be for neuroscience to explain consciousness? How much progress has
neuroscience made in doing so? What challenges does it face? How can
it meet those challenges? What is the philosophical significance of
its findings? This entry addresses these and related questions.</p>

<p>
To bridge the gulf between brain and consciousness, we need neural
data, computational and psychological models, and philosophical
analysis to identify principles to connect brain activity to conscious
experience in an illuminating way. This entry will focus on
identifying such principles without shying away from the neural
details. The theories and data to be considered will be organized
around constructing answers to two questions (see
 <a href="#GeneSpecCons">section 1.4</a>
 for more precise formulations):</p>

<ul>

<li>Generic Consciousness: How might neural properties explain when a
state is conscious rather than not?</li>

<li>Specific Consciousness: How might neural properties explain what
the content of a conscious state is?</li>
</ul>

<p>
A challenge for an objective science of consciousness is to dissect an
essentially subjective phenomenon. As investigators cannot experience
another subject&rsquo;s conscious states, they rely on the
subject&rsquo;s observable behavior to track consciousness. Priority
is given to a subject&rsquo;s introspective reports as these express
the subject&rsquo;s take on her experience. Introspection thus
provides a fundamental way, perhaps <em>the</em> fundamental way, to
track consciousness. That said, consciousness pervasively influences
human behavior and affects physiological responses, so other forms of
behavior and physiological data beyond introspective reports provide a
window on consciousness. How to leverage disparate evidence is a
central issue.</p>

<p>
The term &ldquo;neuroscience&rdquo; covers those scientific fields
whose explanations advert to the properties of neurons, populations of
neurons, or larger parts of the nervous
 system.<sup>[<a href="notes.html#note-1" id="ref-1">1</a>]</sup>
 This includes, but is not limited to, psychologists&rsquo; and
cognitive neuroscientists&rsquo; use of various neuroimaging methods
to monitor the activity of tens of millions of neurons, computational
theorists&rsquo; modeling of biological and artificial neural
networks, neuroscientists&rsquo; use of electrodes inserted into brain
tissue to record neural activity from individual or populations of
neurons, and clinicians&rsquo; study of patients with altered
conscious experiences in light of damage to brain areas. Given the
breadth of neuroscience so conceived, this review focuses mostly on
cortical activity that sustains perceptual consciousness, with
emphasis on vision. This is not because visual consciousness is more
important than other forms of consciousness. Rather, the level of
detail in empirical work on vision often speaks more comprehensively
to the issues that we shall confront.</p>

<p>
That said, there are many forms of consciousness that we will not
discuss. Some are covered in other entries such as split-brain
phenomena (see the entry on
 <a href="../consciousness-unity/">the unity of consciousness</a>,
 section 4.1.1), animal consciousness (see the entry on
 <a href="../consciousness-animal/">animal consciousness</a>),
 and neural correlates of the will and agency (see the entry on
 <a href="../agency/">agency</a>,
 section 5). In addition, this entry will not discuss the neuroscience
of consciousness in audition, olfaction, or gustation; disturbed
consciousness in mental disorders such as schizophrenia; conscious
aspects of pleasure, pain and the emotions; the phenomenology of
thought; the neural basis of dreams; and modulations of consciousness
during sleep and anesthesia among other issues. These are important
topics, and the principles and approaches highlighted in this
discussion will apply to many of these domains.</p>
</div> 

<div id="toc"><!--Entry Contents-->
<ul>
	<li><a href="#Fund">1. Fundamentals</a>

	<ul>
		<li><a href="#MapBrai">1.1 A Map of the Brain</a></li>
		<li><a href="#NeurBrai">1.2 Neurons and Brain</a></li>
		<li><a href="#AcceConsPhenCons">1.3 Access Consciousness and Phenomenal Consciousness</a></li>
		<li><a href="#GeneSpecCons">1.4 Generic and Specific Consciousness</a></li>
		<li><a href="#HardProb">1.5 The Hard Problem</a></li>
		<li><a href="#NeurCorrCons">1.6 Neural Correlates of Consciousness</a></li>
	</ul>
	</li>
	<li><a href="#MethForTracCons">2. Methods for Tracking Consciousness</a>
	<ul>
		<li><a href="#IntrRepo">2.1 Introspection and Report</a></li>
		<li><a href="#AcceConNoRepoPara">2.2 Access Consciousness and No-Report Paradigms</a></li>
		<li><a href="#ConfMetaAppr">2.3 Confidence and Metacognitive Approaches</a></li>
		<li><a href="#InteActiInfe">2.4 The Intentional Action Inference</a></li>
		<li><a href="#UnreWakeSynInteActiInfe">2.5 Unresponsive Wakefulness Syndrome and the Intentional Action Inference</a></li>
	</ul>
	</li>
	<li><a href="#NeurTheoCons">3. Neurobiological Theories of Consciousness</a>
	<ul>
		<li><a href="#GlobNeurWork">3.1 The Global Neuronal Workspace</a></li>
		<li><a href="#RecuProcTheo">3.2 Recurrent Processing Theory</a></li>
		<li><a href="#HighOrdeTheo">3.3 Higher-Order Theory</a></li>
		<li><a href="#InfoInteTheo">3.4 Information Integration Theory</a></li>
		<li><a href="#FronPost">3.5 Frontal or Posterior?</a></li>
	</ul>
	</li>
	<li><a href="#NeurGeneConsUncoVisiCaseStud">4. Neuroscience of Generic Consciousness: Unconscious Vision as Case Study</a>
	<ul>
		<li><a href="#UncoVisiTwoVisuStre">4.1 Unconscious Vision and the Two Visual Streams</a></li>
		<li><a href="#Blin">4.2 Blindsight</a></li>
		<li><a href="#UncoVisiInteActiInfe">4.3 Unconscious Vision and the Intentional Action Inference</a></li>
	</ul>
	</li>
	<li><a href="#SpecCons">5. Specific Consciousness</a>
	<ul>
		<li><a href="#NeurRepr">5.1 Neural Representationalism</a></li>
		<li><a href="#ContStraBinoRiva">5.2 The Contrast Strategy: Binocular Rivalry</a></li>
		<li><a href="#NeurStim">5.3 Neural Stimulation</a>
		<ul>
			<li><a href="#VisuMotiPerc">5.3.1 Visual Motion Perception</a></li>
			<li><a href="#TactVibr">5.3.2 Tactile Vibration</a></li>
		</ul>
		</li>
	</ul>
	</li>
	<li><a href="#Futu">6. The Future</a></li>
	<li><a href="#Bib">Bibliography</a></li>
	<li><a href="#Aca">Academic Tools</a></li>
	<li><a href="#Oth">Other Internet Resources</a></li>
	<li><a href="#Rel">Related Entries</a></li>
</ul>
<!--Entry Contents-->

<hr /></div>

<div id="main-text">

<h2 id="Fund">1. Fundamentals</h2>

<h3 id="MapBrai">1.1 A Map of the Brain</h3>

<p>
The brain can be divided into the cerebral <em>cortex</em> and the
<em>subcortex</em>. The cortex is divided into two hemispheres, left
and right, each of which can be divided into four lobes: frontal,
parietal, temporal and occipital.</p>

<div class="figure" id="fig1">
<img alt="a diagram of the brain. Major clockwise sections labeled are the 'Frontal Lobe' with the PFC marked in it. 'Parietal Lobe' with S1 between it and the Frontal Lobe; also includes the SPL and IPL. 'Dorsal Stream' with, in a circle, MST and MT, V6, V3A. 'Occipital Lobe' with V1 and then in concentric circles around V1 are V2 then V3. 'Ventral Stream' with V4. 'Temporal Lobe' with IT. V1 has an arrow pointing to V4 then IT. V1 also has an arrow pointing to V3A which in turn has arrows pointing to V6 and MST/MT. V6 has arrows pointing to SPL and MST/MT. MST/MT has an arrow pointing to IPL." src="figure1.png" style="width:432px" />

<p>
<span class="figlabel">Figure 1. The Cerebral Cortex and Salient
Areas</span></p>

<p>
Figure Legend: The four lobes of the primate brain, shown for the left
hemisphere. Some areas of interest are highlighted. Abbreviations:
PFC: prefrontal cortex; IT: inferotemporal cortex; S1: primary
somatosensory cortex; IPL and SPL: Inferior and Superior Parietal
Lobule; MST: medial superior temporal visual area; MT: middle temporal
visual area (also called V5 in humans); V1: primary visual cortex;
V2-V6 consist of additional visual areas.</p>
</div>

<p>
The discussion that follows will highlight specific areas of cortex
including the <em>prefrontal cortex</em> that will figure in
discussions of confidence
 (<a href="#ConfMetaAppr">section 2.3</a>),
 the global neuronal workspace
 (<a href="#GlobNeurWork">section 3.1</a>)
 and higher order theories
 (<a href="#HighOrdeTheo">section 3.3</a>);
 the <em>dorsal visual stream</em> that projects into parietal cortex
and the <em>ventral visual stream</em> that projects into temporal
cortex including visual areas specialized for processing places,
faces, and word forms (see sections
 <a href="#UnreWakeSynInteActiInfe">2.5</a>
 on places,
 <a href="#UncoVisiTwoVisuStre">4.1</a>
 on visual agnosia and
 <a href="#NeurStim">5.3</a>
 on seeing words); <em>primary somatosensory cortex</em> S1 (see
section
 <a href="#TactVibr">5.3.2</a>
 on tactile sensation); and early visual areas in the occipital cortex
including the <em>primary visual area, V1</em> (see sections
 <a href="#Blin">4.2</a>
 on blindsight and
 <a href="#ContStraBinoRiva">5.2</a>
 on binocular rivalry) and a motion sensitive area <em>V5</em>, also
known as the <em>middle temporal area</em> (MT;
 <a href="#VisuMotiPerc">section 5.3.1</a>
 on seeing motion). Beneath the cortex is the subcortex, divided into
the forebrain, midbrain, and hindbrain, which covers many regions
although our discussion will largely touch on the <em>superior
colliculus</em> and the <em>thalamus.</em> The latter is critical for
regulating wakefulness and general arousal, and both areas play an
important role in visual processing.</p>

<h3 id="NeurBrai">1.2 Neurons and Brain</h3>

<p>
A neuroscientific explanation of consciousness adduces properties of
the brain, typically the brain&rsquo;s electrical properties. A
salient phenomenon is neural signaling through <em>action
potentials</em> or <em>spikes.</em> A spike is a large change in
electrical potential across a neuron&rsquo;s cellular membrane which
can be transmitted between neurons that form a neural circuit. For a
sensory neuron, the spikes it generates are tied to its <em>receptive
field.</em> For example, in a visual neuron, its receptive field is
understood in spatial terms and corresponds to that area of external
space where an appropriate stimulus triggers the neuron to spike.
Given this correlation between stimulus and spikes, the latter carries
information about the former. Information processing in sensory
systems involves processing of information regarding stimuli within
receptive fields.</p>

<p>
Which electrical property provides the most fruitful explanatory basis
for understanding consciousness remains an open question. For example,
when looking at a single neuron, neuroscientists are not interested in
spikes per se but the spike rate generated by a neuron per unit time.
Yet spike rate is one among many potentially relevant neural
properties. Consider the blood oxygen level dependent signal (BOLD)
measure in functional magnetic resonance imaging (fMRI). The BOLD
signal is a measure of changes in blood flow in the brain when neural
tissue is active and is postulated to be a function of post-synaptic
activity while spikes are tied to presynaptic activity. Furthermore,
neuroscientists are often not interested in the response of a single
neuron but rather that of a population of neurons, of whole brain
regions, and/or their interactions. Higher order properties of brain
regions include the <em>local field potential</em> generated by
populations of neurons and correlated activity such as
<em>synchrony</em> between activity in different areas of the brain as
measured by, for example, electrocorticography (EEG).</p>

<p>
The number of neural properties potentially relevant to explaining
mental phenomena is dizzying. This review focuses on the facts that
neural sensory systems carry information about the subject&rsquo;s
environment and that neural information processing can be tied to a
notion of <em>neural representation</em>. How precisely to understand
neural representation is itself a vexed question (Cao 2012, 2014; Shea
2014; Baker, Lansdell, &amp; Kording 2022), but we will deploy a
simple assumption with respect to spikes which can be reconfigured for
other properties: where a sensory neuron generates spikes when a
stimulus is placed in its receptive field, the spikes carry
information about the stimulus. The sensory neuron&rsquo;s activity
thus <em>represent</em> the relevant aspect of the stimulus that
drives its response (e.g., direction of motion or intensity of a
 sound).<sup>[<a href="notes.html#note-2" id="ref-2">2</a>]
 </sup>We shall return to neural representation in the final section
when discussing how neural representations might explain conscious
contents.</p>

<h3 id="AcceConsPhenCons">1.3 Access Consciousness and Phenomenal Consciousness</h3>

<p>
An important distinction separates <em>access consciousness</em> from
<em>phenomenal consciousness</em> (Block 1995). &ldquo;Phenomenal
consciousness&rdquo; refers to those properties of experience that
correspond to <em>what it is like</em> for a subject to have those
experiences (Nagel 1974 and the entry on
 <a href="../qualia/">qualia</a>).
 These features are apparent to the subject from the inside, so
tracking them arguably depends on one&rsquo;s having the relevant
experience. For example, one understands what it is like to see red
only if one has visual experiences of the relevant type (Jackson
1982).</p>

<p>
As noted earlier, introspection is the first source of evidence about
consciousness. Introspective reports bridge the subjective and
objective. They serve as a behavioral measure that expresses the
subject&rsquo;s own take on what it is like for them in having an
experience. While there have been recent concerns about the
reliability or empirical usefulness of introspection (Schwitzgebel
2011; Irvine 2012a), there are plausibly many contexts where
introspection is reliable (Spener 2015; Wu 2023: chap. 7; see Irvine
2012b for an extended discussion of introspection in consciousness
science; for philosophical theories, see Smithies &amp; Stoljar
2012).</p>

<p>
Introspective reports demonstrate that the subject can access the
targeted conscious state. That is, the state is
<em>access</em>-conscious: it is accessible for use in reasoning,
report, and the control of action. Talk of access-consciousness must
keep track of the distinction between <em>actual</em> access versus
<em>accessibility</em>. When one reports on one&rsquo;s conscious
state, one accesses the state. Thus, access consciousness provides
much of the evidence for empirical theories of consciousness. Still,
it seems plausible that a state can be conscious even if one does not
access it in report so long as that state is <em>accessible</em>. One
<em>can</em> report it. Access-consciousness is usually defined in
terms of this dispositional notion of accessibility.</p>

<p>
We must also consider the type of access/accessibility. Block&rsquo;s
original characterization of access-consciousness emphasized
accessibility in terms of the <em>rational</em> control of behavior,
so we can summarize his account as follows:</p>

<blockquote>

<p>
A representation is access-conscious if it is poised for free use in
reasoning and for direct &ldquo;rational&rdquo; control of action and
speech.</p>
</blockquote>

<p>
Rational access contrasts with a broader conception of
<em>intentional</em> access that takes a mental state to be
access-conscious if it can inform goal-directed or intentional
behavior including behavior that is not rational or done for a reason.
This broader notion allows for additional measurable behaviors as
relevant in assessing phenomenal consciousness especially in
non-linguistic animals, non-verbal infants, and non-communicative
patients. So, if access provides us with evidence for phenomenal
consciousness, this can be (a) through introspective reports; (b)
through rational behavior, (c) through intentional behavior including
nonrational behavior. Indeed, in certain contexts, reflexive behavior
and autonomic physiological measures provide measures of consciousness
 (<a href="#AcceConNoRepoPara">section 2.2</a>).</p>
 
<h3 id="GeneSpecCons">1.4 Generic and Specific Consciousness</h3>

<p>
There are two ways in which consciousness is understood in this entry.
The first focuses on a mental state&rsquo;s <em>being conscious</em>
in general as opposed to not being conscious. Call this property
<em>generic consciousness</em>, a property shared by specific
conscious states such as seeing a red rose, feeling a touch, or being
angry. Thus:</p>

<blockquote>

<p>
<em>Generic Consciousness:</em> What conditions/states <em>N</em> of
nervous systems are necessary and/or sufficient for a mental state,
<em>M</em>, to be conscious as opposed to not?</p>
</blockquote>

<p>
If there is such an <em>N</em>, then the presence of <em>N</em>
entails that an associated mental state <em>M</em> is conscious and/or
its absence entails that <em>M</em> is unconscious.</p>

<p>
A second focus will be on the content of consciousness, say that
associated with a perceptual experience&rsquo;s being of some
perceptible <em>X</em>. This yields a question about specific contents
of consciousness such as experiencing the motion of an object (see
 <a href="#VisuMotiPerc">section 5.3.1</a>)
 or a vibration on one&rsquo;s finger (see
 <a href="#TactVibr">section 5.3.2</a>):</p>
 
<blockquote>

<p>
<em>Specific Consciousness:</em> What neural states or properties are
necessary and/or sufficient for a conscious perceptual state to have
content <em>X</em> rather than <em>Y</em>?</p>
</blockquote>

<p>
In introspectively accessing their conscious states, a subject reports
what their experience is like by reporting what they experiences.
Thus, the subject can report seeing an object moving, changing color,
or being of a certain kind (e.g., a mug) and thus specify the content
of the perceptual state. Discussion of specific consciousness will
focus on perceptual states described as consciously perceiving
<em>X</em> where <em>X</em> can be a particular such as a face or a
property such as the color of an object.</p>

<p>
Posing a clear question involves grasping its possible answers and in
science, this is informed by identifying experiments that can provide
evidence for such answers. The emphasis on necessary and sufficient
conditions in our two questions indicates how to empirically test
specific proposals. To test sufficiency, one would aim to produce or
modulate a certain neural state and then demonstrate that
consciousness of a certain form arises. To test necessity, one would
eliminate a certain neural state and demonstrate that consciousness is
abolished. Notice that such tests go beyond mere correlation between
neural states and conscious states (see
 <a href="#NeurCorrCons">section 1.6</a>
 on neural correlates and sections
 <a href="#AcceConNoRepoPara">2.2</a>,
 <a href="#NeurGeneConsUncoVisiCaseStud">4</a> and
 <a href="#SpecCons">5</a>
 for tests of necessity and sufficiency).</p>

<p>
In many experimental contexts, the underlying idea is <em>causal</em>
necessity and sufficiency. However, if <em>A</em> = <em>B</em>, then
<em>A</em>&rsquo;s presence is also necessary and sufficient for
<em>B</em>&rsquo;s presence since they are identical. Thus, a brain
lesion that eliminates <em>N</em> and thereby eliminates conscious
state <em>S</em> might do so either because <em>N</em> is causally
necessary for <em>S</em> or because <em>N</em> = <em>S</em>. An
intermediate relation is that <em>N</em> <em>constitutes</em> or
<em>grounds</em> <em>S</em> which does not imply that N = S (see the
entry on
 <a href="../grounding/">metaphysical grounding</a>).
 Whichever option holds for <em>S</em>, the first step is to find
<em>N</em>, a neural correlate of consciousness
 (<a href="#NeurCorrCons">section 1.6</a>).</p>
 
<p>
In what follows, to explain generic consciousness, various global
properties of neural systems will be considered
 (<a href="#NeurTheoCons">section 3</a>)
 as well as specific anatomical regions that are tied to conscious
versus unconscious vision as a case study
 (<a href="#NeurGeneConsUncoVisiCaseStud">section 4</a>).
 For specific consciousness, fine-grained manipulations of neural
representations will be examined that plausibly shift and modulate the
contents of perceptual experience
 (<a href="#SpecCons">section 5</a>).</p>
 
<h3 id="HardProb">1.5 The Hard Problem</h3>

<p>
David Chalmers presents the hard problem as follows:</p>

<blockquote>

<p>
It is undeniable that some organisms are subjects of experience. But
the question of how it is that these systems are subjects of
experience is perplexing. Why is it that when our cognitive systems
engage in visual and auditory information-processing, we have visual
or auditory experience: the quality of deep blue, the sensation of
middle C? How can we explain why there is something it is like to
entertain a mental image, or to experience an emotion? It is widely
agreed that experience arises from a physical basis, but we have no
good explanation of why and how it so arises. Why should physical
processing give rise to a rich inner life at all? It seems objectively
unreasonable that it should, and yet it does. If any problem qualifies
as <em>the</em> problem of consciousness, it is this one. (Chalmers
1995: 212)</p>
</blockquote>

<p>
The Hard Problem can be specified in terms of generic and specific
consciousness (Chalmers 1996). In both cases, Chalmers argues that
there is an inherent limitation to empirical explanations of
phenomenal consciousness in that empirical explanations will be
fundamentally either structural or functional, yet phenomenal
consciousness is not reducible to either. This means that there will
be something that is left out in empirical explanations of
consciousness, a missing ingredient (see also the <em>explanatory
gap</em> [Levine 1983]).</p>

<p>
There are different responses to the hard problem. One response is to
sharpen the explanatory targets of neuroscience by focusing on what
Chalmers calls <em>structural features</em> of phenomenal
consciousness, such as the spatial structure of visual experience, or
on the contents of phenomenal consciousness. When we assess
explanations of specific contents of consciousness, these focus on the
neural representations that fix conscious contents. These explanations
leave open exactly what the secret ingredient is that shifts a state
with that content from unconsciousness to consciousness. On
ingredients explaining generic consciousness, a variety of options
have been proposed (see
 <a href="#NeurTheoCons">section 3</a>),
 but it is unclear whether these answer the Hard Problem, especially
if any answer to that the Problem has a necessary condition that the
explanation must conceptually close off certain possibilities, say the
possibility that the ingredient could be added yet consciousness not
ignite as in a zombie, a creature without phenomenal consciousness
(see the entry on
 <a href="../zombies/">zombies</a>).
 Indeed, some philosophers deny the hard problem (see Dennett 2018 for
a recent statement). Patricia Churchland urges: &ldquo;Learn the
science, do the science, and see what happens&rdquo; (Churchland 1996:
408).</p>

<p>
Perhaps the most common attitude for neuroscientists is to set the
hard problem aside. Instead of explaining the existence of
consciousness in the biological world, they set themselves to
explaining generic consciousness by identifying neural properties that
can turn consciousness on and off and explaining specific
consciousness by identifying the neural representational basis of
conscious contents.</p>

<h3 id="NeurCorrCons">1.6 Neural Correlates of Consciousness</h3>

<p>
Modern neuroscience of consciousness has attempted to explain
consciousness by focusing on <em>neural correlates of
consciousness</em> or NCCs (Crick &amp; Koch 1990; LeDoux, Michel,
&amp; Lau 2020; Morales &amp; Lau 2020). Identifying correlates is an
important first step in understanding consciousness, but it is an
early step. After all, correlates are not necessarily explanatory in
the sense of answering specific questions posed by neuroscience. That
one does not want a mere correlate was recognized by Chalmers who
defined an NCC as follows:</p>

<blockquote>

<p>
An NCC is a minimal neural system <em>N</em> such that there is a
mapping from states of <em>N</em> to states of consciousness, where a
given state of <em>N</em> is sufficient under conditions <em>C</em>,
for the corresponding state of consciousness. (Chalmers 2000: 31)</p>
</blockquote>

<p>
A similar way of putting this is that an NCC is &ldquo;the minimal set
of neuronal events and mechanisms jointly sufficient for a specific
conscious percept&rdquo; (Koch 2004: 16). One wants a minimal neural
system since, crudely put, the brain is sufficient for consciousness
but to point this out is hardly to explain consciousness even if it
provides an answer to questions about sufficiency. There is, of
course, much more to be said that is informative even if one does not
drill down to a &ldquo;minimal&rdquo; neural system which is tricky to
define or operationalize (see Chalmers 2000 for discussion; for
criticisms of the NCC approach, see No&euml; &amp; Thompson 2004; for
criticisms of Chalmers&rsquo; definition, see Fink 2016).</p>

<p>
The emphasis on sufficiency goes beyond mere correlation, as
neuroscientists aim to answer more than the question: What is a neural
correlate for conscious phenomenon <em>C</em>? For example,
Chalmers&rsquo; and Koch&rsquo;s emphases on sufficiency indicate that
they aim to answer the question: What neural phenomenon is sufficient
for consciousness? Perhaps more specifically: What neural phenomenon
is causally sufficient for consciousness? Accordingly, talk of
&ldquo;correlate&rdquo; is unfortunate since sufficiency implies
correlation but not vice versa. After all, assume that the NCC is type
identical to a conscious state. Then many neural states will correlate
with the conscious state: (1) the NCC&rsquo;s typical effects, (2) its
typical causes, and (3) states that are necessary for the NCCs
obtaining (e.g., the presence of sufficient oxygen). Thus, some
correlated effects will not be explanatory. For example, citing the
effects of consciousness will not provide causally sufficient
conditions for consciousness.</p>

<p>
Establishing necessary conditions for consciousness is also difficult.
Neuroplasticity, redundancy, and convergent evolution make necessity
claims extremely hard to support experimentally. Under normal
conditions, healthy humans may require certain brain areas or
processes for supporting consciousness. However, this does not mean
those regions or processes are necessary in any strong metaphysical
sense. For example, after a lesion, the brain&rsquo;s functional
connections may change allowing a different structural support for
consciousness to emerge. There is also nothing in the cortical
arrangement of specialized regions that fix them to a particular
function. The so-called &ldquo;visual cortex&rdquo; is recruited in
blind individuals to perform auditory, numerical, and linguistic
processing (Bedny 2017). Similarly, as with many other complex
structures, the brain is highly redundant. Different areas may perform
the same function, preventing any single one from being strictly
necessary. Finally, the way in which the mammalian brain operates is
not the only way to support awareness. Birds&mdash;as well as
cephalopods and insects (Barron &amp; Klein 2016; Birch, Schnell,
&amp; Clayton 2020)&mdash;and even other primates have different
anatomical and functional neural mechanisms and yet their nervous
systems may support consciousness (Nieder, Wagener, &amp; Rinnert
2020). Thus, there may not be a single necessary structure or process
for supporting conscious awareness.</p>

<p>
While many theorists are focused on explanatory correlates, it is not
clear that the field has always grasped this, something recent
theorists have been at pains to emphasize (Aru et al. 2012; Graaf,
Hsieh, &amp; Sack 2012). In other contexts, neuroscientists speak of
the neural <em>basis</em> of a phenomenon where the basis does not
simply correlate with the phenomenon but also explains and possibly
grounds it. However, talk of correlates is entrenched in the
neuroscience of consciousness, so one must remember that the goal is
to find the subset of neural correlates that are explanatory, in
answering concrete questions. Reference to neural correlates in this
entry will always mean neural explanatory correlates of consciousness
(on occasion, we will speak of these as the neural <em>basis</em> of
consciousness). That is, our two questions about specific and generic
consciousness focus the discussion on neuroscientific theories and
data that contribute to explaining them. This project allows that
there are limits to neural explanations of consciousness, precisely
because of the explanatory gap (Levine 1983).</p>

<h2 id="MethForTracCons">2. Methods for Tracking Consciousness</h2>

<p>
Since studying consciousness requires that scientists track its
presence, it will be important to examine various methods used in
neuroscience to isolate and probe conscious states.</p>

<h3 id="IntrRepo">2.1 Introspection and Report</h3>

<p>
Scientists primarily study phenomenal consciousness through subjective
reports; objective measures such as performance in a task are often
used too (for a critical assessment, see Irvine 2013). We can treat
reports in neuroscience as conceptual in that they express how the
subject recognizes things to be, whether regarding what they perceive
(perceptual or observational reports, as in psychophysics) or
regarding what mental states they are in (introspective reports). A
report&rsquo;s conceptual content can be conveyed in words or other
overt behavior whose significance is fixed within an experiment (e.g.,
pressing a button to indicate that a stimulus is present or that one
sees it). Subjective reports of conscious states draw on distinctively
first-personal access to that state. The subject
<em>introspects</em>.</p>

<p>
Introspection raises questions that science has only recently begun to
address systematically in large part because of longstanding suspicion
regarding introspective methods or, in contrast, because of an
unquestioning assumption that introspection is largely and reliably
accurate. Early modern psychology relied on introspection to parse
mental processes but ultimately abandoned it due to worries about
introspection&rsquo;s reliability (Feest 2012; Spener 2018).
Introspection was judged to be an unreliable method for addressing
questions about mental processing (and it is still seen with suspicion
by some; Schwitzgebel 2008). To address these worries, we must
understand how introspection works, but unlike many other
psychological capacities, detailed models of introspection of
consciousness are hard to develop (Feest 2014; for theories of
introspecting propositional attitudes, see Nichols &amp; Stich 2003;
Heal 1996; Carruthers 2011). This makes it difficult to address
long-standing worries about introspective reliability regarding
consciousness.</p>

<p>
In science, questions raised about the reliability of a method are
answered by calibrating and testing the method. This calibration has
not been done with respect to the type of introspection commonly
practiced by philosophers. Such introspection has revealed many
phenomenal features that are targets of active investigation such as
the phenomenology of mineness (Ehrsson 2009), sense of agency (Bayne
2011; Vignemont &amp; Fourneret 2004; Marcel 2003; Horgan, Tienson,
&amp; Graham 2003), transparency (Harman 1990; Tye 1992),
self-consciousness (Kriegel 2003: 122), cognitive phenomenology (Bayne
&amp; Montague 2011); phenomenal unity (Bayne &amp; Chalmers 2003)
among others. A scientist might worry that philosophical introspection
merely recycles rejected methods of a century ago, indeed without the
stringent controls or training imposed by earlier psychologists. How
can we ascertain and ensure the reliability of introspection in the
empirical study of consciousness?</p>

<p>
Model migration, applying well-understood concepts in less
well-understood domains, provides conceptual resources to reveal
patterns across empirical systems and to promote theoretical insights
(Lin 2018; Knuuttila &amp; Loettgers 2014; see the entry on
 <a href="../models-science/">models in science</a>).
 Introspection&rsquo;s range of operation and its reliability
conditions&mdash;when and why it succeeds and when and why it
fails&mdash;can be calibrated by drawing parallels to how Signal
Detection Theory (SDT) models perception (see Do&#322;&#281;ga 2023
for an evaluation of introspective theories). In SDT (Tanner &amp;
Swets 1954; Hautus, Macmillan, &amp; Creelman 2021), perceptually
detecting or discriminating a stimulus is the joint outcome of the
observer&rsquo;s perceptual sensitivity (e.g., how well a perceptual
system can tell signal from noise) and a decision to classify the
available perceptual evidence as signal or not (i.e., observers set a
response criterion for what counts as signal). Consider trying to
detect something moving in the brush at twilight versus at noon. In
the latter, the signal will be greatly separated from noise (the
object will be easier to detect because it generates a strong internal
perceptual response). In contrast, at twilight the signal will not be
easy to disentangle from noise (the object will be harder to detect
because of the weaker internal perceptual response it generates in the
observer). Importantly, even at noon when the signal is strong, rare
as they might be, misses and false alarms are to be expected. Yet in
either case, one might operate with a conservative response criterion,
say because one is afraid to be wrong. Thus, even if the signal is
detectable, one might still opt not to report on it given a
<em>conservative</em> bias (criterion), say if one is in the twilight
scenario and would be ridiculed for &ldquo;false alarms&rdquo;, i.e.,
claiming the object to be present when it is not.</p>

<p>
Introspection can be modeled as a signal detection mechanism that
operates over conscious states. Pains can be strong or weak, mental
images can be vivid or faint, and perceptions can be more or less
striking. In other words, conscious experiences admit degrees of
intensity (Lee 2023; Morales 2023). According to introspective Signal
Detection Theory or iSDT (Morales forthcoming), the intensity of
conscious experiences (their mental strength) modulates the
introspective internal response generated by the intensity of the
conscious experience under examination, which in turn modulates
introspective sensitivity. Everything else being equal, introspectors
are more likely to introspect accurately an intense experience (e.g.,
a strong pain, a vivid mental image, etc.) than a weak experience
(e.g., a mild pain, a faint mental image, etc.). As in perception, a
biased introspective criterion may affect an introspector&rsquo;s
response without necessarily implying changes in the introspectability
of a state. Even when rare, introspection errors (misses and false
alarms) should be possible and in fact expected. By modeling
introspection as any other detection mechanism, iSDT aims to explain
the range of reliability of introspection while preserving our
intuitions that it provides highly accurate judgments in familiar
cases such as detecting severe pain while also accounting for
introspection&rsquo;s potential fallibility.</p>

<p>
Thus, in the context of scientific experiments of consciousness, iSDT
provides a model that predicts higher introspective reliability when
judging strong experiences. When the experiences are weak&mdash;as one
may expect from stimuli presented quickly and at
threshold&mdash;introspective judgments are expected to be more
unreliable (Dijkstra &amp; Fleming 2023).</p>

<p>
Another way to link scientific introspection to consciousness is to
connect it to models of attention. Philosophical conceptions of
introspective attention construe it as capable of directly focusing on
phenomenal properties and experiences. As this idea is fleshed out,
however, it is clearly not a form of attention studied by cognitive
science, for the posited direct introspective attention is neither
perceptual attention nor what psychologists call internal attention
(e.g., the retrieval of thought contents as in memory recollection;
Chun, Golomb, &amp; Turk-Browne 2011). Calibrating introspection as it
is used in the science of consciousness would benefit from concrete
models of introspection (Chirimuuta 2014; Spener 2015; Wu 2023;
Kammerer &amp; Frankish 2023; Morales forthcoming).</p>

<p>
We can draw inspiration from a proposal inspired by Gareth Evans
(1982): in introspecting perceptual states, say judging that one sees
an object, one draws on the same perceptual capacities used to answer
the question whether the object is present. In introspection, one then
appends a further concept of &ldquo;seeing&rdquo; to one&rsquo;s
perceptual
 report.<sup>[<a href="notes.html#note-3" id="ref-3">3</a>]</sup>
 Thus, instead of simply reporting that a red stimulus is present, one
reports that one sees the red stimulus. Paradoxically, introspection
relies on externally directed perceptual attention, but as noted
earlier, identifying what one perceives is a way of characterizing
what one&rsquo;s perception is like, so this &ldquo;outward&rdquo;
perspective can provide information about the inner.</p>

<p>
Further, the advantage of this proposal is that questions of
reliability come down to questions of the reliability of psychological
capacities that can be empirically assessed, say perceptual,
attentional and conceptual reliability. For example, Peters and Lau
(2015) showed that accuracy in judgments about the visibility of a
stimulus, the introspective measure, coincided with accuracy in
judgments about stimulus properties, the objective measure (see also
Rausch &amp; Zehetleitner 2016). Further, in many of the examples to
be discussed, the perceptual attention-based account provides a
plausible cognitive model of introspection (Wu, 2023). Subjects report
on what they perceptually experience by attending to the object of
their experience, and where perception and attention are reliable, a
plausible hypothesis is that their introspective judgments will be
reliable as well. Accordingly, the reliability of introspection in the
empirical studies to be discussed can be assumed. Still, given that no
scientist should assert the reliability of a method without
calibration, introspection must be subject to the same standards.
There is more work to be done.</p>

<h3 id="AcceConNoRepoPara">2.2 Access Consciousness and No-Report Paradigms</h3>

<p>
Introspection illustrates a type of cognitive access, for a state that
is introspected is access conscious. This raises a question that has
epistemic implications: is access consciousness necessary for
phenomenal consciousness? If it is not, then there can be phenomenal
states that are not access conscious, so are in principle not
reportable. That is, phenomenal consciousness can <em>overflow</em>
access consciousness (Block 2007).</p>

<p>
Access is tied to attention, and attention is tied to report. Some
views hold that attention is necessary for access, which entails
phenomenal consciousness (e.g., the Global Workspace theory
 [<a href="#GlobNeurWork">section 3.1</a>]).<sup>[<a href="notes.html#note-4" id="ref-4">4</a>]</sup>
 In contrast, other theories (e.g., recurrent processing theory
 <a href="#RecuProcTheo">section 3.2</a>]),
 hold that there can be phenomenal states that are not accessible.</p>

<p>
Many scientists of consciousness take there to be evidence for no
phenomenal consciousness without access and little if no evidence of
phenomenal consciousness outside of access. While those antagonistic
to overflow have argued that it is not empirically testable (Cohen
&amp; Dennett 2011) testing that attention is necessary for
consciousness may be equally untestable. After all, we must eliminate
attention to a target while gathering evidence for the absence of
consciousness. Yet if gathering evidence for consciousness requires
attention, then in fulfilling the conditions for testing the necessity
of attention, we undercut the access needed to substantiate the
absence of (Wu 2017; for a monograph length discussion of attention
and consciousness, see Montemayor &amp; Haladjian
 2015).<sup>[<a href="notes.html#note-5" id="ref-5">5</a>]</sup>
 How then can we gather the required evidence to assess competing
theories?</p>

<p>
One response is to draw on <em>no-report paradigms</em> which measure
reflexive behaviors correlated with conscious states to provide a
window on the phenomenal that is independent of access (Lumer &amp;
Rees 1999; Tse et al. 2005). In binocular rivalry paradigms (see
 <a href="#ContStraBinoRiva">section 5.2</a>),
 researchers show subjects different images to each eye. Due to their
very different features, these images cannot be fused into a single
percept. This results in alternating experiences that transition back
and forth between one image and the other. Since the presented images
remain constant while experience changes, binocular rivalry has been
used to find the neural correlates of consciousness (see Zou, He,
&amp; Zhang 2016; and Giles, Lau, &amp; Odegaard 2016 for concerns
about using binocular rivalry for studying the neural correlates of
consciousness). However, binocular rivalry tasks have typically
involved asking subjects to report what their experience is like,
requiring the recruitment of attention and explicit report. To
overcome this issue, Hesse &amp; Tsao (2020) recently introduced an
important variation by adding a small fixation point on different
locations of each image. For example, subjects&rsquo; right eye was
shown a photo of a person with a fixation point drawn on the bottom of
the image, and their left eye was presented with a photo of some tacos
with a fixation point on the top. People and monkeys were trained to
look at the fixation point while ignoring the image. This way, the
experimenters could use their eye movement behavior as a proxy for
which image they were experiencing without having to collect explicit
reports: if subjects look up, they are experiencing the tacos; if they
look down, they are experiencing the person. They confirmed with
explicit reports from humans that both monkeys and humans in the
no-report condition behaved similarly. Importantly, they found from
single-cell recordings in the monkeys that neurons in inferotemporal
cortex&mdash;a downstream region associated with high-level visual
processing&mdash;represented the experienced image. Because these
monkeys were never trained to report their percept (they were just
following the fixation point wherever it was), the experimenters could
more confidently conclude that the activity linked to the alternating
images was due to consciously experiencing them and not due to
introspecting or reporting.</p>

<p>
No-report paradigms use indirect responses to track the
subject&rsquo;s perceptual experience in the absence of explicit
(conceptualized) report. These can include eye movements, pupil
changes, electroencephalographic (EEG) activity or galvanic skin
conductance changes, among others. No-report paradigms seem to provide
a way to track phenomenal consciousness even when access is
eliminated. This would broaden the evidential basis for consciousness
beyond introspection and indeed, beyond intentional behavior (our
&ldquo;broad&rdquo; conception of access). However, in practice they
do not always result in drastically different results (Michel &amp;
Morales 2020). Moreover, they do not fully circumvent introspection
(Overgaard &amp; Fazekas 2016). For example, the usefulness of any
indirect physiological measure depends on validating its correlation
with alternating experience <em>given subjective reports.</em> Once it
is validated, monitoring these autonomic responses can provide a way
to substitute for subjective reports within that paradigm. One cannot,
however, simply extend the use of no-report paradigms outside the
behavioral contexts within which the method is validated. With each
new experimental context, we must revalidate the measure with
introspective report. Moreover, no-report paradigms do not match
post-perceptual processing between conscious and unconscious
conditions (Block 2019). Even if overt report is matched, the
cognitive consequences of perceiving a stimulus consciously and
failing to do so are not the same. For example, systematic reflections
may be triggered by one stimulus (the person) but not the other (the
tacos). These post-cognitive differences would generate different
neural activity that is not necessarily related to consciousness. To
avoid this issue, post-perceptual cognition would also need to be
matched to rule out potential confounds (Block 2019). However, this is
easier said than done and no uncontroversial solutions around this
issue have been found (Phillips &amp; Morales 2020; Panagiotaropoulos,
Dwarakanath, &amp; Kapoor 2020).</p>

<p>
Can we use no report paradigms to address whether access is necessary
for phenomenal consciousness? A likely experiment would be one that
validates no-report correlates for some conscious phenomenon
<em>P</em> in a concrete experimental context <em>C</em>. With this
validation in hand, one then eliminates accessibility and attention
with respect to <em>P</em> in <em>C</em>. If the no-report correlate
remains, would this clearly support overflow? Perhaps, it could still
be argued that the result does not rule out the possibility that
phenomenal consciousness disappears with access consciousness
<em>despite</em> the no-report correlate remaining. For example, the
reflexive response and phenomenal consciousness might have a common
cause that remains even if phenomenal consciousness is selectively
eliminated by removing access.</p>

<h3 id="ConfMetaAppr">2.3 Confidence and Metacognitive Approaches</h3>

<p>
Given worries about calibrating introspection, researchers have asked
subjects to provide a different metacognitive assessment of conscious
states via reports about <em>confidence</em> (Fleming 2023a, 2020;
Pouget, Drugowitsch, &amp; Kepecs 2016). A standard approach is to
have subjects perform a task, say perceptual discrimination of a
stimulus, and then report how <em>confident</em> they are that their
perceptual judgment was accurate. This metacognitive judgment, a
confidence rating, about perception can be assessed for accuracy by
comparing it with perceptual performance (for discussion of formal
methods such as metacognitive signal detection theory, see Maniscalco
&amp; Lau 2012, 2014). Related paradigms include post-decision
wagering where subjects place wagers on specific responses as a way of
estimating their confidence (Persaud, McLeod, &amp; Cowey 2007; but
see Dienes &amp; Seth 2010).</p>

<p>
There are some advantages of using confidence judgments for studying
consciousness (Michel 2023; Morales &amp; Lau 2022; Peters 2022).
While standard introspective judgments about conscious experiences may
capture more directly the phenomenon of interest, confidence judgments
are easier to explain to subjects and they are also more interpretable
from the experimenter&rsquo;s point of view. Confidence judgments
provide an objective measure of <em>metacognitive sensitivity</em>:
how well subjects&rsquo; confidence judgments track their performance
in the task. Subjects can also receive feedback on those ratings and,
at least in principle, improve their metacognitive sensitivity (though
this has proved hard to achieve in laboratory tasks; Rouy et al. 2022;
Haddara &amp; Rahnev 2022). Metacognitive judgments also have the
advantage over direct judgments about conscious experiences (e.g.,
Rams&oslash;y &amp; Overgaard 2004) in that they allow for comparisons
between different domains that have very different phenomenology
(e.g., perception vs memory) (e.g., Gardelle, Corre, &amp; Mamassian
2016; Faivre et al. 2017; Morales, Lau, &amp; Fleming 2018; Mazancieux
et al. 2020).</p>

<p>
One concern with metacognitive approaches is that they also rely on
introspection (Rosenthal 2019; see also Sandberg et al. 2010; Dienes
&amp; Seth 2010). If metacognition relies on introspection, does it
not accrue all the disadvantages of the latter? Perhaps, but an
important gain in metacognitive approaches is that it allows for
quantitative psychophysical analysis. While it does not replace
introspection, it brings an analytical rigor to addressing certain
aspects of conscious awareness.</p>

<p>
How might we bridge metacognition with consciousness as probed by
traditional introspection? The metacognitive judgment reflects
introspective assessment of the quality of perceptual states and can
provide information about the presence of consciousness. For example,
Peters and Lau (2015) found that increases in metacognitive confidence
tracked increases in perceptual sensitivity, which presumably underly
the quality of perceptual experiences. They also did not find
significant differences between asking for confidence or visibility
judgments. Even studies that have found (small) differences between
the two kinds of judgments indicate considerable association between
the two ratings, suggesting similar behavioral patterns (Rausch &amp;
Zehetleitner 2016; Zehetleitner &amp; Rausch 2013).</p>

<p>
Beyond behavior, neuroscientific work shows that similar regions of
the brain (e.g., prefrontal cortex) may be involved in supporting both
conscious awareness and metacognitive judgments. Studies with
non-human primates and rodents have begun to shed light on neural
processing for metacognition (for reviews, see Grimaldi, Lau, &amp;
Basso 2015; Pouget, Drugowitsch, &amp; Kepecs 2016). From animal
studies, one theory is that metacognitive information regarding
perception is already present in perceptual areas that guide
observational judgments, and these studies implicate parietal cortex
(Kiani &amp; Shadlen 2009; Fetsch et al. 2014) and the superior
colliculus (Kim &amp; Basso 2008; but see Odegaard et al. 2018).
Alternatively, information about confidence might be read out by other
structures (see
 <a href="#HighOrdeTheo">section 3.3</a>
 on Higher-Order Theory; also the entry on
 <a href="../consciousness-higher/">higher order theories of consciousness</a>).
 In both human and animal studies, the prefrontal cortex
(specifically, subregions in dorsolateral and orbitofrontal prefrontal
cortex) has been found to support subjective reports of awareness (Cul
et al. 2009; Lau &amp; Passingham 2006), subjective appearances (Liu
et al. 2019); visibility ratings (Rounis et al. 2010); confidence
ratings (Fleming, Huijgen, &amp; Dolan 2012; Shekhar &amp; Rahnev
2018) and conscious experiences without reports (Mendoza-Halliday
&amp; Martinez-Trujillo 2017; Kapoor et al. 2022; Michel &amp; Morales
2020).</p>

<h3 id="InteActiInfe">2.4 The Intentional Action Inference</h3>

<p>
Metacognitive and introspective judgments result from intentional
action, so why not look at intentional action, broadly construed, for
evidence of consciousness? Often, when subjects perform perception
guided actions, we infer that they are relevantly conscious. It would
be odd if a person cooks dinner and then denies having seen any of the
ingredients. That they did something intentionally provides evidence
that they were consciously aware of what they acted on. An emphasis on
intentional action embraces a broader evidential basis for
consciousness. Consider the <em>Intentional Action Inference</em> to
phenomenal consciousness:</p>

<blockquote>

<p>
If some subject acts intentionally, where her action is guided by a
perceptual state, then the perceptual state is phenomenally
conscious.</p>
</blockquote>

<p>
An epistemic version takes the action to provide good evidence that
the state is conscious. Notice that introspection is typically an
intentional action so it is covered by the inference. In this way, the
Inference effectively levels the evidential playing field:
introspective reports are simply one form among many types of
intentional actions that provide evidence for consciousness. Those
reports are not privileged.</p>

<p>
The intentional action inference and no-report paradigms highlight
that the science of consciousness has largely restricted its
behavioral data to one type of intentional action, introspection. What
is the basis of privileging one intentional action over others?
Consider the calibration issue. For many types of intentional action
deployed in experiments, scientists can calibrate performance by
objective measures such as accuracy. This has not been formally done
for introspection of consciousness, so scientists have privileged an
uncalibrated measure over a calibrated one. This seems empirically
ill-advised. On the flip side, one worry about the intentional action
inference is that it ignores guidance by unconscious perceptual states
(see sections
 <a href="#NeurGeneConsUncoVisiCaseStud">4</a>
 and
 <a href="#VisuMotiPerc">5.3.1</a>).</p>
 
<h3 id="UnreWakeSynInteActiInfe">2.5 Unresponsive Wakefulness Syndrome and the Intentional Action Inference</h3>

<p>
The Intentional Action Inference is operative when subjective reports
are not available. For example, it is deployed in arguing that some
patients diagnosed with unresponsive wakefulness syndrome are
conscious (Shea &amp; Bayne 2010; Drayson 2014).</p>

<blockquote>

<p>
A patient [with unresponsive wakefulness syndrome] appears at times to
be wakeful, with cycles of eye closure and eye opening resembling
those of sleep and waking. However, close observation reveals no sign
of awareness or of a &lsquo;functioning mind&rsquo;: specifically,
there is no evidence that the patient can perceive the environment or
his/her own body, communicate with others, or form intentions. As a
rule, the patient can breathe spontaneously and has a stable
circulation. The state may be a transient stage in the recovery from
coma or it may persist until death. (Working Party RCP 2003: 249)</p>
</blockquote>

<p>
These patients are not clinically comatose but fall short of being in
a &ldquo;minimally conscious state&rdquo;. Unlike unresponsive wakeful
patients, minimally conscious state patients seemingly perform
intentional actions.</p>

<p>
Recent work suggests that some patients diagnosed with unresponsive
wakeful syndrome are conscious. Owen et al. (2006) used fMRI to
demonstrate correlated activity in such patients in response to
commands to deploy imagination. In an early study, a young female
patient was scanned by fMRI while presented with three auditory
commands: &ldquo;imagine playing tennis&rdquo;, &ldquo;imagine
visiting the rooms in your home&rdquo;, &ldquo;now just relax&rdquo;.
The commands were presented at the beginning of a thirty-second
period, alternating between imagination and relax commands. The
patient demonstrated similar activity when matched to control subjects
performing the same task: sustained activation of the supplementary
motor area (SMA) was observed during the motor imagery task while
sustained activation of the parahippocampal gyrus including the
parahippocampal place area (PPA) was observed during the spatial
imagery task. Later work reproduced this result in other patients and
in one patient, the tasks were used as a proxy for &ldquo;yes&rdquo;/
&ldquo;no&rdquo; responses to questions (Monti et al. 2010; for a
review, see Fern&aacute;ndez-Espejo &amp; Owen 2013). Note that these
tasks probe <em>specific</em> contents of consciousness by monitoring
neural correlates of conscious imagery.</p>

<p>
Several authors (Greenberg 2007; Nachev &amp; Husain 2007) have
countered that the observed activity was an automatic, non-intentional
response to the command sentences, specifically to the words
&ldquo;tennis&rdquo; and &ldquo;house&rdquo;. In normal subjects,
reading action words is known to activate sensorimotor areas
(Pulverm&uuml;ller 2005). Owen and colleagues (2007), responded that
the sustained activity over thirty-seconds made an automatic response
less likely than an intentional response. One way to rule out
automaticity is to provide the patient with different sentences such
as &ldquo;do <em>not</em> imagine playing tennis&rdquo; or
&ldquo;<em>Sharlene</em> was playing tennis&rdquo;. Owen et al. (2007)
demonstrated that presenting &ldquo;Sharlene was playing tennis&rdquo;
to a normal subject did not induce the same activity as when the
subject obeyed the command &ldquo;imagine playing tennis&rdquo;, but
the same intervention was not tried on patients. However, subsequent
experiments using other measures such as EEG (Goldfine et al. 2011;
Curley et al. 2018; Cruse et al. 2012) and functional connectivity
(Demertzi et al. 2015), indicate conscious awareness is indeed present
in patients with (wrongly diagnosed) unresponsive wakefulness syndrome
(for a review, see Edlow et al. 2021; for ethical reflections around
diagnosing awareness in patients with disorders of consciousness, see
Young et al. 2021).</p>

<p>
Owen et al. draw on a neural correlate of imagination, a mental
action. Arguing that the neural correlate provides evidence of the
patient&rsquo;s executing an intentional action, they invoke a version
of the Intentional Action Inference to argue that performance provides
evidence for specific consciousness tied to the information carried in
the brain areas
 activated.<sup>[<a href="notes.html#note-6" id="ref-6">6</a>]</sup></p>
 
<h2 id="NeurTheoCons">3. Neurobiological Theories of Consciousness</h2>

<p>
Recall that the Generic Consciousness question asks:</p>

<blockquote>

<p>
What conditions/states <em>N</em> of nervous systems are necessary
and/or sufficient for a mental state, <em>M</em>, to be conscious as
opposed to not?</p>
</blockquote>

<p>
Victor Lamme notes:</p>

<blockquote>

<p>
Deciding whether there is phenomenality in a mental representation
implies putting a boundary&mdash;drawing a line&mdash;between
different types of representations&hellip;We have to start from the
intuition that consciousness (in the phenomenal sense) exists, and is
a mental function in its own right. That intuition immediately implies
that there is also <em>un</em>conscious information processing. (Lamme
2010: 208)</p>
</blockquote>

<p>
It is uncontroversial that there is unconscious information
processing, say processing occurring in a computer. What Lamme means
is that there are conscious and unconscious mental states
(representations). For example, there might be visual states of seeing
<em>X</em> that are conscious or not
 (<a href="#NeurGeneConsUncoVisiCaseStud">section 4</a>).</p>
 
<p>
In what follows, the theories discussed provide higher level neural
properties that are necessary and/or sufficient for generic
consciousness of a given state. To provide a gloss on the hypotheses:
For the Global Neuronal Workspace, entry into the neural workspace is
necessary and sufficient for a state or content to be consciousness.
For Recurrent Processing Theory, a type of recurrent processing in
sensory areas is necessary and sufficient for perceptual
consciousness, so entry into the Workspace is not necessary. For
Higher-Order Theories, the presence of a higher-order state tied to
prefrontal areas is necessary and sufficient for phenomenal
experience, so recurrent processing in sensory areas is not necessary
nor is entry into the workspace. For Information Integration Theories,
a type of integration of information is necessary and sufficient for a
state to be conscious.</p>

<h3 id="GlobNeurWork">3.1 The Global Neuronal Workspace</h3>

<p>
One explanation of generic consciousness invokes the <em>global
neuronal workspace</em>. Bernard Baars first proposed the global
workspace theory as a cognitive/computational model (Baars 1988), but
we will focus on the neural version of Stanislas Dehaene and
colleagues: a state is conscious when and only when it (or its
content) is present in the global neuronal workspace making the state
(content) globally <em>accessible</em> to multiple systems including
long-term memory, motor, evaluational, attentional and perceptual
systems (Dehaene, Kerszberg, &amp; Changeux 1998; Dehaene &amp;
Naccache 2001; Dehaene et al. 2006). Notice that the previous
characterization does not commit to whether it is phenomenal or access
consciousness that is being defined.</p>

<p>
<em>Access</em> should be understood as a relational notion:</p>

<blockquote>

<p>
A system <em>X</em> accesses content from system <em>Y</em> iff
<em>X</em> uses that content in its computations/processing.</p>
</blockquote>

<p>
The <em>accessibility</em> of information is then defined as its
potential access by other systems. Dehaene (Dehaene et al. 2006)
introduces a threefold distinction: (1) neural states that carry
information that is not accessible (<em>subliminal</em> information);
(2) states that carry information that is accessible but not accessed
(not in the workspace; <em>preconscious</em> information); and (3)
states whose information is accessed by the workspace
(<em>conscious</em> information) and is globally accessible to other
systems. So, a necessary and sufficient condition for a state&rsquo;s
being conscious rather than not is the access of a state or content by
the workspace, making that state or content accessible to other
systems. Hence, only states in (3) are conscious.</p>

<div class="figure">
<img alt="see legend. The top figure is a series of dotted concentric circles with a network of lines and nodes imposed on top; the innermost circle is labeled 'Global Workspace'. The circles are divided into five sectors [except the sector radii don't cross the innermost circle]; each sector has a labels in an arrow pointing in [unless otherwise noted] clockwise from the top as, 'Evaluative Systems (VALUE)', 'Attentional Systems (FOCUSING)', 'Motor systems (FUTURE)' [the only one with an arrow pointing out], 'Perceptual systems (PRESENT)', 'Long-Term Memory (PAST)'. The bottom figure is the same network of lines and nodes (minus circles, arrows, and labels but with on the left a picture labeled 'frontal' and on the right a picture labeled 'sensory'. " src="figure2.png" style="width:400px" />

<p>
<span class="figlabel">Figure 2. The Global Neuronal
Workspace</span></p>

<p>
Figure Legend: The top figure provides a neural architecture for the
workspace, indicating the systems that can be involved. The lower
figure sets the architecture within the six layers of the cortex
spanning frontal and sensory areas, with emphasis on neurons in layers
2 and 3. Figure reproduced from Dehaene, Kerszberg, and Changeux 1998.
Copyright (1998) National Academy of Sciences.</p>
</div>

<p>
The global neuronal workspace theory ties access to brain
architecture. It postulates a cortical structure that involves
workspace neurons with long-range connections linking systems:
perceptual, mnemonic, attentional, evaluational and motoric.</p>

<p>
What is the global workspace in neural terms? Long-range workspace
neurons within different systems can constitute the workspace, but
they should not necessarily be identified with <em>the</em> workspace.
A subset of workspace neurons becomes the workspace when they
exemplify certain neural properties. What determines which workspace
neurons constitute the workspace at a given time is the activity of
those neurons given the subject&rsquo;s current state. The workspace
then is not a rigid neural structure but a rapidly changing neural
network, typically only a proper subset of all workspace neurons.</p>

<p>
Consider then a neural population that carries content <em>p</em> and
is constituted by workspace neurons. In virtue of being workspace
neurons, the content <em>p</em> is accessible to other systems, but it
does not yet follow that the neurons then constitute the global
workspace. A further requirement is that workspace neurons are (1) put
into an active state that must be sustained so that (2) the activation
generates a <em>recurrent</em> activity between workspace systems.
Only when these systems are recurrently activated are they, along with
the units that access the information they carry, constituents of the
workspace. This activity accounts for the idea of global
<em>broadcast</em> in that workspace contents are accessible to
further systems. Broadcasting explains the idea of consciousness as
<em>for the subject</em>: globally broadcasted content is accessible
for the subject&rsquo;s use in informing behavior.</p>

<p>
The global neuronal workspace theory provides an account of
<em>access</em> consciousness but what of phenomenal consciousness?
The theory predicts widespread activation of a cortical workspace
network as correlated with phenomenal conscious experience, and
proponents often appeal to imaging results that reveal widespread
activation when consciousness is reported (Dehaene &amp; Changeux
2011). There is, however, a potential confound. We track phenomenal
consciousness by access in introspective report, so widespread
activity during reports of conscious experience correlates with both
access and phenomenal consciousness. Correlation cannot tell us
whether the observed activity is the basis of phenomenal consciousness
or of access consciousness in report (Block 2007). This remains a live
question for as discussed in
 <a href="#AcceConNoRepoPara">section 2.2</a>,
 we do not have empirical evidence that overflow is false.</p>

<p>
To eliminate the confound, experimenters ensure that performance does
not differ between conditions where consciousness is present and where
it is not. Where this was controlled, widespread activation was not
clearly observed (Lau &amp; Passingham 2006). Still, the absence of
observed activity by an imaging technique does not imply the absence
of actual activity for the activity might be beyond the limits of
detection of that technique. Further, there is a general concern about
the significance of null results given that neuroscience studies
focused on prefrontal cortex are typically underpowered (for
discussion, see Odegaard, Knight, &amp; Lau 2017).</p>

<h3 id="RecuProcTheo">3.2 Recurrent Processing Theory</h3>

<p>
A different explanation ties perceptual consciousness to processing
independent of the workspace, with focus on <em>recurrent</em>
activity in sensory areas. This approach emphasizes properties of
first-order neural representation as explaining consciousness. Victor
Lamme (2006, 2010) argues that recurrent processing is necessary and
sufficient for consciousness. Recurrent processing occurs where
sensory systems are highly interconnected and involve feedforward and
feedback connections. For example, forward connections from primary
visual area V1, the first cortical visual area, carry information to
higher-level processing areas, and the initial registration of visual
information involves a forward sweep of processing. At the same time,
there are many feedback connections linking visual areas (Felleman
&amp; Van Essen 1991), and later in processing, these connections are
activated yielding dynamic activity within the visual system.</p>

<p>
Lamme identifies four stages of normal visual processing:</p>

<ul>

<li>Stage 1: Superficial feedforward processing: visual signals are
processed locally within the visual system.</li>

<li>Stage 2: Deep feedforward processing: visual signals have
travelled further forward in the processing hierarchy where they can
influence action.</li>

<li>Stage 3: Superficial recurrent processing: information has
traveled back into earlier visual areas, leading to local, recurrent
processing.</li>

<li>Stage 4: Widespread recurrent processing: information activates
widespread areas (and as such is consistent with global workspace
access).</li>
</ul>

<p>
Lamme holds that recurrent processing in Stage 3 is necessary and
sufficient for consciousness. Thus, what it is for a visual state to
be conscious is for a certain recurrent processing state to hold of
the relevant visual circuitry. This identifies the crucial difference
between the global neuronal workspace and recurrent processing theory:
the former holds that recurrent processing at Stage 4 is necessary for
consciousness while the latter holds that recurrent processing at
Stage 3 is sufficient. Thus, recurrent processing theory affirms
phenomenal consciousness without access by the global neuronal
workspace. In that sense, it is an overflow theory (see
 <a href="#AcceConNoRepoPara">section 2.2</a>).</p>
 
<p>
Why think that Stage 3 processing is sufficient for consciousness?
Given that Stage 3 processing is not accessible to introspective
report, we lack introspective evidence for sufficiency. Lamme appeals
to experiments with brief presentation of stimuli such as letters
where subjects are said to report seeing more than they can identify
in report (Lamme 2010). For example, in George Sperling&rsquo;s
partial report paradigm (Sperling 1960), subjects are briefly
presented with an array of 12 letters (e.g., in 300 ms presentations)
but are typically able to report only three to four letters even as
they claim to see more letters (but see Phillips 2011). It is not
clear that this is strong motivation for recurrent processing, since
the very fact that subjects can report seeing more letters shows that
they have some access to them, just not access to letter identity.</p>

<p>
Lamme also presents what he calls <em>neuroscience arguments.</em>
This strategy compares two neural networks, one taken to be sufficient
for consciousness, say the processing at Stage 4 as per Global
Workspace theories, and one where sufficiency is in dispute, say
recurrent activity in Stage 3. Lamme argues that certain features
found in Stage 4 are also found in Stage 3 and given this similarity,
it is reasonable to hold that Stage 3 processing suffices for
consciousness. For example, both stages exhibit recurrent processing.
Global neuronal workspace theorists can allow that recurrent
processing in stage 3 is correlated, even necessary, but deny that
this activity is explanatory in the relevant sense of identifying
sufficient conditions for consciousness.</p>

<p>
It is worth reemphasizing the empirical challenge in testing whether
access is necessary for phenomenal consciousness (sections
 <a href="#IntrRepo">2.1&ndash;2.2</a>).
 The two theories return different answers, one requiring access, the
other denying it. As we saw, the methodological challenge in testing
for the presence of phenomenal consciousness independently of access
remains a hurdle for both theories.</p>

<h3 id="HighOrdeTheo">3.3 Higher-Order Theory</h3>

<p>
A long-standing approach to conscious states holds that one is in a
conscious state if and only if one relevantly represents oneself as
being in such a state. For example, one is in a conscious visual state
of seeing a moving object if and only if one suitably represents
oneself being in that visual state. This higher-order state, in
representing the first-order state that represents the world, results
in the first order state&rsquo;s being conscious as opposed to not.
The intuitive rationale for such theories is that if one were in a
visual state but in no way aware of that state, then the visual state
would not be conscious. Thus, to be in a conscious state, one must be
aware of it, i.e., represent it (Rosenthal 2002; see the entry on
 <a href="../consciousness-higher/">higher order theories of consciousness</a>).
 On certain higher-order theories (Higher-Order Thought Theory,
Rosenthal 2005; and Higher-Order Representation of a Representation
(HOROR), Brown 2015), one can be in a conscious visual state even if
there is no visual system activity, so long as one represents oneself
as being in that state (for a debate, see Block 2011; Rosenthal 2011).
Other family of theories postulates that experiences are jointly
determined by first- and higher-order states [e.g., Higher-Order State
Space (HOSS) (Fleming 2020); Perceptual Reality Monitoring (PRM) (Lau
2019)]. An intermediate perspective proposes that higher-order states
track our mental attitudes towards first-order states along different
dimensions that include familiarity, vividness, value, and so on
(Self-Organizing Meta-Representational Account (SOMA)&mdash;Cleeremans
2011; Cleeremans et al. 2020). These differences apart, higher-order
theories merge with empirical work by tying high-order representations
with brain activity, typically in the prefrontal cortex, which is
taken to be the neural substrate of the required higher-order
representations.</p>

<p>
The focus on the prefrontal cortex allows for empirical tests of the
higher-order theory against other accounts (Lau &amp; Rosenthal 2011;
LeDoux &amp; Brown 2017; Brown, Lau, &amp; LeDoux 2019; Lau 2022). For
example, on the higher-order theory, lesions to prefrontal cortex
should affect consciousness (see Kozuch 2013, 2022, 2023), testing the
necessity of prefrontal cortex for consciousness. Against higher-order
theories, some reports claim that patients with prefrontal cortex
surgically removed maintain preserved perceptual consciousness (Boly
et al. 2017) and that intracranial electrical stimulation (iES) to the
prefrontal cortex does not alter consciousness (Raccah, Block, &amp;
Fox 2021). This would lend support to recurrent processing theories
that hold that prefrontal cortical activity is not necessary for
consciousness (and would be evidence against both GWT and higher-order
theories). It is not clear, however, that the interventions succeeded
in removing <em>all</em> of prefrontal cortex, leaving perhaps
sufficient frontal areas needed to sustain consciousness (Odegaard,
Knight, &amp; Lau 2017), or that simple, localized stimulation to
prefrontal cortex would be the right kind of stimulation for altering
awareness (see Naccache et al. 2021). Bilateral suppression of
prefrontal activity using transcranial magnetic stimulation also seems
to selectively impair visibility as evidenced by metacognitive report
(Rounis et al. 2010). Furthermore, certain syndromes and experimental
manipulations suggest consciousness in the absence of appropriate
sensory processing as predicted by some higher-order accounts (Lau
&amp; Brown 2019), a claim that coheres with the theory&rsquo;s
sufficiency claims.</p>

<p>
Subjective reports of conscious versus unconscious trials activate
frontal regions as shown with EEG (Cul et al. 2009) and fMRI (Lau
&amp; Passingham 2006). Liu and colleagues (Liu et al. 2019) leveraged
the &ldquo;double-drift illusion&rdquo; to show that real and apparent
motion shared patterns of neural activity only in lateral and medial
frontal cortex, not visual cortex. The double-drift illusion is a
dramatic mismatch between physical and apparent motion created by a
patch of gratings moving vertically while the gratings cycle
horizontally; this creates the illusion of the patch&rsquo;s path to
be more than 45&ordm; away from vertical. The conscious experience of
seeing a stimulus veer off diagonally, whether physically or
illusorily, was similarly encoded only in the prefrontal cortex,
suggesting that the conscious experience of stimulus&rsquo;s diagonal
motion was represented there, not in visual cortex. In a carefully
designed no-report paradigm, Hatamimajoumerd and colleagues (2022)
found that conscious stimuli were decodable from prefrontal cortex
well above chance. Intracranial electrophysiological recording, where
electrodes are placed directly on the surface of the brain, reveals
prefrontal activity related to visual consciousness even when subjects
were not required to respond to the stimulus (Noy et al. 2015).
Fazekas and Nemeth (2018) discuss studies using different neuroimaging
techniques showing significant increases in activity in the prefrontal
cortex during dreams, a natural case of phenomenal awareness without
report. Convergent evidence about the role of the prefrontal cortex
sustaining awareness comes from single-cell recordings in macaques.
Using binocular rivalry (see
 <a href="#ContStraBinoRiva">section 5.2</a>),
 Dwarakanath et al. (2023) and Kapoor et al. (2022) show that dynamic
subjective changes in monkeys&rsquo; conscious experiences are
robustly represented in prefrontal cortex.</p>

<h3 id="InfoInteTheo">3.4 Information Integration Theory</h3>

<p>
<em>Information Integration Theory of Consciousness</em> (IIT) draws
on the notion of <em>integrated information</em>, symbolized by &Phi;,
as a way to explain generic consciousness&mdash;specifically the
quantity of consciousness present in a system (Tononi 2004, 2008;
Oizumi, Albantakis, &amp; Tononi 2014; Albantakis et al. 2023). IIT
also aims to explain specific consciousness (i.e., the quality or
content of conscious experiences) by appealing to the conceptual
causal structure of the integrated information complex (i.e., the set
of units of the physical substrate that is maximally integrated).</p>

<p>
Integrated information theory (IIT) &ldquo;starts from phenomenology
and makes use of thought experiments to claim that consciousness is
integrated information&rdquo; (Tononi 2008: 216). IIT&rsquo;s first
step is to find &ldquo;<em>phenomenological</em> axioms&rdquo;, that
is, immediately given essential properties of every conceivable
experience. Once properly understood, these phenomenological axioms
are taken by IIT to be irrefutably true (Albantakis et al. 2023: 3).
These axioms, obtained by drawing on introspection and reason, are
that consciousness exists, and that it is intrinsic, specific,
unitary, definite, and structured. These axioms lead to postulates of
<em>physical</em> existence. In other words, physical implementations
that respect the properties first discovered through introspection.
Finally, IIT develops mathematical formalisms that aim to preserve all
these features and that can in principle help calculate the quality
and quantity of integrated information in a system, or
 &Phi;.<sup>[<a href="notes.html#note-7" id="ref-7">7</a>]</sup></p>
 
<p>
Integrated information is defined in terms of the effective
information carried by the parts of the system in light of its causal
profile. For example, consider a circuit (neuronal, electrical, or
otherwise). We can focus on a part of the whole circuit, say two
connected nodes, and compute the effective information that can be
carried by this microcircuit. The system carries <em>integrated
information</em> if the effective informational content of the whole
is greater than the sum of the informational content of the parts. If
there is no partitioning where the summed informational content of the
parts equals the whole, then the system as a whole carries integrated
information and it has a positive value for &Phi;. Intuitively, the
interaction of the parts adds more to the system than the parts do
alone.</p>

<p>
IIT holds that an above-zero value for &Phi; implies that a system is
conscious, with more consciousness going with greater values for
&Phi;. For example, Tononi argues that the human cerebellum does not
contribute to consciousness due to its highly modular anatomical
organization. Thus, it is hypothesized that the cerebellum has a value
of zero &Phi; despite there being four to five times the number of
neurons in the cerebellum versus in the human cortex. On IIT, what
matters is the presence of appropriate connections and not the number
of neurons (the soundness of this argument about cerebellum has been
contested; Aaronson 2014b; Merker, Williford, &amp; Rudrauf 2022). For
this same reason, IIT also makes counterintuitive predictions. From
panpsychist conclusions, such as admitting that even a 2D grid of
inactivated logic gates &ldquo;doing absolutely nothing, may in fact
have a large value of PHI&rdquo; (Tononi 2014 in
 <a href="#Oth">Other Internet Resources</a>),
 to neuroscientifically implausible conclusions: &ldquo;a brain where
no neurons were activated, but were kept ready to respond in a
differentiated manner to different perturbations, would be conscious
(perhaps that nothing was going on)&rdquo; (Tononi 2004). (In
 <a href="#Oth">Other Internet Resources</a>,
 see Aaronson 2014a and 2014b for striking counterexamples; see Tononi
2014 for a response).</p>

<p>
It is important to note that IIT is not in and of itself a
neuroscientific theory of consciousness. Rather, IIT is probably best
understood as a metaphysical theory about the essential features of
consciousness. Accordingly, these features could be present not just
in organisms with neural systems but in any physical system (organic
or not) that integrates information with a &Phi; larger than 0 (Tononi
et al. 2016). Evidence of its metaphysical status are the
theory&rsquo;s idealist corollaries (see entry for Idealism).
According to Tononi, and in stark contrast to current neuroscientific
assumptions, only intrinsic entities (i.e., conscious entities)
&ldquo;truly exist and truly cause, whereas my neurons or my atoms
neither truly exist nor truly cause&rdquo; (Tononi, et al. 2022: 2
[Other Internet Resources]).  Relatedly, IIT has received criticisms
about its soundness and its neuroscientific status based on both
empirical and theoretical arguments. One notable concern is the lack
of clarity between IIT&rsquo;s metaphysical claims and their potential
relevance for a neuroscientific understanding of
consciousness.<sup>[<a href="notes.html#note-8"
id="ref-8">8</a>]</sup></p>
 
<h3 id="FronPost">3.5 Frontal or Posterior?</h3>

<p>
In recent years, one way to frame the debate between theories of
generic consciousness is whether the &ldquo;front&rdquo; or the
&ldquo;back&rdquo; of the brain is crucial. Using this rough
distinction allows us to draw the following contrasts: Recurrent
processing theories focus on sensory areas (in vision, the
&ldquo;back&rdquo; of the brain) such that where processing achieves a
certain recurrent state, the relevant contents are conscious even if
no higher-order thought is formed or no content enters the global
workspace. Similarly, proponents of IIT have recently emphasized a
&ldquo;posterior hot zone&rdquo; covering parietal and occipital areas
as a neural correlate for consciousness, as they speculate that this
zone may have the highest value for &Phi; (Boly et al. 2017; but see
Lau 2023). For certain higher-order thought theories, having a
higher-order state, supported by prefrontal cortex, without
corresponding sensory states can suffice for conscious states. In this
case, the front of the brain would be sufficient for consciousness.
Finally, the global neuronal workspace, drawing on workspace neurons
that are present across brain areas to form the workspace, might be
taken to straddle the difference, depending on the type of conscious
state involved. They require entry into the global workspace such that
neither sensory activity nor a higher order thought on its own is
sufficient, i.e., neither just the front nor the back of the
brain.</p>

<p>
The point of talking coarsely of brain anatomy in this way is to
highlight the neural focus of each theory and thus, of targets of
manipulation as we aim for explanatory neural correlates in terms of
what is necessary and/or sufficient for generic consciousness. What is
clear is that once theories make concrete predictions of brain areas
involved in generic consciousness, neuroscience can test them.</p>

<h2 id="NeurGeneConsUncoVisiCaseStud">4. Neuroscience of Generic Consciousness: Unconscious Vision as Case Study</h2>

<p>
Since generic consciousness is a matter of a state&rsquo;s being
conscious or not, we can examine work on specific types of mental
state that shift between being conscious or not and isolate neural
substrates. Work on unconscious vision provides an informative
example. In the past decades, scientists have argued for unconscious
seeing and investigated its brain basis especially in
<em>neuropsychology</em>, the study of subjects with brain damage.
Interestingly, if there is unconscious seeing, then the intentional
action inference must be restricted in scope since some intentional
behaviors might be guided by unconscious perception
 (<a href="#InteActiInfe">section 2.4</a>).
 That is, the existence of unconscious perception blocks a direct
inference from perceptually guided intentional behavior to perceptual
consciousness. The case study of unconscious vision promises to
illuminate more specific studies of generic consciousness along with
having repercussions for how we attribute conscious states.</p>

<h3 id="UncoVisiTwoVisuStre">4.1 Unconscious Vision and the Two Visual Streams</h3>

<p>
Since the groundbreaking work of Leslie Ungerleider &amp; Mortimer
Mishkin (1982), scientists divide primate cortical vision into two
streams: dorsal and ventral (for further dissection, see Kravitz et
al. 2011). The dorsal stream projects into the parietal lobe while the
ventral stream projects into the temporal lobe (see Figure 1).
Controversy surrounds the functions of the streams. Ungerleider and
Mishkin originally argued that the streams were functionally divided
in terms of <em>what</em> and <em>where</em>: the ventral stream for
categorical perception and the dorsal stream for spatial perception.
David Milner and Melvyn Goodale (1995) have argued that the dorsal
stream is for <em>action</em> and the ventral stream for
&ldquo;<em>perception</em>&rdquo;, namely for guiding thought, memory
and complex action planning (see Goodale &amp; Milner 2004 for an
engaging overview). There continues to be debate surrounding the
Milner and Goodale account (Schenk &amp; McIntosh 2010) but it has
strongly influenced philosophers of mind.</p>

<p>
Substantial motivation for Milner and Goodale&rsquo;s division draws
on lesion studies in humans. Lesions to the dorsal stream do not seem
to affect conscious vision in that subjects are able to provide
accurate reports of what they see (but see Wu 2014a). Rather, dorsal
lesions can affect visual-guidance of action with <em>optic
ataxia</em> being a common result. Optic ataxic subjects perform
inaccurate motor actions. For example, they grope for objects, yet
they can accurately report the object&rsquo;s features (for reviews,
see Andersen et al. 2014; Pisella et al. 2009; Rossetti, Pisella,
&amp; Vighetto 2003). Lesions in the ventral stream disrupt normal
conscious vision, yielding visual agnosia, an inability to see visual
form or to visually categorize objects (Farah 2004).</p>

<p>
Dorsal stream processing is said to be unconscious. If the dorsal
stream is critical in the visual guidance of many motor actions such
as reaching and grasping, then those actions would be guided by
unconscious visual states. The visual agnosic patient DF provides
critical support for this
 claim.<sup>[<a href="notes.html#note-9" id="ref-9">9</a>]</sup>
 Due to carbon monoxide poisoning, DF suffered focal lesions largely
in the ventral stream spanning the <em>lateral occipital complex</em>
that is associated with processing of visual form (high resolution
imaging also reveals small lesions in the parietal lobe; James et al.
2003). Like other visual agnosics with similar lesions, DF is at
chance in reporting aspects of form, say the orientation of a line or
the shape of objects. Nevertheless, she retains color and texture
vision. Strikingly, DF can generate accurate visually guided action,
say the manipulation of objects along specific parameters: putting an
object through a slot or reaching for and grasping round stones in a
way sensitive to their center of mass. Simultaneously, DF denies
seeing the relevant features and, if asked to verbally report them,
she is at chance. In this dissociation, DF&rsquo;s verbal reports give
evidence that she does not visually experience the features to which
her motor actions remain sensitive.</p>

<p>
What is uncontroversial is that there is a division in explanatory
neural correlates of visually guided behavior with the dorsal stream
weighted towards the visual guidance of motor movements and the
ventral stream weighted towards the visual guidance of conceptual
behavior such as report and reasoning (see section
 <a href="#NeurStim">5.3</a>
 on manipulation of seeing words via ventral stream stimulation). A
substantial further inference is that consciousness is segregated away
from the dorsal stream to the ventral stream. How strong is this
inference?</p>

<p>
Recall the intentional action inference. In performing the slot task,
DF is doing something intentionally and in a visually guided way. For
control subjects performing the task, we conclude that this visually
guided behavior is guided by conscious vision. Indeed, a
folk-psychological assumption might be that consciousness informs
mundane action (Clark 2001; for a different perspective see Wallhagen
2007). Since DF shows similar performance on the same task, why not
conclude that she is also visually conscious? Presumably, one
hesitates because DF&rsquo;s introspective reports clash with the
intentional action inference. DF denies seeing features she is
visually sensitive to in action. Should introspection then trump
intentional action in attributing consciousness?</p>

<p>
Two issues are worth considering. The first is that introspective
reports involve a specific type of intentional action guided by the
experience at issue. One type of intentional behavior is being
prioritized over another in adjudicating whether a subject is
conscious. What is the empirical justification for this
prioritization? The second issue is that DF is possibly unique among
visual agnosics. It is a substantial inference to move from DF to a
general claim about the dorsal stream being unconscious in
neurotypical individuals (see Mole 2009 for arguments that
consciousness does not divide between the streams; see Wu 2013 for an
argument for unconscious visually guided action in normal subjects).
What this shows is that the methodological decisions that we make
regarding how we track consciousness are substantial in theorizing
about the neural bases of conscious and unconscious vision.</p>

<h3 id="Blin">4.2 Blindsight</h3>

<p>
Two issues are worth considering. The first is that introspective
reports involve a specific type of intentional action guided by the
experience at issue. One type of intentional behavior is being
prioritized over another in adjudicating whether a subject is
conscious. What is the empirical justification for this
prioritization? The second issue is that DF is possibly unique among
visual agnosics. It is a substantial inference to move from DF to a
general claim about the dorsal stream being unconscious in
neurotypical individuals (see Mole 2009 for arguments that
consciousness does not divide between the streams; see Wu 2013 for an
argument for unconscious visually guided action in normal subjects).
What this shows is that the methodological decisions that we make
regarding how we track consciousness are substantial in theorizing
about the neural bases of conscious and unconscious
 vision.<sup>[<a href="notes.html#note-10" id="ref-10">10</a>]</sup></p>
 
<p>
The neuroanatomical basis of blindsight capacities remains unclear.
Certainly, the loss of V1 deprives later cortical visual areas of a
normal source of visual information. Still, there are other ways that
information from the eye bypasses V1 to provide inputs to later visual
areas. Alternative pathways include the superior colliculus (SC), the
lateral geniculate nucleus (LGN) in the thalamus, and the pulvinar as
likely sources.</p>

<div class="figure" id="fig3">
<img alt="see legend, to the left is a collection of 5 ovals labeled 'retina', 'LGN', 'pulvinar', 'SC', 'amygdala'; to the right are 5 rectangles labeled 'V1' through 'V5'. Above V3 and to its left, V5, which are on the top is a black arrow going right to left labeled 'dorsal stream'; below V1 and to its left V4 is a black arrow going right to left labeled 'ventral stream'. A thick orange arrow connects retina to LGN to V1 and thick blue arrows connect V1 to V2 then from V2 to both V3 and V4 and from V3 to V5. Thin blue arrows connect V1 to V4, V5, and V3 and well as V2 to V5. Thin orange arrows connect retina to pulvinar and SC ; LGN to SC, V5, V2, and V4; SC to LGN, pulvinar, and amygdala; pulvinar to V5, V2, and amygdala." src="figure3.png" style="width:500px" />

<p>
<span class="figlabel">Figure 3. Subcortical Pathways and their
Connection to Cortical Vision (from Urbanski, Coubard, &amp; Bourlon
2014)</span></p>

<p>
Figure Legend: The front of the head is to the left, the back of the
head is to the right. One should imagine that the blue-linked regions
are above the orange-linked regions, cortex above subcortex. V4 is
assigned to the base of the ventral stream; V5, called area MT in
nonhuman primates, is assigned to the base of the dorsal stream.</p>
</div>

<p>
The latter two have direct <em>extrastriate</em> projections
(projections to visual areas in the occipital lobe outside of V1)
while the superior colliculus synapses onto neurons in the LGN and
pulvinar which then connect to extrastriate areas (Figure 3). Which of
these provide for the basis for blindsight remains an open question
though all pathways might play some role (Cowey 2010; Leopold 2012).
If blindsight involves nonphenomenal, unconscious vision, then these
pathways would be a substrate for it, and a functioning V1 might be
necessary for normal conscious vision.</p>

<p>
Campion et al. (1983) raised an important alternative explanation:
blindsight subjects in fact have severely degraded conscious vision
but merely report on them with low confidence (see Phillips 2021 for a
recent version of this critique; see Michel &amp; Lau 2021 for a
response). In their reports, blindsight subjects feel like they are
guessing about stimuli they can objectively discriminate. Campion et
al. drew on <em>signal detection theory</em>, which emphasizes two
determinants of detection behavior: <em>perceptual sensitivity</em>
and <em>response criterion</em> (see
 <a href="#IntrRepo">section 2.1</a>).
 Campion et al. hypothesized that blindsight patients are conscious in
that they are aware of visual signal where discriminability is low.
Further, blindsight patients are more conservative in their response
so will be apt to report the absence of a signal by saying that they
do not see the relevant stimulus even though the signal is there, and
they can detect it, as verified by their above chance visually guided
behavior.</p>

<p>
This possibility was explicitly tested by Azzopardi and Cowey (1997)
with the well-studied blindsight patient, GY. They compared blindsight
performance with normal subjects at threshold vision using signal
detection measures and found that with respect to motion stimuli, the
difference between discrimination and detection used to argue for
blindsight can be explained by changes in response criterion, as
Campion et al. hypothesized. That is, GYs claim that he does not see
the stimulus is due to a conservative criterion and not to a detection
incapacity. Interestingly, for static stimuli, his response criterion
did not change but his sensitivity did, as if he was tapping into two
different visual processing mechanisms in each task (for an
alternative explanation based on shifting response criterion, see Ko
&amp; Lau 2012).</p>

<p>
In introspecting, what concepts are available to subjects will
determine their sensitivity in report. In many studies with
blindsight, subjects are given a binary option: do you see the
stimulus or do you not see it? The concern is that the <em>do not
see</em> option would cover cases of degraded consciousness that
subjects might be unwilling to classify as seeing due to a
conservative response criterion. So, what if subjects are given more
options for report? Rams&oslash;y and Overgaard (2004; see also
Overgaard et al. 2006) provided subjects with four categories for
introspective report: <em>no experience; brief glimpse; almost clear
experience; clear experience.</em> Using this <em>perceptual awareness
scale</em>, they found that subjects&rsquo; objective performance
tracked their introspective reports where performance was at chance
when subjects reported no visual experience. As visibility increased,
so did performance. When the scale was used with a blindsight patient
(Overgaard et al. 2008), no above chance performance was detected when
the subject reported no visual experience (see also Mazzi, Bagattini,
&amp; Savazzi 2016 for further evidence). A live alternative
hypothesis is that blindsight does not present a case of unconscious
vision, but of degraded conscious vision with a conservative response
bias that affects introspection. At the very least, the issue depends
on how introspection is deployed, a topic that deserves further
attention (see Phillips 2016, 2021 for further discussion of
blindsight).</p>

<h3 id="UncoVisiInteActiInfe">4.3 Unconscious Vision and the Intentional Action Inference</h3>

<p>
Blindsight and DF show that damage to specific regions of the brain
disrupts normal visual processing, yet subjects can access visual
information in preserved visual circuits to inform behavior despite
failing to report on the relevant visual contents. The received view
is that these subjects demonstrate unconscious vision. One implication
is that the normal processing in the ventral stream, tied to normal V1
activity, plays a necessary role in normal conscious vision. Another
is that dorsal stream processing or visual stream processing that
bypasses V1 via subcortical processing yields only unconscious visual
states. This points to a set of networks that begin to provide an
answer to what makes visual states conscious or not. An important
further step will be to integrate these results with the general
theories noted earlier
 (<a href="#NeurTheoCons">section 3</a>).</p>
 
<p>
Still, the complexities of the empirical data bring us back to
methodological issues about tracking consciousness and the following
question: What behavioral data should form the basis of attributions
of phenomenal consciousness? The intentional action inference is used
in a variety of cases to attribute conscious states, yet the results
of the previous sections counsel us to be wary of applying that
inference widely. After all, some intentional behavior might be
unconsciously guided.</p>

<p>
In the case of DF, we noted that unlike many other visual agnosics,
she can direct motor actions towards stimuli that she cannot
explicitly report and which she denies seeing. In her case, we
prioritize introspective reports over intentional action as evidence
for unconscious vision. Yet, one might take a broader view that vision
for action is always conscious and that what DF vividly illustrates is
that some visual contents (dorsal stream) are tied directly to
performance of intentional motor behavior and are not directly
available to conceptual capacities deployed in report. In contrast,
other aspects of conscious vision, supported by the ventral stream,
are directly available to guide reports. This functional divergence is
explained by the anatomical division in cortical visual
processing.</p>

<p>
For some time now, these striking cases have been taken as clear cases
of unconscious vision and if this hypothesis is correct, the work has
begun to identify visual areas critical for creating seeing, sometimes
conscious and sometimes not. The neuroanatomy demonstrates that
visually-guided behavior has a complex neural basis involving cortical
and subcortical structures that demonstrate a substantial level of
specialization. Understanding consciousness and unconsciousness in
vision will need to be sensitive to the complexities of the underlying
neural substrate.</p>

<h2 id="SpecCons">5. Specific Consciousness</h2>

<p>
We turn to experimental work on specific consciousness:</p>

<blockquote>

<p>
<em>Specific Consciousness:</em> What neural states or properties are
necessary and/or sufficient for a conscious perceptual state to have
content <em>X</em> rather than <em>Y</em>?</p>
</blockquote>

<p>
In this section, we examine attempts to address claims about necessity
and sufficiency by manipulation of the contents of consciousness
through direct modulation of neural representational content.</p>

<h3 id="NeurRepr">5.1 Neural Representationalism</h3>

<p>
In thinking about neural explanations of specific consciousness,
namely the contents of consciousness, we will provisionally assume a
type of first-order representationalism about phenomenal content,
namely that such content supervenes on neural content (see the entry
on
 <a href="../consciousness-representational/">representational theories of consciousness</a>).
 One strong position would be that phenomenal content is
<em>identical</em> to appropriate neural content. A weaker correlation
claim affirms only supervenience: no change in phenomenal content
without a change in neural content. This neural representationalism
allows us to link phenomenal properties to the brain via linking
neural contents to perceptual contents.</p>

<p>
In invoking neural content, the assumption is that neural content
mirrors perceptual content, so that if one is experiencing dots moving
in a certain direction, there is a neural representation with the same
content. This is a simplification and does not cohere with a common
current approach to neural content that takes it to be
<em>probabilistic</em> (for accessible discussions, see Pouget, Dayan,
&amp; Zemel 2003; Colombo &amp; Seri&egrave;s 2012; Rescorla 2015).
Yet perceptual content does not <em>seem</em> probabilistic. This
emphasizes a <em>prima facie</em> disconnect between current theories
of neural content and those of phenomenal content. One option is to
find nonprobabilistic content at the neural level. The other is to
find probabilistic content at the phenomenal level (for related ideas,
see Morrison 2016 and a response by; Denison 2017; and Beck 2020; also
Munton 2016; Block 2018; Shea 2018; Gross 2020; Vance 2021; Siegel
2022; Lee &amp; Orlandi 2022). For simplicity, in what follows we
assume a simple mapping between neural content and perceptual
experiences.</p>

<h3 id="ContStraBinoRiva">5.2 The Contrast Strategy: Binocular Rivalry</h3>

<p>
A common approach, the <em>contrast strategy</em>, enjoins
experimentalists to identify relevant correlates for some phenomenon
<em>P</em> by contrasting cases where <em>P</em> is present from cases
where <em>P</em> is not. Work on binocular rivalry illustrates this
strategy (among many reviews, see Tong, Meng, &amp; Blake 2006; Blake,
Brascamp, &amp; Heeger 2014; Blake 2022). When each eye receives a
different image simultaneously, the subject does not see both, say one
stimulus overlapping the other. Rather, visual experience alternates
between them. Call this <em>phenomenal alternation</em>. An initial
restatement of our question about specific consciousness in respect of
binocular rivalry is:</p>

<blockquote>

<p>
<em>Specific Rivalry</em>: What neural property is necessary and/or
sufficient for phenomenal alternation in binocular rivalry in
condition <em>C</em>?</p>
</blockquote>

<p>
That is, empirical theories aim to explain how visual content
alternates in binocular
 rivalry.<sup>[<a href="notes.html#note-11" id="ref-11">11</a>]</sup>
 Notice that this is a question about specific rather than generic
consciousness, as the contrast is not between a state&rsquo;s being
conscious versus not but about the contrast between two conscious
states with different contents</p>

<p>
Neural explanations of binocular rivalry concern competition at some
level of visual processing: (a) &ldquo;interocular&rdquo; competition
between <em>monocular</em> neurons early in the visual system, namely
visual neurons that receive input from only one eye or (b) competition
between <em>binocular</em> neurons later in the visual system, namely
neurons that receive input from both eyes. The winner of competition
fixes which stimulus the subject experiences at a given time. Some
imaging studies in humans suggest that neural activity in V1 did
correlate with alternation of the experienced images. For example,
Polonsky et al. used fMRI to demonstrate that V1 activity to competing
stimuli tracked perception (Polonsky et al. 2000; but see Maier et al.
2008). In contrast, some of the earliest electrophysiological studies
(Leopold &amp; Logothetis 1996; Logothetis, Leopold, &amp; Sheinberg
1996; see also Hesse &amp; Tsao 2020) on awake behaving monkeys
supported later binocular processing as the neural basis of binocular
rivalry. Processing in later (inferotemporal cortex, IT; see
 <a href="#fig1">Figure 1</a>)
 rather than earlier visual areas (V1 or V2) was observed to be best
correlated to the monkey&rsquo;s reported perception based on the
monkey&rsquo;s stimuli-specific response.</p>

<p>
Recent accounts have taken binocular rivalry as resulting from
processes at multiple levels (Wilson 2003; Freeman 2005; Tong, Meng,
&amp; Blake 2006). For example, when the two competing stimuli have
parts that can be fused into a coherent stimulus, as when half of a
picture is presented to each eye, the subject can perceive the fusion,
integrating content from each eye (Kov&aacute;cs et al. 1996; Ngo et
al. 2000). This suggests that binocular rivalry can be sensitive to
global properties of the stimulus (see Baker &amp; Graf 2009). What
unifies the mechanisms, perhaps, is the function of resolving a
conflict generated by the stimuli.</p>

<p>
Assume that some neural process <em>R</em> resolves interocular
competition: when <em>R</em> resolves competition between stimuli
<em>X</em> and <em>Y</em> in favor of <em>X</em>, then the subject is
phenomenally conscious of <em>X</em> rather than <em>Y</em> and vice
versa. Notice that <em>R</em> has the same &ldquo;gating&rdquo;
function for any stimuli <em>X</em> and <em>Y</em> that are subject to
binocular rivalry. So, while the presence of <em>R</em> can explain
why the subject is having one conscious visual experience rather than
another, <em>R</em> is not tied to a specific content. This suggests
that in answering the question about rivalry, we will at best be
identifying a necessary but not sufficient condition for a conscious
visual state having a content <em>X</em>. <em>R</em> is a general gate
for consciousness (cf. attention in global workspace theory).</p>

<p>
An example of this gating of awareness can be found in recent studies
using no-report paradigms, in which analysis of local field potentials
in macaque prefrontal cortex has shown that transitions in frequency
field activity track transitions in experience and encoding of the
contents of conscious experiences (Dwarakanath et al. 2023). Moreover,
feature-selective neurons in the prefrontal cortex are modulated by
subjective changes in experiences (regardless if these were due to
physical changes or rivalry). This modulation allows trial-by-trial
successful decoding of the contents of experience from prefrontal
cortex (Kapoor et al. 2022). Together, these recent studies offer
robust evidence for a causal role of prefrontal cortex in supporting
conscious experiences.</p>

<p>
A narrower explanation of specific consciousness would identify the
specific neural representations that explain a conscious state&rsquo;s
having the specific content <em>X</em> (rather than <em>Y</em>). By
the representationalism assumption, this will involve identifying
neural representations with the same content, <em>X</em>. Focusing on
a gate in explaining alternation in rivalry stops short of identifying
those representations. Still, binocular rivalry can provide a useful
method for isolating neural populations that carry relevant content.
In principle, for any stimulus type of interest, <em>X</em> (e.g.,
faces, words, etc.), so long as <em>X</em> is subject to binocular
rivalry, we can use rivalry paradigms to isolate brain areas that
carry the relevant information that correlate with the subject&rsquo;s
perceiving <em>X</em>. That would allow us to identify potential
candidates for the neural basis of conscious content. (For a criticism
of binocular rivalry paradigms as a means for discovering the neural
basis of perceptual awareness&mdash;as opposed to mere perceptual
processing&mdash;see Zou, He, &amp; Zhang 2016; Giles, Lau, &amp;
Odegaard 2016).</p>

<h3 id="NeurStim">5.3 Neural Stimulation</h3>

<p>
There are limited opportunities to manipulate human brain activity in
a targeted way. Recent use of transcranial magnetic stimulation to
activate or suppress neural activity has provided illumination, but
such interventions are coarse-grained. Ultimately, to locate an
explanatory correlate for specific conscious contents, we will need
more fine-grained interventions in brain tissue. In humans, such
opportunities are generally confined to manipulation before surgical
interventions, say for brain tumors or epilepsy.</p>

<p>
In the middle of the last century, neurosurgeon Wilder Penfield and
colleagues performed a set of direct electrical microstimulations
during preoperative procedures (Penfield &amp; Perot 1963), and in
certain cases induced hallucinations by stimulating primary sensory
cortices such as V1 or S1 (see
 <a href="#fig1">Figure 1</a>).
 This provided evidence that endogenous activity could be causally
sufficient for phenomenal experiences. Penfield&rsquo;s interventions,
however, were not based on fine-grained targeting of specific neural
representations. As Cohen and Newsome note:</p>

<blockquote>

<p>
Penfield&rsquo;s approach failed to generate substantial new insights
into the neural basis of perception and cognition&hellip;because the
gross electrical activation elicited by surface electrodes could not
be related mechanistically to the information being processed within
the excited neural tissue. (Cohen &amp; Newsome 2004: 1)</p>
</blockquote>

<p>
A different approach begins with a more detailed understanding of
underlying neural representations tied to different brain regions
(Figure 4). For example, the fusiform face (FFA) area appears to be
necessary for normal human face experience in that lesions in FFA lead
to <em>prosopagnosia</em>, the inability to see faces even if one can
see their parts [whether FFA is necessary for seeing faces
specifically, or more generally for visual expertise is a point of
contention (Kanwisher 2000; Tarr &amp; Gauthier 2000)]. FFA is part of
a larger network that is important in visual processing of faces
(Behrmann &amp; Plaut 2013). Recently, microstimulation of FFA in an
awake human epilepsy patient induced visual distortions of actual
faces as opposed to other objects (Parvizi et al. 2012). Alterations
of visual experience were also reported during microstimulating the
parahippocampal place (PPA) area in an awake pre-operative epileptic
patient that induced visual hallucinations of scenes (M&eacute;gevand
et al. 2014). PPA is the same area that showed activation in patients
with unresponsive wakefulness syndrome when they putatively imagined
walking around their home
 (<a href="#InteActiInfe">section 2.4</a>).</p>
 
<div class="figure" id="fig4">
<img alt="See legend. The cortex picture is labeled at the top as 'Anterior' and the bottom as 'Posterior', the left is labeled 'RH' and the right is labeled 'LH'. A picture of a face is labeled 'FFA' in red with and arrow to a region in red of the cortex picture on the far left and about a third up from the bottom, other regions in red are on both sides of the cortex and to the top and about half way up on the left. A picture of a cup is labeled 'LO' in blue and points to a largish blue region in the lower right hand side of the cortex picture; another largish blue region is on the lower left hand side of the picture. A picture of a house is labeled 'PPA' in green with arrows pointing to two smallish green regions above and slightly overlapping the two blue regions" src="figure4.png" style="width:432px" />

<p>
<span class="figlabel">Figure 4. Ventral Stream Areas </span></p>

<p>
A view from the bottom of cortex with location of areas FFA, PPA and
LO identified. Occipital cortex is on the bottom. LO is lesioned in
the visual agnosic patient, DF (see
 <a href="#UncoVisiTwoVisuStre">section 4.1</a>).
 This figure is modified from figure 1 of Behrmann and Plaut 2013,
kindly provided by Marlene Behrmann and used with her permission.</p>
</div>

<p>
<br />
Another successful example of target microstimulation involves the
visual word form area. This region in the left midfusiform gyrus
(lmFG) is important for normal processing of visual word forms during
reading. Microstimulation in human epileptic patients selectively
disrupted word and letter reading&mdash;presumably altering the visual
experience of letters&mdash;without changing general form perception
(Hirshorn et al. 2016) (see Movies
 <a href="#S1">S1</a>
 and
 <a href="#S2">S2</a>
 in
 <a href="#Oth">Other Internet Resources</a>).</p>
 
<p>
It is worth noting that many neuroscientists of vision take themselves
to be investigating seeing in the ordinary sense, one that implies
consciousness, but very few of them would characterize their work as
about consciousness. That said, their work is of direct relevance to
our understanding of specific consciousness even if it is not always
characterized as
 such.<sup>[<a href="notes.html#note-12" id="ref-12">12</a>]</sup></p>
 
<p>
An important approach in visual neuroscience was articulated by A.J.
Parker and William Newsome in &ldquo;Sense and the Single
Neuron&rdquo; (1998) via &ldquo;principles&rdquo; to connect
electrophysiological data about information processing to perception
(for a recent discussion, see Ruff &amp; Cohen 2014). To probe the
neural basis of perception, neuroscientists need to explanatorily link
neural data to the subject&rsquo;s perception that guides behavior.
The experimenter must ensure that recorded neural content correlates
with perceptual content and not just response. Further, manipulation
of the neurons carrying information should affect perception: inducing
appropriate neural activity should shift perceptual response while
abolishing or reducing that activity should eliminate or reduce
perceptual response as measured in behavior. These proposals address
concerns about necessity and sufficiency.</p>

<p>
The intentional action inference is applicable (or at least its
evidential version):</p>

<blockquote>

<p>
If some subject acts intentionally, where her action is guided by a
perceptual state, then that state is phenomenally conscious.</p>
</blockquote>

<p>
We will consider the strength of this inference in two cases. The
first case, visual motion perception, introduces the principles that
guide the manipulation of neural content while the second case
concerns tactile experience of vibration. These cases involve
experiments with non-human primates, so we lack introspective reports.
But direct manipulation of the human brain along with introspective
reports have been performed too as discussed above with respect to
face and letter perception.</p>

<p>
These experiments involve microstimulation of small populations of
neurons that are targeted precisely because of their informational
content. Microstimulation involves injecting a small current from the
tip of an electrode inserted into brain tissue that directly
stimulates nearby neurons or, through synaptic connections to other
neurons, indirectly activates more distant neurons (see Histed, Ni,
&amp; Maunsell 2013 for a review). It is assumed that neurons
<em>tuned</em> in similar ways, that is neurons that respond to
similar stimuli, tend to be interconnected, so microstimulation is
taken to largely drive similarly tuned neurons.</p>

<h4 id="VisuMotiPerc">5.3.1 Visual Motion Perception</h4>

<p>
We begin with visual motion perception in primates. Since the
principles introduced here are central to much perceptual neuroscience
and provide the basis for probing the link between neural
representations and perceptual content, I examine it carefully. The
salient question will be whether conscious experience is changed by
the manipulations.</p>

<p>
The work we shall discuss was done in awake behaving macaque monkeys.
Visual area MT in the monkey brain (called V5 in humans) plays an
important role in the visual experience of motion. MT is taken to lie
in the dorsal visual stream
 (<a href="#fig1">Figure 1</a>).
 Lesions that disrupt MT are known to cause <em>akinetopsia</em>, the
inability to see motion. One patient with an MT (V5) lesion reported
the following phenomenology: &ldquo;people were suddenly here or there
but I have not seen them moving&rdquo; (Zihl, von Cramon, &amp; Mai
1983: 315). MT processing looks to be necessary for normal visual
motion experience. Furthermore, MT neurons represent (carry
information regarding) the direction of motion of visible stimuli: MT
neurons are <em>tuned</em> for motion in specific directions with the
highest firing rate for a specific direction of motion (for other
functions and responses of MT, see Born &amp; Bradley 2005). By
placing motion stimuli in a neuron&rsquo;s receptive field, scientists
can map its tuning:</p>

<div class="figure" id="fig5">
<img alt="a graph with a y-axis going from 0 to 120 and labeled 'Responses (spikes/s) and a x-axis from -180 to 180 labeled 'Directions of motion (deg)'. There are three lines on the graph: a horizontal dot-dash line almost at the 0 point of the y-axis; a solid line starting about the 15 point of the y-axis rising in a curve to about 90 at the 0 point of the x-axis before symmetrically falling back to 15 at the 180 point of the x-axis; a dashed line that starts and and ends at the same points as the solid line but rises higher to about 110 at the 0 point of x-axis." src="figure5.png" style="width:400px" />

<p>
<span class="figlabel">Figure 5.</span> MT Neuron Tuning Curve</p>

<p>
Figure Legend: Tuning of a neuron in MT showing a peak response in
spiking rate at 0 degrees of motion. The dashed curve is generated
when the animal is attending to the motion stimulus while it is in the
receptive field (we shall not discuss the neural basis of attention,
but see Wu 2014b: chap. 2 for a summary of the neuroscience of
attention). The solid curve shows MT response when the animal is not
attending to the motion stimulus in the receptive field. Figure from
(Lee &amp; Maunsell 2009).</p>
</div>

<p>
What is plotted is the activity of an MT neuron, in spikes per second,
to a specific type of motion stimulus placed within its receptive
field. How to relate a tuning curve to a determinate content is
complicated. Since the neural response is not simply to one stimulus
value, it is not obvious that the neuron should be taken to represent
0 degrees of motion, namely the value at its peak response. Indeed,
theorists have noted that the tuning curve looks like a probability
density function, and many now take neurons to have
<em>probabilistic</em> content
 (<a href="#NeurRepr">section 5.1</a>).</p>
 
<p>
Experimenters have trained macaque monkeys to perform discrimination
tasks reporting direction of motion. Typically, the monkey maintains
fixation while the moving stimulus is placed within the receptive
field of the recorded neuron. The monkey reports the direction of the
stimulus by moving its eyes to a target that stands for either
leftward or rightward motion (other behavioral reports can be
generated such as moving a joystick). Provisionally, we apply the
intentional action inference, so we assume that such reports are
guided by conscious visual experience of the stimuli. Thus,
<em>changes in behavior will be evidence for changes in conscious
content.</em></p>

<p>
Early work suggested that the activity of a single neuron provides a
strong correlate of the animal&rsquo;s visually guided performance.
This can be seen by plotting both the animal and the neuron&rsquo;s
performance across different stimulus values. In these experiments,
the value concerns the percent <em>coherence</em> of motion of a set
of dots defined as the number of dots moving in the same direction (0%
coherence being random motion; 100% being all dots moving in the same
direction). In the first case, we construct a <em>psychometric
curve</em> that plots the animal&rsquo;s percent correct reports
relative to percent coherence of motion of the stimuli. As one might
expect, percent correct reports drop as coherence drops, and the
inflection point reflects where the subject is equally likely to
indicate left or right motion. We can do the same for the neural
activity of the neuron across the same stimulus values, a
<em>neurometric curve</em>.</p>

<p>
The experimentalist&rsquo;s window onto conscious experience is
through behavior, the assumption being that report about motion
correlates with perceptual experience. Correlation is assessed by
asking the following question: would an ideal observer, using the
activity of the neuron in question, be able to <em>predict</em> the
animal&rsquo;s visually guided performance? Essentially, do the
psychometric and neurometric curves overlap? Strikingly, yes. MT
neurons were observed to predict the animal&rsquo;s behavior (Britten
et al. 1992).</p>

<div class="figure" id="fig6">
<img alt="A graph labeled 'Neurometric and psychometric curves' with a y-axis going from 0.5 to 1 and labeled 'Proposition correct' and a x-axis labeled 'Motion coherence (%)' going from an unknown point to 10 at the halfway point and 100 at the far right. An elongated S curve goes from the lower left (about 0.5 on the y and a quarter of the way from the origin to 10 on the x) to 1 on the y-axis and 100 on the x-axis. A close look shows there are two lines, one black which seems to end before the other, gray, line. The legend says the black line is 'psychometric' and the gray line 'neurometric'." src="figure6.png" style="width:416px" />

<p>
<span class="figlabel">Figure 6. Psychometric and Neurometric
Curves</span></p>

<p>
Figure Legend: Psychometric and neurometric curves for a single MT
neuron during performance of a motion direction detection task.
Percent correct performance is plotted on the y-axis while percent
motion coherence is plotted in a log scale on the x-axis. Figure
modified from Ruff &amp; Cohen 2014 and kindly provided by Doug
Ruff.</p>
</div>

<p>
This shows that the activity of a single MT neuron provides a neural
correlate of the animal&rsquo;s visual discrimination of motion. Note
that this is just a neural correlate of behavior. No one suggested
that this neuron was causally sufficient for the behavior or for
perception. Later results have suggested that individual neurons are
not quite as sensitive as Britten suggested, but that small groups of
MT neurons are sufficient to predict behavior (Cohen &amp; Newsome
2009).</p>

<p>
Earlier, we worried about mere correlates. To get causal or
explanatory purchase, the content of the MT neurons correlated to the
animal&rsquo;s behavior must be shown to contribute to perceptual
guidance. This predicts that if we manipulate the content of the
neurons, i.e., manipulate neural representations, then we should
manipulate the content of the animal&rsquo;s visual experience of
motion as reflected by predicted changes in behavior. This would be to
test sufficiency with respect to specific consciousness.</p>

<p>
Newsome and colleagues demonstrated that microstimulation of MT
neurons shifted the animal&rsquo;s performance in predictable ways.
Assume that neural population <em>P</em>, by encoding information
about stimulus motion, can inform the subject&rsquo;s report of motion
direction. This information is accessible for the control of behavior.
Activation of <em>P</em> by microstimulation should shift behavior in
a motion selective way correlated with the direction that <em>P</em>
is tuned to (represents). This was first demonstrated by Salzman et
al. (1990). They inserted electrodes into MT and identified neurons
tuned to a particular orientation. During a motion discrimination
task, microstimulation of neurons with that tuning led to a shift in
the psychometric curve as if that neuron was given more weight in
driving behavior.</p>

<p>
In conditions of microstimulation relative to its absence, the monkey
was more likely to report that there was motion in the stimulated
neuron&rsquo;s preferred direction. In the original experiment, the
psychophysical effect of microstimulation was equivalent to the
addition of 7&ndash;20% coherence in the stimulus with respect to the
neuron&rsquo;s preferred direction, depending on the experimental
conditions. Further, as a test of necessity, a selective lesion of MT
disrupted motion discrimination though the animals were able to
recover some function suggesting that other visual information streams
could be tapped so as to support performance (Newsome &amp;
Par&eacute; 1988).</p>

<p>
Adopting the intentional action inference, one can conclude that the
microstimulation shifted perceptual content (or again, that we have
good evidence for this shift). That said, given our discussion of
unconscious vision
 (<a href="#NeurGeneConsUncoVisiCaseStud">section 4</a>),
 another possibility is that MT microstimulation only changes
unconscious visual representations. Newsome himself asked:</p>

<blockquote>

<p>
What is the conscious experience that accompanies the stimulation and
the monkey&rsquo;s decision? Even if you knew everything about how the
neurons encode and transmit information, you may not know what the
monkey experiences when we stimulate his MT. (Singer 2006)</p>
</blockquote>

<p>
Clearly, having the monkey provide an introspective report would add
evidential weight, but obtaining such reports from non-linguistic
creatures is difficult. How can we get the animal to turn attention
<em>inward</em> to their perceptual states in an experimental
 context?<sup>[<a href="notes.html#note-13" id="ref-13">13</a>]</sup></p>
 
<h4 id="TactVibr">5.3.2 Tactile Vibration</h4>

<p>
What of microstimulation in the absence of a stimulus? Might we induce
<em>hallucinations</em> as Penfield did in his patients? Rather than
modulation of ongoing perceptual processing, the issue here is to
create an internal signal that mimics perception. Romo et al. (1998)
demonstrated that monkeys can carry out sensory tasks via activation
triggered by microstimulation. The monkeys&rsquo; task was to
discriminate the frequency of two sequential &ldquo;flutters&rdquo; on
their fingertips, that is, mechanical vibrations on the skin at
specific frequencies. In an experimental trial, an initial
<em>sample</em> flutter was presented for 500 ms and after a gap of
1&ndash;3 seconds, a second <em>test</em> flutter of either higher or
lower frequency was presented. The animal reported whether the second
test frequency was higher or lower than the sample.</p>

<p>
The experimenters examined whether direct microstimulation in the
absence of a stimulus could tap into the same neural representations
that guided the animal&rsquo;s report. They isolated neurons in
primary somatosensory cortex responsive to vibration frequency on the
fingers (S1, the somatosensory homunculus discovered by Penfield &amp;
Boldrey [1937]; see
 <a href="#fig1">Figure 1</a>).
 The investigators then stimulated the same neurons in S1 in the
absence of the test flutter, so used stimulation as a substitute for
an actual vibration. Thus, the animal had to make a comparison between
the frequency of a mechanical sample to either a subsequent (1) real
mechanical test vibration (i.e., the good case with an actual
stimulus) or (2) to a microstimulation test stimulus (i.e., the
&ldquo;hallucinatory&rdquo; case where direct activation of the S1
neurons occurred in the absence of a stimulus). Romo et al.
demonstrated that discrimination performance based either on
mechanical stimulation or microstimulation was <em>equivalent</em>. In
other words, the animals could match either mechanical or
microstimulation to a remembered mechanical
 sample.<sup>[<a href="notes.html#note-14" id="ref-14">14</a>]
 </sup>(For a related approach with visual stimuli and optogenetic
stimulation, see Azadi et al. 2023)</p>

<p>
In subsequent work (Romo et al. 2000), the investigators inverted the
experiment, using the microstimulation as the <em>sample.</em> In this
case, the animals had to remember the information conveyed by the
microstimulation (effectively, a hallucination) and then compare it to
either a subsequent (a) mechanically generated stimulation on the
finger (actual test stimulus) or (b) a microstimulation of S1 as test
(i.e., no stimulus). In both cases, performance was similar to earlier
results. The striking finding is that behavior could be driven
entirely by microstimulation. At least for the tactile stimulations at
issue, the animal might have been in the <em>Matrix</em>!</p>

<p>
One might think that the intentional action inference is stronger in
this paradigm, given the elegant flipping of stimuli in Romo et al.
2000. Still, the authors comment:</p>

<blockquote>

<p>
This study, therefore, has directly established a strong link between
neural activity and perception. However, we do not know yet whether
microstimulation of the QA circuit in S1 elicits a subjective flutter
sensation in the fingertips. This can only be explored by
microstimulating S1 in an attending human observer. (Romo et al. 2000:
 277)<sup>[<a href="notes.html#note-15" id="ref-15">15</a>]</sup></p>
 </blockquote>

<p>
Like Newsome, the authors reach for introspection. Yet they might
undersell their result, for it seems that the animals are having a
tactile hallucination: (a) Penfield showed that stimulation of primary
sensory cortices like S1 induces hallucinations in humans; (b) action
is engaged not at low stimulation of S1 in monkeys but only at higher
level stimulation; (c) at that point, when the stimulation grabs their
attention, the monkeys do what they were trained to do, namely
discriminate stimuli, either with (d) just mechanical stimulation
(normal experience), or (e) with a mix of mechanical and
microstimulation or with just microstimulation; (f) given the
behavioral equivalence of these three cases, one might then argue that
if performance in the mechanical stimulation cases involves conscious
tactile experience, then that same experience is involved in the other
cases.</p>

<p>
Taken together, these cases provide examples of detailed manipulations
in different sensory modalities, animals, and contents that test for
causal sufficiency and necessity across different levels of the
sensory processing hierarchy, from early levels (e.g., S1) to
mid-levels (MT) and, as discussed at the outset of this section, to
higher levels (lmFG or FFA). One issue that remains open is whether in
tapping into neural processing by microstimulation, one has simply
identified an earlier causal node in the neural processes that
generate perceptual experience, there being more informative neural
correlates later in the causal pathway.</p>

<h2 id="Futu">6. The Future</h2>

<p>
Talk of the neuroscience of consciousness has, thus far, focused on
the neural correlates of consciousness. Not all neural correlates are
explanatory, so finding correlates is a first step in the neuroscience
of consciousness. The next step involves manipulation of relevant
correlates to test claims about sufficiency and necessity, as isolated
in our two questions:</p>

<blockquote>

<p>
<em>Generic Consciousness:</em> What conditions/states <em>N</em> of
nervous systems are necessary and (or) sufficient for a mental state,
<em>M</em>, to be conscious as opposed to not?</p>

<p>
<em>Specific Consciousness:</em> What neural states or properties are
necessary and/or sufficient for a conscious perceptual state to have
content <em>X</em> rather than <em>Y</em>?</p>
</blockquote>

<p>
A productive neuroscience of consciousness requires that we understand
the relevant neural properties at the right level of analysis. For
generic consciousness, this will involve manipulation of relevant
properties in a way that can avoid the access/phenomenal confound, and
recent work focuses on pitting the many theories we have considered
against each other. For specific consciousness, the critical issue
will be to understand neural representational content and to find ways
to link experimentally and explanatorily neural content to phenomenal
content. We have tools to manipulate neural contents to affect
phenomenal content, and in doing so, we can begin to uncover the
neural basis of conscious contents. There is much interesting work yet
to be done, philosophically and empirically, and we can look forward
to a productive interdisciplinary research program.</p>
</div>

<div id="bibliography">

<h2 id="Bib">Bibliography</h2>

<ul class="hanging">

<li>Aglioti, Salvatore, Joseph F.X. DeSouza, &amp; Melvyn A. Goodale,
1995, &ldquo;Size-Contrast Illusions Deceive the Eye but Not the
Hand,&rdquo; <em>Current Biology</em>, 5 (6): 679&ndash;85.
doi:10.1016/s0960-9822(95)00133-3</li>

<li>Albantakis, Larissa, Leonardo Barbosa, Graham Findlay, Matteo
Grasso, Andrew M. Haun, William Marshall, William G. P. Mayner, et
al., 2023, &ldquo;Integrated Information Theory (IIT) 4.0: Formulating
the Properties of Phenomenal Existence in Physical Terms,&rdquo;
<em>PLOS Computational Biology</em>, 19 (10): e1011465.
doi:10.1371/journal.pcbi.1011465</li>

<li>Andersen, Richard A., Kristen N. Andersen, Eun Jung Hwang, &amp;
Markus Hauschild, 2014, &ldquo;Optic Ataxia: From Balint&rsquo;s
Syndrome to the Parietal Reach Region,&rdquo; <em>Neuron</em>, 81 (5):
967&ndash;83. doi:10.1016/j.neuron.2014.02.025</li>

<li>Arnold, Derek Henry, 2011a, &ldquo;I Agree: Binocular Rivalry
Stimuli Are Common but Rivalry Is Not,&rdquo; <em>Frontiers in Human
Neuroscience</em>, 5: 157. doi:10.3389/fnhum.2011.00157</li>

<li>&ndash;&ndash;&ndash;, 2011b, &ldquo;Why Is Binocular Rivalry
Uncommon? Discrepant Monocular Images in the Real World,&rdquo;
<em>Frontiers in Human Neuroscience</em>, 5: 116.
doi:10.3389/fnhum.2011.00116</li>

<li>Aru, Jaan, Talis Bachmann, Wolf Singer, &amp; Lucia Melloni, 2012,
&ldquo;Distilling the Neural Correlates of Consciousness,&rdquo;
<em>Neuroscience and Biobehavioral Reviews</em>, 36 (2): 737&ndash;46.
doi:10.1016/j.neubiorev.2011.12.003</li>

<li>Azadi, Reza, Simon Bohn, Emily Lopez, Rosa Lafer-Sousa, Karen
Wang, Mark A.G. Eldridge, &amp; Arash Afraz, 2023,
&ldquo;Image-Dependence of the Detectability of Optogenetic
Stimulation in Macaque Inferotemporal Cortex,&rdquo; <em>Current
Biology</em>, 33 (3): 581&ndash;588.e4. doi:10.1016/j.cub.2022.12.021</li>

<li>Azzopardi, P., &amp; A. Cowey, 1997, &ldquo;Is Blindsight like
Normal, Near-Threshold Vision?&rdquo; <em>Proceedings of the National
Academy of Sciences</em>, 94 (25): 14190&ndash;94.
doi:10.1073/pnas.94.25.14190</li>

<li>Baars, Bernard J., 1988, <em>A Cognitive Theory of
Consciousness</em>, Cambridge University Press.</li>

<li>Baker, Ben, Benjamin Lansdell, &amp; Konrad P. Kording, 2022,
&ldquo;Three Aspects of Representation in Neuroscience,&rdquo;
<em>Trends in Cognitive Sciences</em>, 26 (11): 942&ndash;58.
doi:10.1016/j.tics.2022.08.014</li>

<li>Baker, Daniel H., &amp; Erich W. Graf, 2009, &ldquo;Natural Images
Dominate in Binocular Rivalry,&rdquo; <em>Proceedings of the National
Academy of Sciences</em>, 106 (13): 5436&ndash;41.
doi:10.1073/pnas.0812860106</li>

<li>Barrett, Adam B., &amp; Pedro A. M. Mediano, 2019, &ldquo;The Phi
Measure of Integrated Information Is Not Well-Defined for General
Physical Systems,&rdquo; <em>Journal of Consciousness Studies</em>, 26
(1&ndash;2): 11&ndash;20.
 [<a href="https://arxiv.org/pdf/1902.04321" target="other">Preprint of Barrett and Mediano 2019 available online</a></li>
 
<li>Barron, Andrew B., &amp; Colin Klein, 2016, &ldquo;What Insects
Can Tell Us about the Origins of Consciousness,&rdquo; <em>Proceedings
of the National Academy of Sciences</em>, 113 (18): 4900&ndash;4908.
doi:10.1073/pnas.1520084113</li>

<li>Bartlett, Gary, 2022, &ldquo;Does Integrated Information Theory
Make Testable Predictions about the Role of Silent Neurons in
Consciousness?&rdquo; <em>Neuroscience of Consciousness</em>, 2022
(1): niac015. doi:10.1093/nc/niac015</li>

<li>Bayne, Tim, 2011, &ldquo;The Sense of Agency,&rdquo; in <em>The
Senses: Classic and Contemporary Philosophical Perspectives</em>,
edited by Fiona Macpherson, 355&ndash;74. Oxford: Oxford University
Press.</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;On the Axiomatic Foundations
of the Integrated Information Theory of Consciousness,&rdquo;
<em>Neuroscience of Consciousness</em>, 4 (1): niy007.
doi:10.1093/nc/niy007</li>
 
<li>Bayne, Tim, &amp; David J. Chalmers, 2003, &ldquo;What Is the Unity
of Consciousness?&rdquo; In <em>The Unity of Consciousness: Binding,
Integration, and Dissociation</em>, Axel Cleeremans (eds.),
Oxford: Oxford University Press, 23&ndash;58.
doi:10.1093/acprof:oso/9780198508571.003.0002</li>

<li>Bayne, Tim, Jakob Hohwy, &amp; Adrian M. Owen, 2016, &ldquo;Are
There Levels of Consciousness?&rdquo; <em>Trends in Cognitive
Sciences</em>, 20 (6): 405&ndash;13.
doi:10.1016/j.tics.2016.03.009</li>

<li>Bayne, Tim, &amp; Michelle Montague, eds., 2011, <em>Cognitive
Phenomenology</em>, Oxford: Oxford University Press.
doi:10.1093/acprof:oso/9780199579938.001.0001</li>

<li>Beck, Jacob, 2020, &ldquo;On Perceptual Confidence and
&lsquo;Completely Trusting Your Experience,&rsquo;&rdquo; <em>Analytic
Philosophy</em>, 61 (2): 174&ndash;88. doi:10.1111/phib.12151</li>

<li>Bedny, Marina, 2017, &ldquo;Evidence from Blindness for a
Cognitively Pluripotent Cortex,&rdquo; <em>Trends in Cognitive
Sciences</em>, 21 (9): 637&ndash;48.
doi:10.1016/j.tics.2017.06.003</li>

<li>Behrmann, Marlene, &amp; David C. Plaut, 2013, &ldquo;Distributed
Circuits, Not Circumscribed Centers, Mediate Visual
Recognition,&rdquo; <em>Trends in Cognitive Sciences</em>, 17 (5):
210&ndash;19. doi:10.1016/j.tics.2013.03.007</li>

<li>Birch, Jonathan, Alexandra K. Schnell, &amp; Nicola S. Clayton,
2020, &ldquo;Dimensions of Animal Consciousness,&rdquo; <em>Trends in
Cognitive Sciences</em>, 24 (10): 789&ndash;801.
doi:10.1016/j.tics.2020.07.007</li>

<li>Blake, Randolph, 2022, &ldquo;The Perceptual Magic of Binocular
Rivalry,&rdquo; <em>Current Directions in Psychological Science</em>,
31 (2): 139&ndash;46. doi:10.1177/09637214211057564</li>

<li>Blake, Randolph, Jan Brascamp, &amp; David J. Heeger, 2014,
&ldquo;Can Binocular Rivalry Reveal Neural Correlates of
Consciousness?&rdquo; <em>Philosophical Transactions of the Royal
Society B: Biological Sciences</em>, 369 (1641): 20130211.
doi:10.1098/rstb.2013.0211</li>

<li>Block, Ned, 1995, &ldquo;On a Confusion about a Function of
Consciousness,&rdquo; <em>Behavioral and Brain Sciences</em>, 18 (2):
227&ndash;47. doi:10.1017/s0140525x00038188</li>

<li>&ndash;&ndash;&ndash;, 2007, &ldquo;Consciousness, Accessibility,
and the Mesh between Psychology and Neuroscience,&rdquo;
<em>Behavioral and Brain Sciences</em>, 30 (5&ndash;6): 481&ndash;548.
doi:10.1017/s0140525x07002786</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;The Higher Order Approach to
Consciousness Is Defunct,&rdquo; <em>Analysis</em>, 71 (3):
419&ndash;31. doi:10.1093/analys/anr037</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;If Perception Is
Probabilistic, Why Does It Not Seem Probabilistic?&rdquo;
<em>Philosophical Transactions of the Royal Society B: Biological
Sciences</em>, 373 (1755): 20170341. doi:10.1098/rstb.2017.0341</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;What Is Wrong with the
No-Report Paradigm and How to Fix It,&rdquo; <em>Trends in Cognitive
Sciences</em>, 23 (12): 1003&ndash;13.
doi:10.1016/j.tics.2019.10.001</li>

<li>Boly, Melanie, Marcello Massimini, Naotsugu Tsuchiya, Bradley R
Postle, Christof Koch, &amp; Giulio Tononi, 2017, &ldquo;Are the
Neural Correlates of Consciousness in the Front or in the Back of the
Cerebral Cortex? Clinical and Neuroimaging Evidence,&rdquo; <em>The
Journal of Neuroscience</em>, 37 (40): 9603&ndash;13.
doi:10.1523/jneurosci.3218&ndash;16.2017</li>

<li>Born, Richard T., &amp; David C. Bradley, 2005, &ldquo;Structure
and Function of Visual Area MT,&rdquo; <em>Annual Review of
Neuroscience</em>, 28 (1): 157&ndash;89.
doi:10.1146/annurev.neuro.26.041002.131052</li>

<li>Britten, KH, MN Shadlen, WT Newsome, &amp; JA Movshon, 1992,
&ldquo;The Analysis of Visual Motion: A Comparison of Neuronal and
Psychophysical Performance,&rdquo; <em>The Journal of
Neuroscience</em>, 12 (12): 4745&ndash;65.
doi:10.1523/jneurosci.12-12-04745.1992</li>

<li>Brown, Richard, 2015, &ldquo;The HOROR Theory of Phenomenal
Consciousness,&rdquo; <em>Philosophical Studies</em>, 172 (7):
1&ndash;12. doi:10.1007/s11098-014-0388-7</li>

<li>Brown, Richard, Hakwan Lau, &amp; Joseph E. LeDoux, 2019,
&ldquo;Understanding the Higher-Order Approach to
Consciousness,&rdquo; <em>Trends in Cognitive Sciences</em>, 23 (9):
754&ndash;68. doi:10.1016/j.tics.2019.06.009</li>

<li>Campion, John, Richard Latto, &amp; Y. M. Smith, 1983, &ldquo;Is
Blindsight an Effect of Scattered Light, Spared Cortex, and
near-Threshold Vision?&rdquo; <em>Behavioral and Brain Sciences</em>,
6 (3): 423&ndash;48. doi:10.1017/s0140525x00016861</li>

<li>Cao, Rosa, 2012, &ldquo;A Teleosemantic Approach to Information in
the Brain,&rdquo; <em>Biology &amp; Philosophy</em>, 27 (1):
49&ndash;71. doi:10.1007/s10539-011-9292-0</li>

<li>&ndash;&ndash;&ndash;, 2014, &ldquo;Signaling in the Brain: In
Search of Functional Units,&rdquo; <em>Philosophy of Science</em>, 81
(5): 891&ndash;901. doi:10.1086/677688</li>

<li>Carruthers, Peter, 2011, <em>The Opacity of Mind: An Integrative
Theory of Self-Knowledge</em>, New York: Oxford University Press.</li>

<li>Chalmers, David J., 1995, &ldquo;Facing up the Problem of
Consciousness,&rdquo; <em>Journal of Consciousness Studies</em>, 2
(3): 200&ndash;219</li>

<li>&ndash;&ndash;&ndash;, 1996, <em>The Conscious Mind</em>, New York:
Oxford University Press.</li>

<li>&ndash;&ndash;&ndash;, 2000, &ldquo;What Is a Neural Correlate of
Consciousness,&rdquo; in <em>Neural Correlates of Consciousness:
Empirical and Conceptual Questions</em>, edited by Thomas Metzinger,
17&ndash;39. Cambridge, MA: MIT Press.</li>

<li>Chirimuuta, M., 2014, &ldquo;Psychophysical Methods and the Evasion
of Introspection,&rdquo; <em>Philosophy of Science</em>, 81 (5):
914&ndash;26. doi:10.1086/677890</li>

<li>Chun, Marvin M., Julie D. Golomb, &amp; Nicholas B. Turk-Browne,
2011, &ldquo;A Taxonomy of External and Internal Attention,&rdquo;
<em>Annual Review of Psychology</em>, 62 (1): 73&ndash;101.
doi:10.1146/annurev.psych.093008.100427</li>

<li>Churchland, Patricia, 1996, &ldquo;The Hornswoggle Problem,&rdquo;
<em>Journal of Consciousness Studies</em>, 3 (5&ndash;6):
402&ndash;408</li>

<li>Clark, Andy, 2001, &ldquo;Visual Experience and Motor Action: Are
the Bonds Too Tight?&rdquo; <em>The Philosophical Review</em>, 110
(4): 495&ndash;519. doi:10.2307/3182592</li>

<li>Cleeremans, Axel, 2011, &ldquo;The Radical Plasticity Thesis: How
the Brain Learns to Be Conscious,&rdquo; <em>Frontiers in
Psychology</em>, 2: 86. doi:10.3389/fpsyg.2011.00086</li>

<li>Cleeremans, Axel, Dalila Achoui, Arnaud Beauny, Lars Keuninckx,
Jean-Remy Martin, Santiago Mu&ntilde;oz-Moldes, Laur&egrave;ne
Vuillaume, &amp; Ad&eacute;la&iuml;de de Heering, 2020,
&ldquo;Learning to Be Conscious,&rdquo; <em>Trends in Cognitive
Sciences</em>, 24 (2): 112&ndash;23.
doi:10.1016/j.tics.2019.11.011</li>

<li>Cohen, Marlene R., &amp; William T. Newsome, 2004, &ldquo;What
Electrical Microstimulation Has Revealed about the Neural Basis of
Cognition,&rdquo; <em>Current Opinion in Neurobiology</em>, 14 (2):
169&ndash;77. doi:10.1016/j.conb.2004.03.016</li>

<li>&ndash;&ndash;&ndash;, 2009, &ldquo;Estimates of the Contribution
of Single Neurons to Perception Depend on Timescale and Noise
Correlation,&rdquo; <em>The Journal of Neuroscience</em>, 29 (20):
6635&ndash;48. doi:10.1523/jneurosci.5179-08.2009</li>

<li>Cohen, Michael A., &amp; Daniel C. Dennett, 2011,
&ldquo;Consciousness Cannot Be Separated from Function,&rdquo;
<em>Trends in Cognitive Sciences</em>, 15 (8): 358&ndash;64.
doi:10.1016/j.tics.2011.06.008</li>

<li>Colombo, Matteo, &amp; Peggy Seri&egrave;s, 2012, &ldquo;Bayes in
the Brain&mdash;On Bayesian Modelling in Neuroscience,&rdquo; <em>The
British Journal for the Philosophy of Science</em>, 63 (3):
697&ndash;723. doi:10.1093/bjps/axr043</li>

<li>Cowey, Alan, 2010, &ldquo;The Blindsight Saga,&rdquo;
<em>Experimental Brain Research</em>, 200 (1): 3&ndash;24.
doi:10.1007/s00221-009-1914-2</li>

<li>Crick, Francis, &amp; Christof Koch, 1990, &ldquo;Towards a
Neurobiological Theory of Consciousness,&rdquo; <em>Seminars in The
Neurosciences</em>, 2: 263&ndash;75</li>

<li>Cruse, Damian, Srivas Chennu, Camille Chatelle, Tristan A.
Bekinschtein, Davinia Fern&aacute;ndez-Espejo, John D Pickard, Steven
Laureys, &amp; Adrian M Owen, 2012, &ldquo;Bedside Detection of
Awareness in the Vegetative State: A Cohort Study,&rdquo; <em>The
Lancet</em>, 378 (9809): 2088&ndash;94.
doi:10.1016/s0140-6736(11)61224-5</li>

<li>Cul, A. Del, Stanislas Dehaene, P. Reyes, E. Bravo, &amp; A
Slachevsky, 2009, &ldquo;Causal Role of Prefrontal Cortex in the
Threshold for Access to Consciousness,&rdquo; <em>Brain</em>, 132 (9):
2531&ndash;40. doi:10.1093/brain/awp111</li>

<li>Curley, William H., Peter B. Forgacs, Henning U. Voss, Mary M. Conte,
&amp; Nicholas D. Schiff, 2018, &ldquo;Characterization of EEG Signals
Revealing Covert Cognition in the Injured Brain,&rdquo;
<em>Brain</em>, 141 (5): 1404&ndash;21. doi:10.1093/brain/awy070</li>

<li>Dehaene, Stanislas, &amp; Jean-Pierre Changeux, 2011,
&ldquo;Experimental and Theoretical Approaches to Conscious
Processing&rdquo; 70 (2): 200&ndash;227.
doi:10.1016/j.neuron.2011.03.018</li>

<li>Dehaene, Stanislas, Jean-Pierre Changeux, Lionel Naccache,
J&eacute;r&ocirc;me Sackur, &amp; Claire Sergent, 2006,
&ldquo;Conscious, Preconscious, and Subliminal Processing: A Testable
Taxonomy,&rdquo; <em>Trends in Cognitive Sciences</em>, 10 (5):
204&ndash;11. doi:10.1016/j.tics.2006.03.007</li>

<li>Dehaene, Stanislas, M. Kerszberg, &amp; J. P. Changeux, 1998,
&ldquo;A Neuronal Model of a Global Workspace in Effortful Cognitive
Tasks,&rdquo; <em>Proceedings of the National Academy of Sciences of
the United States of America</em>, 95 (24): 14529&ndash;34.
doi:10.1073/pnas.95.24.14529</li>

<li>Dehaene, Stanislas, &amp; Lionel Naccache, 2001, &ldquo;Towards a
Cognitive Neuroscience of Consciousness: Basic Evidence and a
Workspace Framework,&rdquo; <em>Cognition</em>, 79 (1&ndash;2):
1&ndash;37. doi:10.1016/s0010-0277(00)00123-2</li>

<li>Demertzi, Athena, Georgios Antonopoulos, Lizette Heine, Henning U.
Voss, Julia Sophia Crone, Carlo de Los Angeles, Mohamed Ali Bahri, et
al., 2015, &ldquo;Intrinsic Functional Connectivity Differentiates
Minimally Conscious from Unresponsive Patients,&rdquo; <em>Brain</em>,
138 (9): 2619&ndash;31. doi:10.1093/brain/awv169</li>

<li>Denison, Rachel N., 2017, &ldquo;Precision, Not Confidence,
Describes the Uncertainty of Perceptual Experience: Comment on John
Morrison&rsquo;s &lsquo;Perceptual Confidence.&rsquo;&rdquo;
<em>Analytic Philosophy</em>, 58 (1): 58&ndash;70.
doi:10.1111/phib.12092</li>

<li>Dennett, Daniel C., 2018, &ldquo;Facing up to the Hard Question of
Consciousness,&rdquo; <em>Philosophical Transactions of the Royal
Society B: Biological Sciences</em>, 373 (1755): 20170342&ndash;47.
doi:10.1098/rstb.2017.0342</li>

<li>Dienes, Zolt&aacute;n, &amp; Anil Seth, 2010, &ldquo;Gambling on
the Unconscious: A Comparison of Wagering and Confidence Ratings as
Measures of Awareness in an Artificial Grammar Task,&rdquo;
<em>Consciousness and Cognition</em>, 19 (2): 674&ndash;81.
doi:10.1016/j.concog.2009.09.009</li>

<li>Dijkstra, Nadine, &amp; Stephen M. Fleming, 2023,
&ldquo;Subjective Signal Strength Distinguishes Reality from
Imagination,&rdquo; <em>Nature Communications</em>, 14: 1627.
doi:10.1038/s41467-023-37322-1</li>

<li>Doerig, Adrien, Aaron Schurger, Kathryn Hess, &amp; Michael H.
Herzog, 2019, &ldquo;The Unfolding Argument: Why IIT and Other Causal
Structure Theories Cannot Explain Consciousness,&rdquo;
<em>Consciousness and Cognition</em>, 72: 49&ndash;59.
doi:10.1016/j.concog.2019.04.002</li>

<li>Do&#322;&#281;ga, Krzysztof, 2023, &ldquo;Models of Introspection
vs. Introspective Devices Testing the Research Programme for Possible
Forms of Introspection,&rdquo; <em>Journal of Consciousness
Studies</em>, 30 (9): 86&ndash;101.
doi:10.53765/20512201.30.9.086</li>

<li>Drayson, Zoe, 2014, &ldquo;Intentional Action and the Post-Coma
Patient,&rdquo; <em>Topoi</em>, 33 (1): 23&ndash;31.
doi:10.1007/s11245-013-9185-8</li>

<li>Dretske, Fred, 1981, <em>Knowledge and the Flow of
Information</em>, Cambridge, MA: MIT Press.</li>

<li>Dwarakanath, Abhilash, Vishal Kapoor, Joachim Werner, Shervin
Safavi, Leonid A. Fedorov, Nikos K. Logothetis, &amp; Theofanis I.
Panagiotaropoulos, 2023, &ldquo;Bistability of Prefrontal States Gates
Access to Consciousness,&rdquo; <em>Neuron</em>, 111 (10):
1666-1683.e4. doi:10.1016/j.neuron.2023.02.027</li>

<li>Edlow, Brian L., Leandro R. D. Sanz, Len Polizzotto, Nader
Pouratian, John D. Rolston, Samuel B. Snider, Aurore Thibaut, et al.,
2021, &ldquo;Therapies to Restore Consciousness in Patients with
Severe Brain Injuries: A Gap Analysis and Future Directions,&rdquo;
<em>Neurocritical Care</em>, 35 (Supplement 1): 68&ndash;85.
doi:10.1007/s12028-021-01227-y</li>

<li>Ehrsson, Henrik H., 2009, &ldquo;Rubber Hand Illusion,&rdquo; in
<em>The Oxford Companion to Consciousness</em>, edited by Tim Bayne,
Axel Cleeremans, &amp; Patrick Wilken, 531&ndash;73. Oxford: Oxford
University Press.</li>

<li>Evans, Gareth, 1982, <em>The Varieties of Reference</em>, Oxford:
Oxford University Press.</li>

<li>Faivre, Nathan, Elisa Filevich, Guillermo Solovey, Simone Kuhn,
&amp; Olaf Blanke, 2017, &ldquo;Behavioural, Modeling, and
Electrophysiological Evidence for Supramodality in Human
Metacognition,&rdquo; <em>The Journal of Neuroscience</em>, 38 (2):
263&ndash;77. doi:10.1523/jneurosci.0322-17.2017</li>

<li>Farah, Martha J., 2004, <em>Visual Agnosia: Disorders of Object
Recognition and What They Tell Us About Normal Vision</em>, second
edition, Cambridge, MA: MIT Press.</li>

<li>Fazekas, Peter, &amp; Georgina Nemeth, 2018, &ldquo;Dream
Experiences and the Neural Correlates of Perceptual Consciousness and
Cognitive Access,&rdquo; <em>Philosophical Transactions of the Royal
Society B: Biological Sciences</em>, 373 (1755): 20170356.
doi:10.1098/rstb.2017.0356</li>

<li>Feest, Uljana, 2012, &ldquo;Introspection as a Method and
Introspection as a Feature of Consciousness,&rdquo; <em>Inquiry</em>,
55 (1): 1&ndash;16. doi:10.1080/0020174x.2012.643619</li>

<li>&ndash;&ndash;&ndash;, 2014, &ldquo;Phenomenal Experiences,
First-Person Methods, and the Artificiality of Experimental
Data,&rdquo; <em>Philosophy of Science</em>, 81 (5): 927&ndash;39.
doi:10.1086/677689</li>

<li>Felleman, Daniel J., &amp; David C. Van Essen, 1991,
&ldquo;Distributed Hierarchical Processing in the Primate Cerebral
Cortex,&rdquo; <em>Cerebral Cortex</em>, 1 (1): 1&ndash;47.
doi:10.1093/cercor/1.1.1-a</li>

<li>Fern&aacute;ndez-Espejo, Davinia, &amp; Adrian M. Owen, 2013,
&ldquo;Detecting Awareness after Severe Brain Injury,&rdquo;
<em>Nature Reviews Neuroscience</em>, 14 (11): 801&ndash;9.
doi:10.1038/nrn3608</li>

<li>Fetsch, Christopher R., Roozbeh Kiani, William T. Newsome, &amp;
Michael N. Shadlen, 2014, &ldquo;Effects of Cortical Microstimulation
on Confidence in a Perceptual Decision,&rdquo; 83 (4): 797&ndash;804.
doi:10.1016/j.neuron.2014.07.011</li>

<li>Fink, Sascha Benjamin, 2016, &ldquo;A Deeper Look at the
&lsquo;Neural Correlate of Consciousness&rsquo;,&rdquo; <em>Frontiers
in Psychology</em>, 7 (July): 1044. doi:10.3389/fpsyg.2016.01044</li>

<li>Fleming, Stephen M., 2020, &ldquo;Awareness as Inference in a
Higher-Order State Space,&rdquo; <em>Neuroscience of
Consciousness</em>, 2020 (1): niz020. doi:10.1093/nc/niz020</li>

<li>&ndash;&ndash;&ndash;, 2023a, &ldquo;Metacognition and Confidence: A
Review and Synthesis,&rdquo; <em>Annual Review of Psychology</em>, 75
(1). doi:10.1146/annurev-psych-022423-032425</li>

<li>&ndash;&ndash;&ndash;, Josefien Huijgen, &amp; Raymond J. Dolan, 2012,
&ldquo;Prefrontal Contributions to Metacognition in Perceptual
Decision Making,&rdquo; <em>The Journal of Neuroscience</em>, 32 (18):
6117&ndash;25. doi:10.1523/jneurosci.6489-11.2012</li>

<li>Franz, Volker H., 2001, &ldquo;Action Does Not Resist Visual
Illusions,&rdquo; <em>Trends in Cognitive Sciences</em>, 5 (11):
457&ndash;59. doi:10.1007/s00221-002-1364-6</li>

<li>Franz, Volker H., &amp; Karl R. Gegenfurtner, 2008, &ldquo;Grasping
Visual Illusions: Consistent Data and No Dissociation,&rdquo;
<em>Cognitive Neuropsychology</em>, 25 (7&ndash;8): 920&ndash;50.
doi:10.1080/02643290701862449</li>

<li>Freeman, Alan W., 2005, &ldquo;Multistage Model for Binocular
Rivalry,&rdquo; <em>Journal of Neurophysiology</em>, 94 (6):
4412&ndash;20. doi:10.1152/jn.00557.2005</li>

<li>Gardelle, Vincent de, Fran&ccedil;ois Le Corre, &amp; Pascal
Mamassian, 2016, &ldquo;Confidence as a Common Currency between Vision
and Audition,&rdquo; edited by Suliann Ben Hamed, <em>PLoS ONE</em>,
11 (1): e0147901&ndash;11. doi:10.1371/journal.pone.0147901</li>

<li>Gelder, Beatrice de, Marco Tamietto, Geert van Boxtel, Rainer
Goebel, Arash Sahraie, Jan van den Stock, Bernard M C Stienen,
Lawrence Weiskrantz, &amp; Alan Pegna, 2008, &ldquo;Intact Navigation
Skills after Bilateral Loss of Striate Cortex,&rdquo; <em>Current
Biology</em>, 18 (24): R1128&ndash;29.
doi:10.1016/j.cub.2008.11.002</li>

<li>Giles, Nathan, Hakwan Lau, &amp; Brian Odegaard, 2016, &ldquo;What
Type of Awareness Does Binocular Rivalry Assess?&rdquo; <em>Trends in
Cognitive Sciences</em>, 20 (10): 719&ndash;20.
doi:10.1016/j.tics.2016.08.010</li>

<li>Goldfine, Andrew M., Jonathan D. Victor, Mary M. Conte, Jonathan
C. Bardin, &amp; Nicholas D. Schiff, 2011, &ldquo;Determination of
Awareness in Patients with Severe Brain Injury Using EEG Power
Spectral Analysis,&rdquo; <em>Clinical Neurophysiology</em>, 122 (11):
2157&ndash;68. doi:10.1016/j.clinph.2011.03.022</li>

<li>Goodale, Melvyn A., &amp; A. David Milner, 2004, <em>Sight Unseen:
An Exploration of Conscious and Unconscious Vision</em>, Oxford:
Oxford University Press.
doi:10.1093/acprof:oso/9780199596966.001.0001</li>

<li>Graaf, Tom A. de, Po-Jang Hsieh, &amp; Alexander T. Sack, 2012,
&ldquo;The &lsquo;Correlates&rsquo; in Neural Correlates of
Consciousness,&rdquo; <em>Neuroscience and Biobehavioral Reviews</em>,
36 (1): 191&ndash;97. doi:10.1016/j.neubiorev.2011.05.012</li>

<li>Greenberg, D. L., 2007, &ldquo;Comment on &lsquo;Detecting Awareness
in the Vegetative State.&rsquo;&rdquo; <em>Science</em>, 315 (5816):
1221b&ndash;1221b. doi:10.1126/science.1135284</li>

<li>Grimaldi, Piercesare, Hakwan Lau, &amp; Michele A. Basso, 2015,
&ldquo;There Are Things That We Know That We Know, and There Are
Things That We Do Not Know We Do Not Know: Confidence in
Decision-Making,&rdquo; <em>Neuroscience and Biobehavioral
Reviews</em>, May, 1&ndash;10.
doi:10.1016/j.neubiorev.2015.04.006</li>

<li>Gross, Steven, 2020, &ldquo;Probabilistic Representations in
Perception: Are There Any, and What Would They Be?&rdquo; <em>Mind
&amp; Language</em>, 35 (3): 377&ndash;89. doi:10.1111/mila.12280</li>

<li>Haddara, Nadia, &amp; Dobromir Rahnev, 2022, &ldquo;The Impact of
Feedback on Perceptual Decision-Making and Metacognition: Reduction in
Bias but No Change in Sensitivity,&rdquo; <em>Psychological
Science</em>, 33 (2): 259&ndash;75. doi:10.1177/09567976211032887</li>

<li>Haffenden, Angela M., &amp; Melvyn A. Goodale, 1998, &ldquo;The
Effect of Pictorial Illusion on Prehension and Perception,&rdquo;
<em>Journal of Cognitive Neuroscience</em>, 10 (1): 122&ndash;36.
doi:10.1162/089892998563824</li>

<li>Haffenden, Angela M., K. C. Schiff, &amp; Melvyn A. Goodale, 2001,
&ldquo;The Dissociation between Perception and Action in the
Ebbinghaus Illusion: Nonillusory Effects of Pictorial Cues on
Grasp,&rdquo; <em>Current Biology</em>, 11 (3): 177&ndash;81.
doi:10.1016/s0960-9822(01)00023-9</li>

<li>Hanson, Jake R., &amp; Sara I. Walker, 2019, &ldquo;Integrated
Information Theory and Isomorphic Feed-Forward Philosophical
Zombies,&rdquo; <em>Entropy</em>, 21 (11): 1073.
doi:10.3390/e21111073</li>

<li>Harman, Gilbert, 1990, &ldquo;The Intrinsic Quality of
Experience,&rdquo; <em>Philosophical Perspectives</em>, 4:
31&ndash;52. doi:10.2307/2214186</li>

<li>Hatamimajoumerd, Elaheh, N. Apurva Ratan Murty, Michael Pitts,
&amp; Michael A. Cohen, 2022, &ldquo;Decoding Perceptual Awareness
across the Brain with a No-Report FMRI Masking Paradigm,&rdquo;
<em>Current Biology</em>, 32 (19): 4139&ndash;49.
doi:10.1016/j.cub.2022.07.068</li>

<li>Hautus, Michael J., Neil A. Macmillan, &amp; C. Douglas Creelman,
2021, <em>Detection Theory: A User&rsquo;s Guide</em>, 3rd edition,
London: Routledge. doi:10.4324/9781003203636-17</li>

<li>Heal, Jane, 1996, &ldquo;Simulation, Theory, and Content,&rdquo;
in <em>Theories of Theories of Mind</em>, edited by Peter Carruthers
&amp; Peter K. Smith, 75&ndash;89. Cambridge: Cambridge University
Press. doi:10.1017/cbo9780511597985.006</li>

<li>Hesse, Janis Karan, &amp; Doris Y. Tsao, 2020, &ldquo;A New
No-Report Paradigm Reveals That Face Cells Encode Both Consciously
Perceived and Suppressed Stimuli,&rdquo; <em>eLife</em>, 9 (November):
e58360. doi:10.7554/elife.58360</li>

<li>Hirshorn, Elizabeth A., Yuanning Li, Michael J. Ward, R. Mark
Richardson, Julie A. Fiez, &amp; Avniel Singh Ghuman, 2016,
&ldquo;Decoding and Disrupting Left Midfusiform Gyrus Activity during
Word Reading,&rdquo; <em>Proceedings of the National Academy of
Sciences</em>, 113 (29): 8162&ndash;67.
doi:10.1073/pnas.1604126113</li>

<li>Histed, Mark H., Amy M. Ni, &amp; John H.R. Maunsell, 2013,
&ldquo;Insights into Cortical Mechanisms of Behavior from
Microstimulation Experiments,&rdquo; <em>Progress in
Neurobiology</em>, 103: 115&ndash;30.
doi:10.1016/j.pneurobio.2012.01.006</li>

<li>Horgan, Terence, John Tienson, &amp; George Graham, 2003,
&ldquo;The Phenomenology of First Person Agency,&rdquo; in
<em>Physicalism and Mental Causation: The Metaphysics of Mind and
Action</em>, edited by Sven Walter &amp; Heinz-Dieter Heckmann,
323&ndash;41. Exeter: Imprint Academic.</li>

<li>Irvine, Elizabeth, 2012a, &ldquo;Old Problems with New Measures in
the Science of Consciousness,&rdquo; <em>The British Journal for the
Philosophy of Science</em>, 63 (3): 627&ndash;48.
doi:10.1093/bjps/axs019</li>

<li>&ndash;&ndash;&ndash;, 2012b, <em>Consciousness as a Scientific
Concept: A Philosophy of Science Perspective</em>, Springer Science
&amp; Business Media. Dordrecht: Springer Netherlands.
doi:10.1007/978-94-007-5173-6</li>

<li>&ndash;&ndash;&ndash;, 2013, &ldquo;Measures of
Consciousness,&rdquo; <em>Philosophy Compass</em>, 8 (3):
285&ndash;97. doi:10.1111/phc3.12016</li>

<li>Jackson, Frank, 1982, &ldquo;Epiphenomenal Qualia,&rdquo; <em>The
Philosophical Quarterly</em>, 32 (127): 127&ndash;36.
doi:10.2307/2960077</li>

<li>James, Thomas W., Jody Culham, G. Keith Humphrey, A. David Milner,
&amp; Melvyn A. Goodale, 2003, &ldquo;Ventral Occipital Lesions Impair
Object Recognition but Not Object-directed Grasping: An FMRI
Study,&rdquo; <em>Brain</em>, 126 (11): 2463&ndash;75.
doi:10.1093/brain/awg248</li>

<li>Kammerer, Fran&ccedil;ois, &amp; Keith Frankish, 2023, &ldquo;What
Forms Could Introspective Systems Take? A Research Programme,&rdquo;
<em>Journal of Consciousness Studies</em>, 30 (9): 13&ndash;48.
doi:10.53765/20512201.30.9.013</li>

<li>Kanwisher, Nancy, 2000, &ldquo;Domain Specificity in Face
Perception,&rdquo; <em>Nature Neuroscience</em>, 3 (8): 759&ndash;63.
doi:10.1038/77664</li>

<li>Kapoor, Vishal, Abhilash Dwarakanath, Shervin Safavi, Joachim
Werner, Michel Besserve, Theofanis I. Panagiotaropoulos, &amp; Nikos
K. Logothetis, 2022, &ldquo;Decoding Internally Generated Transitions
of Conscious Contents in the Prefrontal Cortex without Subjective
Reports,&rdquo; <em>Nature Communications</em>, 13 (1): 1535.
doi:10.1038/s41467-022-28897-2</li>

<li>Kiani, R., &amp; M. N. Shadlen, 2009, &ldquo;Representation of
Confidence Associated with a Decision by Neurons in the Parietal
Cortex,&rdquo; <em>Science</em>, 324 (5928): 759&ndash;64.
doi:10.1126/science.1169405</li>

<li>Kim, Byounghoon, &amp; Michele A. Basso, 2008, &ldquo;Saccade
Target Selection in the Superior Colliculus: A Signal Detection Theory
Approach,&rdquo; <em>The Journal of Neuroscience</em>, 28 (12):
2991&ndash;3007. doi:10.1523/jneurosci.5424-07.2008</li>

<li>Kim, Hyoungkyu, Anthony G. Hudetz, Joseph Lee, George A. Mashour,
UnCheol Lee, Michael S. Avidan, Tarik Bel-Bahar, et al., 2018,
&ldquo;Estimating the Integrated Information Measure Phi from
High-Density Electroencephalography during States of Consciousness in
Humans,&rdquo; <em>Frontiers in Human Neuroscience</em>, 12: 42.
doi:10.3389/fnhum.2018.00042</li>

<li>King, Sheila M., Paul Azzopardi, Alan Cowey, John Oxbury, &amp;
Susan Oxbury, 1996, &ldquo;The Role of Light Scatter in the Residual
Visual Sensitivity of Patients with Complete Cerebral
Hemispherectomy,&rdquo; <em>Visual Neuroscience</em>, 13 (1):
1&ndash;13. doi:10.1017/s0952523800007082</li>

<li>Klein, Colin, 2017, &ldquo;Consciousness, Intention, and
Command-Following in the Vegetative State,&rdquo; <em>The British
Journal for the Philosophy of Science</em>, 68 (1): 27&ndash;54.
doi:10.1093/bjps/axv012</li>

<li>Knuuttila, Tarja, &amp; Andrea Loettgers, 2016, &ldquo;Model
Templates within and between Disciplines: From Magnets to Gases
&ndash; and Socio-Economic Systems,&rdquo; <em>European Journal for
Philosophy of Science</em>, 6 (3): 377&ndash;400.
doi:10.1007/s13194-016-0145-1</li>

<li>Ko, Yoshiaki, &amp; Hakwan Lau, 2012, &ldquo;A Detection Theoretic
Explanation of Blindsight Suggests a Link between Conscious Perception
and Metacognition,&rdquo; <em>Philosophical Transactions of the Royal
Society B: Biological Sciences</em>367 (1594): 1401&ndash;11.
doi:10.1098/rstb.2011.0380</li>

<li>Koch, Christof, 2004, <em>The Quest for Consciousness</em>,
Englewood, CO: Roberts and Company Publishers.</li>

<li>&ndash;&ndash;&ndash;, 2012, <em>Consciousness: Confessions of a
Romantic Reductionist</em>, Cambridge, MA: MIT Press.</li>

<li>Kov&aacute;cs, Ilona, Thomas V. Papathomas, Ming Yang, &amp;
&Aacute;kos Feh&eacute;r, 1996, &ldquo;When the Brain Changes Its
Mind: Interocular Grouping during Binocular Rivalry,&rdquo;
<em>Proceedings of the National Academy of Sciences</em>, 93 (26):
15508&ndash;11. doi:10.1073/pnas.93.26.15508</li>

<li>Kozuch, Benjamin, 2013, &ldquo;Prefrontal Lesion Evidence against
Higher-Order Theories of Consciousness,&rdquo; <em>Philosophical
Studies</em>, 167 (3): 721&ndash;46.
doi:10.1007/s11098-013-0123-9</li>

<li>&ndash;&ndash;&ndash;, 2022, &ldquo;Underwhelming Force:
Evaluating the Neuropsychological Evidence for Higher-Order Theories
of Consciousness,&rdquo; <em>Mind &amp; Language</em>, 37 (5):
790&ndash;813. doi:10.1111/mila.12363</li>

<li>&ndash;&ndash;&ndash;, 2023, &ldquo;A Legion of Lesions: The
Neuroscientific Rout of Higher-Order Thought Theory,&rdquo;
<em>Erkenntnis</em>, first online 23 March 2023.
doi:10.1007/s10670-023-00669-4</li>

<li>Kravitz, Dwight J., Kadharbatcha S. Saleem, Chris I. Baker, &amp;
Mortimer Mishkin, 2011, &ldquo;A New Neural Framework for Visuospatial
Processing,&rdquo; <em>Nature Reviews Neuroscience</em>, 12 (4):
217&ndash;30. doi:10.1038/nrn3008</li>

<li>Kriegel, Uriah, 2003, &ldquo;Consciousness as Intransitive
Self-Consciousness: Two Views and an Argument,&rdquo; <em>Canadian
Journal of Philosophy</em>, 33 (1): 103&ndash;32.
doi:10.1080/00455091.2003.10716537</li>

<li>Lamme, Victor A. F., 2006, &ldquo;Towards a True Neural Stance on
Consciousness,&rdquo; <em>Trends in Cognitive Sciences</em>, 10 (11):
494&ndash;501. doi:10.1016/j.tics.2006.09.001</li>

<li>&ndash;&ndash;&ndash;, 2010, &ldquo;How Neuroscience Will Change
Our View on Consciousness,&rdquo; <em>Cognitive Neuroscience</em>, 1
(3): 204&ndash;20. doi:10.1080/17588921003731586</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;Consciousness, Metacognition,
&amp; Perceptual Reality Monitoring,&rdquo; <em>PsyArXiv</em>, June.
doi:10.31234/osf.io/ckbyf</li>

<li>&ndash;&ndash;&ndash;, 2022, <em>In Consciousness We Trust</em>,
Oxford: Oxford University Press.</li>

<li>&ndash;&ndash;&ndash;, 2023, &ldquo;What Is a Pseudoscience of
Consciousness? Lessons from Recent Adversarial Collaborations,&rdquo;
<em>PsyArXiv</em>. doi:10.31234/osf.io/28z3y</li>

<li>Lau, Hakwan, &amp; Richard Brown, 2019, &ldquo;The Emperor&rsquo;s
New Phenomenology? The Empirical Case for Conscious Experience without
First-Order Representations,&rdquo; in <em>Blockheads! Essays on Ned
Block&rsquo;s Philosophy of Mind and Consciousness</em>, edited by
Adam Pautz &amp; Daniel Stoljar, Ch. 11. Cambridge, MA: MIT Press.</li>

<li>Lau, Hakwan, &amp; R. E. Passingham, 2006, &ldquo;Relative
Blindsight in Normal Observers and the Neural Correlate of Visual
Consciousness,&rdquo; <em>Proceedings of the National Academy of
Sciences</em>, 103 (49): 18763&ndash;68.
doi:10.1073/pnas.0607716103</li>

<li>Lau, Hakwan, &amp; David Rosenthal, 2011, &ldquo;Empirical Support
for Higher-Order Theories of Conscious Awareness,&rdquo; <em>Trends in
Cognitive Sciences</em>, 15 (8): 365&ndash;73.
doi:10.1016/j.tics.2011.05.009</li>

<li>LeDoux, Joseph E., &amp; Richard Brown, 2017, &ldquo;A Higher-Order
Theory of Emotional Consciousness,&rdquo; <em>Proceedings of the
National Academy of Sciences of the United States of America</em>, 114
(10): E2016&ndash;25. doi:10.1073/pnas.1619316114</li>

<li>LeDoux, Joseph E., Matthias Michel, &amp; Hakwan Lau, 2020,
&ldquo;A Little History Goes a Long Way toward Understanding Why We
Study Consciousness the Way We Do Today,&rdquo; <em>Proceedings of the
National Academy of Sciences of the United States of America</em>, 117
(13): 6976&ndash;84. doi:10.1073/pnas.1921623117</li>

<li>Lee, Andrew Y., 2023, &ldquo;Degrees of Consciousness,&rdquo;
<em>No&ucirc;s</em>, 57 (3): 553&ndash;75. doi:10.1111/nous.12421</li>

<li>Lee, Geoffrey, &amp; Nico Orlandi, 2022, &ldquo;Representing
Probability in Perception and Experience,&rdquo; <em>Review of
Philosophy and Psychology</em>, 13 (4): 907&ndash;45.
doi:10.1007/s13164-022-00647-9</li>

<li>Lee, Joonyeol, &amp; John H. R. Maunsell, 2009, &ldquo;A
Normalization Model of Attentional Modulation of Single Unit
Responses,&rdquo; <em>PLoS ONE</em>, 4 (2): e4651.
doi:10.1371/journal.pone.0004651</li>

<li>Leopold, David A., 2012, &ldquo;Primary Visual Cortex: Awareness
and Blindsight*,&rdquo; <em>Neuroscience</em>, 35 (1): 91&ndash;109.
doi:10.1146/annurev-neuro-062111-150356</li>

<li>Leopold, David A., &amp; Nikos K. Logothetis, 1996, &ldquo;Activity
Changes in Early Visual Cortex Reflect Monkeys&rsquo; Percepts during
Binocular Rivalry,&rdquo; <em>Nature</em>, 379 (6565): 549&ndash;53.
doi:10.1038/379549a0</li>

<li>Levine, Joseph, 1983, &ldquo;Materialism and Qualia: The
Explanatory Gap,&rdquo; <em>Pacific Philosophical Quarterly</em>, 64
(4): 354&ndash;61. doi:10.1111/j.1468-0114.1983.tb00207.x</li>

<li>Li, F. F., R. VanRullen, Christof Koch, &amp; Pietro Perona, 2002,
&ldquo;Rapid Natural Scene Categorization in the near Absence of
Attention,&rdquo; <em>Proceedings of the National Academy of Sciences
of the United States of America</em>, 99 (14): 9596&ndash;9601.
doi:10.1073/pnas.092277599</li>

<li>Lin, Chia-Hua, 2018, &ldquo;Tool Migration: A Framework for
Analyzing Cross-Disciplinary Use of Mathematical Constructs,&rdquo;
<em>PhilSci Archive</em>, 1&ndash;11</li>

<li>Liu, Sirui, Qing Yu, Peter U. Tse, &amp; Patrick Cavanagh, 2019,
&ldquo;Neural Correlates of the Conscious Perception of Visual
Location Lie Outside Visual Cortex,&rdquo; <em>Current Biology</em>,
29 (November): 1&ndash;9. doi:10.1016/j.cub.2019.10.033</li>

<li>Logothetis, Nikos K., David A. Leopold, &amp; David L. Sheinberg,
1996, &ldquo;What Is Rivalling during Binocular Rivalry?&rdquo;
<em>Nature</em>, 380 (6575): 621&ndash;24. doi:10.1038/380621a0</li>

<li>Lumer, Erik D., &amp; Geraint Rees, 1999, &ldquo;Covariation of
Activity in Visual and Prefrontal Cortex Associated with Subjective
Visual Perception,&rdquo; <em>Proceedings of the National Academy of
Sciences</em>, 96 (4): 1669&ndash;73. doi:10.1073/pnas.96.4.1669</li>

<li>Maier, Alexander, Melanie Wilke, Christopher Aura, Charles Zhu,
Frank Q. Ye, &amp; David A. Leopold, 2008, &ldquo;Divergence of FMRI and
Neural Signals in V1 during Perceptual Suppression in the Awake
Monkey,&rdquo; <em>Nature Neuroscience</em>, 11 (10): 1193&ndash;1200.
doi:10.1038/nn.2173</li>

<li>Maniscalco, Brian, &amp; Hakwan Lau, 2012, &ldquo;A Signal
Detection Theoretic Approach for Estimating Metacognitive Sensitivity
from Confidence Ratings,&rdquo; <em>Consciousness and Cognition</em>,
21 (1): 422&ndash;30. doi:10.1016/j.concog.2011.09.021</li>

<li>&ndash;&ndash;&ndash;, 2014, &ldquo;Signal Detection Theory
Analysis of Type 1 and Type 2 Data: Meta-D&prime;, Response-Specific
Meta-D&prime;, and the Unequal Variance SDT Model,&rdquo; in <em>The
Cognitive Neuroscience of Metacognition</em>, edited by Stephen M
Fleming &amp; Christopher D. Frith, 25&ndash;66.</li>

<li>Marcel, Anthony J., 2003, &ldquo;The Sense of Agency: Awareness
and Ownership of Action,&rdquo; in <em>Agency and Self-Awareness:
Issues in Philosophy and Psychology</em>, edited by Fiona McPherson,
48&ndash;93. Oxford: Oxford University Press.</li>

<li>Mashour, George A., Pieter Roelfsema, Jean-Pierre Changeux, &amp;
Stanislas Dehaene, 2020, &ldquo;Conscious Processing and the Global
Neuronal Workspace Hypothesis,&rdquo; <em>Neuron</em>, 105 (5):
776&ndash;98. doi:10.1016/j.neuron.2020.01.026</li>

<li>Massimini, Marcello, Fabio Ferrarelli, Reto Huber, Steve K. Esser,
Harpreet Singh, &amp; Giulio Tononi, 2005, &ldquo;Breakdown of
Cortical Effective Connectivity During Sleep,&rdquo; <em>Science</em>,
309 (5744): 2228&ndash;32. doi:10.1126/science.1117256</li>

<li>Mazancieux, Audrey, Stephen M. Fleming, C&eacute;line Souchay,
&amp; Chris J. A. Moulin, 2020, &ldquo;Is There a G Factor for
Metacognition? Correlations in Retrospective Metacognitive Sensitivity
across Tasks,&rdquo; <em>Journal of Experimental Psychology:
General</em>, 149 (9): 1788&ndash;99. doi:10.1037/xge0000746</li>

<li>Mazancieux, Audrey, Michael Pereira, Nathan Faivre, Pascal
Mamassian, Chris J. A. Moulin, &amp; C&eacute;line Souchay, 2023,
&ldquo;Towards a Common Conceptual Space for Metacognition in
Perception and Memory,&rdquo; <em>Nature Reviews Psychology</em>, 2
(12): 751&ndash;66. doi:10.1038/s44159-023-00245-1</li>

<li>Mazzi, Chiara, Chiara Bagattini, &amp; Silvia Savazzi, 2016,
&ldquo;Blind-Sight vs. Degraded-Sight: Different Measures Tell a
Different Story,&rdquo; <em>Frontiers in Psychology</em>, 7: 901.
doi:10.3389/fpsyg.2016.00901</li>

<li>Mediano, Pedro A. M., Fernando E. Rosas, Daniel Bor, Anil K. Seth,
&amp; Adam B. Barrett, 2022, &ldquo;The Strength of Weak Integrated
Information Theory,&rdquo; <em>Trends in Cognitive Sciences</em>, 26
(8): 646&ndash;55. doi:10.1016/j.tics.2022.04.008</li>

<li>M&eacute;gevand, Pierre, David M. Groppe, Matthew S. Goldfinger,
Sean T. Hwang, Peter B. Kingsley, Ido Davidesco, &amp; Ashesh D.
Mehta, 2014, &ldquo;Seeing Scenes: Topographic Visual Hallucinations
Evoked by Direct Electrical Stimulation of the Parahippocampal Place
Area,&rdquo; <em>The Journal of Neuroscience</em>, 34 (16):
5399&ndash;5405. doi:10.1523/jneurosci.5202-13.2014</li>

<li>Mendoza-Halliday, Diego, &amp; Julio C. Martinez-Trujillo, 2017,
&ldquo;Neuronal Population Coding of Perceived and Memorized Visual
Features in the Lateral Prefrontal Cortex,&rdquo; <em>Nature
Communications</em>, 8 (May): 15471. doi:10.1038/ncomms15471</li>

<li>Merker, Bjorn, Kenneth Williford, &amp; David Rudrauf, 2022,
&ldquo;The Integrated Information Theory of Consciousness: A Case of
Mistaken Identity,&rdquo; <em>Behavioral and Brain Sciences</em>, 45
(e41): 1&ndash;63. doi:10.1017/s0140525x21000881</li>

<li>Michel, Matthias, 2023, &ldquo;Confidence in Consciousness
Research,&rdquo; <em>Wiley Interdisciplinary Reviews: Cognitive
Science</em>14 (2): e1628. doi:10.1002/wcs.1628</li>

<li>Michel, Matthias, &amp; Hakwan Lau, 2020, &ldquo;On the Dangers of
Conflating Strong and Weak Versions of a Theory of
Consciousness,&rdquo; <em>Philosophy and the Mind Sciences</em>, 1
(II). doi:10.33735/phimisci.2020.ii.54</li>

<li>&ndash;&ndash;&ndash;, 2021, &ldquo;Is Blindsight Possible Under
Signal Detection Theory? Comment on Phillips (2021),&rdquo;
<em>Psychological Review</em>, 128 (3): 585&ndash;91.
doi:10.1037/rev0000266</li>

<li>Michel, Matthias, &amp; Jorge Morales, 2020, &ldquo;Minority
Reports: Consciousness and the Prefrontal Cortex,&rdquo; <em>Mind
&amp; Language</em>, 35 (4): 493&ndash;513.
doi:10.1111/mila.12264</li>

<li>Milner, A. David, &amp; Melvyn A. Goodale, 1995, <em>The Visual
Brain in Action</em>, New York: Oxford University Press.</li>

<li>Mole, C., 2009, &ldquo;Illusions, Demonstratives, and the Zombie
Action Hypothesis,&rdquo; <em>Mind</em>, 118 (472): 995&ndash;1011.
doi:10.1093/mind/fzp109</li>

<li>Montemayor, Carlos, &amp; Harry Haroutioun Haladjian, 2015,
<em>Consciousness, Attention, and Conscious Attention</em>, Cambridge,
MA: MIT Press.</li>

<li>Monti, Martin M., Audrey Vanhaudenhuyse, Martin R. Coleman,
Melanie Boly, John D. Pickard, Luaba Tshibanda, Adrian M. Owen, &amp;
Steven Laureys, 2010, &ldquo;Willful Modulation of Brain Activity in
Disorders of Consciousness,&rdquo; <em>The New England Journal of
Medicine</em>, 362 (7): 579&ndash;89. doi:10.1056/nejmoa0905370</li>

<li>Morales, Jorge, 2023, &ldquo;Mental Strength: A Theory of
Experience Intensity,&rdquo; <em>Philosophical Perspectives</em>, 37
(1): 248&ndash;68. doi:10.1111/phpe.12189</li>

<li>&ndash;&ndash;&ndash;, forthcoming, &ldquo;Introspection Is Signal
Detection,&rdquo; <em>The British Journal for the Philosophy of
Science</em>.</li>

<li>Morales, Jorge, &amp; Hakwan Lau, 2020, &ldquo;The Neural
Correlates of Consciousness,&rdquo; in <em>Oxford Handbook of the
Philosophy of Consciousness</em>, edited by Uriah Kriegel,
233&ndash;60. Oxford University Press.</li>

<li>&ndash;&ndash;&ndash;, 2022, &ldquo;Confidence Tracks
Consciousness,&rdquo; in <em>Qualitative Consciousness Themes from the
Philosophy of David Rosenthal</em>, edited by Josh Weisberg,
91&ndash;108. Cambridge: Cambridge University Press.</li>

<li>Morales, Jorge, Hakwan Lau, &amp; Stephen M. Fleming, 2018,
&ldquo;Domain-General and Domain-Specific Patterns of Activity
Supporting Metacognition in Human Prefrontal Cortex,&rdquo; <em>The
Journal of Neuroscience</em>, 38 (14): 3534&ndash;46.
doi:10.1523/jneurosci.2360-17.2018</li>

<li>Morrison, John, 2016, &ldquo;Perceptual Confidence,&rdquo;
<em>Analytic Philosophy</em>, 57 (1): 15&ndash;48.
doi:10.1111/phib.12077</li>

<li>Munton, Jessie, 2016, &ldquo;Visual Confidences and Direct
Perceptual Justification,&rdquo; <em>Philosophical Topics</em>, 44
(2): 301&ndash;26. doi:10.5840/philtopics201644225</li>

<li>Naccache, Lionel, Jean-Pierre Changeux, Theofanis I
Panagiotaropoulos, &amp; Stanislas Dehaene, 2021, &ldquo;Why
Intracranial Electrical Stimulation of the Human Brain Suggests an
Essential Role for Prefrontal Cortex in Conscious Processing: A
Commentary on Raccah et al.,&rdquo; <em>PsyArXiv</em>.
doi:10.31219/osf.io/zrqp8</li>

<li>Nachev, Parashkev, &amp; Masud Husain, 2007, &ldquo;Comment on
&lsquo;Detecting Awareness in the Vegetative State,&rsquo;&rdquo;
<em>Science</em>, 315 (5816): 1221a. doi:10.1126/science.1135096</li>

<li>Nagel, Thomas, 1974, &ldquo;What Is It Like to Be a Bat?&rdquo;
<em>The Philosophical Review</em>, 83 (4): 435&ndash;50.
doi:10.2307/2183914</li>

<li>Newsome, William T., &amp; Edmond B. Par&eacute;, 1988, &ldquo;A
Selective Impairment of Motion Perception Following Lesions of the
Middle Temporal Visual Area (MT),&rdquo; <em>The Journal of
Neuroscience</em>, 8 (6): 2201&ndash;11.
doi:10.1523/jneurosci.08-06-02201.1988</li>

<li>Ngo, Trung T., Steven M. Miller, Guang B. Liu, &amp; John D.
Pettigrew, 2000, &ldquo;Binocular Rivalry and Perceptual
Coherence,&rdquo; <em>Current Biology</em>, 10 (4): R134&ndash;36.
doi:10.1016/s0960-9822(00)00399-7</li>

<li>Nichols, Shaun, &amp; Stephen P. Stich, 2003, <em>Mindreading: An
Integrated Account of Pretence, Self-Awareness, and Understanding
Other Minds</em>, Oxford: Oxford University Press.
doi:10.1093/0198236107.001.0001</li>

<li>Nieder, Andreas, Lysann Wagener, &amp; Paul Rinnert, 2020,
&ldquo;A Neural Correlate of Sensory Consciousness in a Corvid
Bird,&rdquo; <em>Science</em>, 369 (6511): 1626&ndash;29.
doi:10.1126/science.abb1447</li>

<li>Noble, Stephanie, Joshua Curtiss, Luiz Pessoa, &amp; Dustin
Scheinost, 2023, &ldquo;The Tip of the Iceberg: A Call to Embrace
Anti-Localizationism in Human Neuroscienceresearch,&rdquo;
<em>PsyArXiv</em>, November. doi:10.31234/osf.io/9eqh6</li>

<li>No&euml;, Alva, &amp; E. Thompson, 2004, &ldquo;Are There Neural
Correlates of Consciousness?&rdquo; <em>Journal of Consciousness
Studies</em>11 (1): 3&ndash;28</li>

<li>Noy, N., S. Bickel, E. Zion-Golumbic, M. Harel, T. Golan, I. Davidesco,
C. A. Schevon, et al., 2015, &ldquo;Ignition&rsquo;s Glow: Ultra-Fast
Spread of Global Cortical Activity Accompanying Local
&lsquo;Ignitions&rsquo; in Visual Cortex during Conscious Visual
Perception,&rdquo; <em>Consciousness and Cognition</em>, 35
(September): 206&ndash;24. doi:10.1016/j.concog.2015.03.006</li>

<li>Odegaard, Brian, Min Yu Chang, Hakwan Lau, &amp; Sing-Hang Cheung,
2018, &ldquo;Inflation versus Filling-in: Why We Feel We See More than
We Actually Do in Peripheral Vision,&rdquo; <em>Philosophical
Transactions of the Royal Society B: Biological Sciences</em>, 373
(1755): 20170345. doi:10.1098/rstb.2017.0345</li>

<li>Odegaard, Brian, Robert T. Knight, &amp; Hakwan Lau, 2017,
&ldquo;Should A Few Null Findings Falsify Prefrontal Theories Of
Conscious Perception?&rdquo; <em>The Journal of Neuroscience</em>, 37
(40): 9593&ndash;9602. doi:10.1101/122267</li>

<li>Oizumi, Masafumi, Larissa Albantakis, &amp; Giulio Tononi, 2014,
&ldquo;From the Phenomenology to the Mechanisms of Consciousness:
Integrated Information Theory 3.0,&rdquo; <em>PLoS Computational
Biology</em>, 10 (5): e1003588.
doi:10.1371/journal.pcbi.1003588.s004</li>

<li>O&rsquo;Shea, Robert Paul, 2011, &ldquo;Binocular Rivalry Stimuli
Are Common but Rivalry Is Not,&rdquo; <em>Frontiers in Human
Neuroscience</em>, 5: 148. doi:10.3389/fnhum.2011.00148</li>

<li>Overgaard, Morten, &amp; Peter Fazekas, 2016, &ldquo;Can No-Report
Paradigms Extract True Correlates of Consciousness?&rdquo; <em>Trends
in Cognitive Sciences</em>, 20 (4): 241&ndash;42.
doi:10.1016/j.tics.2016.01.004</li>

<li>Overgaard, Morten, Katrin Fehl, Kim Mouridsen, Bo Bergholt, &amp;
Axel Cleeremans, 2008, &ldquo;Seeing without Seeing? Degraded
Conscious Vision in a Blindsight Patient,&rdquo; <em>PLoS ONE</em>, 3
(8): e3028. doi:10.1371/journal.pone.0003028.t001</li>

<li>Overgaard, Morten, Julian Rote, Kim Mouridsen, &amp; Thomas
Zo&euml;ga Rams&oslash;y, 2006, &ldquo;Is Conscious Perception Gradual
or Dichotomous? A Comparison of Report Methodologies during a Visual
Task,&rdquo; <em>Consciousness and Cognition</em>, 15 (4):
700&ndash;708. doi:10.1016/j.concog.2006.04.002</li>

<li>Owen, Adrian M., Martin R. Coleman, Melanie Boly, Matthew H.
Davis, Steven Laureys, Dietsje Jolles, &amp; John D. Pickard, 2007,
&ldquo;Response to Comments on &lsquo;Detecting Awareness in the
Vegetative State.&rsquo;&rdquo; <em>Science</em>, 315 (5816):
1221&ndash;1221. doi:10.1126/science.1135583</li>

<li>Owen, Adrian M., Martin R. Coleman, Melanie Boly, Matthew H.
Davis, Steven Laureys, &amp; John D. Pickard, 2006, &ldquo;Detecting
Awareness in the Vegetative State,&rdquo; <em>Science</em>, 313
(5792): 1402&ndash;1402. doi:10.1126/science.1130197</li>

<li>Panagiotaropoulos, Theofanis I., Abhilash Dwarakanath, &amp;
Vishal Kapoor, 2020, &ldquo;Prefrontal Cortex and Consciousness:
Beware of the Signals,&rdquo; <em>Trends in Cognitive Sciences</em>,
24 (5): 343&ndash;44. doi:10.1016/j.tics.2020.02.005</li>

<li>Parker, A. J., &amp; W. T. Newsome, 1998, &ldquo;Sense and the
Single Neuron: Probing the Physiology of Perception,&rdquo; <em>Annual
Review of Neuroscience</em>, 21 (1): 227&ndash;77.
doi:10.1146/annurev.neuro.21.1.227</li>

<li>Parvizi, Josef, Corentin Jacques, Brett L Foster, Nathan Witthoft,
Nathan Withoft, Vinitha Rangarajan, Kevin S Weiner, &amp; Kalanit
Grill-Spector, 2012, &ldquo;Electrical Stimulation of Human Fusiform
Face-Selective Regions Distorts Face Perception,&rdquo; <em>Journal of
Neuroscience</em>, 32 (43): 14915&ndash;20.
doi:10.1523/jneurosci.2609-12.2012</li>

<li>Penfield, Wilder, &amp; Edwin Boldrey, 1937, &ldquo;Somatic Motor
and Sensory Representation in the Cerebral Cortex of Man as Studied by
Electrical Stimulation,&rdquo; <em>Brain</em>, 60 (4): 389&ndash;443.
doi:10.1093/brain/60.4.389</li>

<li>Penfield, Wilder, &amp; Phanor Perot, 1963, &ldquo;The
Brain&rsquo;s Record of Auditory and Visual Experience: A Final
Summary and Discussion,&rdquo; <em>Brain</em>, 86 (4): 595&ndash;696.
doi:10.1093/brain/86.4.595</li>

<li>Peng, Yueqing, Sarah Gillis-Smith, Hao Jin, Dimitri Tr&auml;nkner,
Nicholas J. P. Ryba, &amp; Charles S. Zuker, 2015, &ldquo;Sweet and
Bitter Taste in the Brain of Awake Behaving Animals,&rdquo;
<em>Nature</em>, 527 (7579): 512&ndash;15.
doi:10.1038/nature15763</li>

<li>Persaud, Navindra, Peter McLeod, &amp; Alan Cowey, 2007,
&ldquo;Post-Decision Wagering Objectively Measures Awareness,&rdquo;
<em>Nature Neuroscience</em>, 10 (2): 257&ndash;61.
doi:10.1038/nn1840</li>

<li>Pessoa, Luiz, 2022, <em>The Entangled Brain: How Perception,
Cognition, and Emotion Are Woven Together</em>, Cambridge, MA: MIT
Press.</li>

<li>&ndash;&ndash;&ndash;, 2023, &ldquo;The Entangled Brain,&rdquo;
<em>Journal of Cognitive Neuroscience</em>, 35 (3): 349&ndash;60.
doi:10.1162/jocn_a_01908</li>

<li>Peters, Megan A. K., 2022, &ldquo;Towards Characterizing the
Canonical Computations Generating Phenomenal Experience,&rdquo;
<em>Neuroscience &amp; Biobehavioral Reviews</em>, 142: 104903.
doi:10.1016/j.neubiorev.2022.104903</li>

<li>Peters, Megan A. K., &amp; Hakwan Lau, 2015, &ldquo;Human Observers
Have Optimal Introspective Access to Perceptual Processes Even for
Visually Masked Stimuli,&rdquo; <em>eLife</em>, 4: e09651.
doi:10.7554/elife.09651</li>

<li>Phillips, Ian, 2011, &ldquo;Perception and Iconic Memory: What
Sperling Doesn&rsquo;t Show,&rdquo; <em>Mind &amp; Language</em>, 26
(4): 381&ndash;411. doi:10.1111/j.1468-0017.2011.01422.x</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;Consciousness and Criterion:
On Block&rsquo;s Case for Unconscious Seeing,&rdquo; <em>Philosophy
and Phenomenological Research</em>, 93 (2): 419&ndash;51.
doi:10.1111/phpr.12224</li>

<li>&ndash;&ndash;&ndash;, 2021, &ldquo;Blindsight Is Qualitatively
Degraded Conscious Vision,&rdquo; <em>Psychological Review</em>, 3
(128): 558&ndash;84. doi:10.1037/rev0000254</li>

<li>Phillips, Ian, &amp; Jorge Morales, 2020, &ldquo;The Fundamental
Problem with No-Cognition Paradigms,&rdquo; <em>Trends in Cognitive
Sciences</em>, 24 (3): 165&ndash;67.
doi:10.1016/j.tics.2019.11.010</li>

<li>Pisella, Laure, Lauren Sergio, Annabelle Blangero,
H&eacute;lo&iuml;se Torchin, Alain Vighetto, &amp; Yves Rossetti,
2009, &ldquo;Optic Ataxia and the Function of the Dorsal Stream:
Contributions to Perception and Action,&rdquo;
<em>Neuropsychologia</em>, 47 (14): 3033&ndash;44.
doi:10.1016/j.neuropsychologia.2009.06.020</li>

<li>Polonsky, Alex, Randolph Blake, Jochen Braun, &amp; David J.
Heeger, 2000, &ldquo;Neuronal Activity in Human Primary Visual Cortex
Correlates with Perception during Binocular Rivalry,&rdquo; <em>Nature
Neuroscience</em>, 3 (11): 1153&ndash;59. doi:10.1038/80676</li>

<li>Pouget, Alexandre, Peter Dayan, &amp; Richard S. Zemel, 2003,
&ldquo;Inference and Computation with Population Codes,&rdquo;
<em>Neuroscience</em>, 26 (1): 381&ndash;410.
doi:10.1146/annurev.neuro.26.041002.131112</li>

<li>Pouget, Alexandre, Jan Drugowitsch, &amp; Adam Kepecs, 2016,
&ldquo;Confidence and Certainty: Distinct Probabilistic Quantities for
Different Goals,&rdquo; <em>Nature Neuroscience</em>, 19 (3):
366&ndash;74. doi:10.1038/nn.4240</li>

<li>Prinz, Jesse, 2012, <em>The Conscious Brain</em>, New York: Oxford
University Press.</li>

<li>Pulverm&uuml;ller, Friedemann, 2005, &ldquo;Brain Mechanisms
Linking Language and Action,&rdquo; <em>Nature Reviews
Neuroscience</em>, 6 (7): 576&ndash;82. doi:10.1038/nrn1706</li>

<li>Raccah, Omri, Ned Block, &amp; Kieran C. R. Fox, 2021, &ldquo;Does
the Prefrontal Cortex Play an Essential Role in Consciousness?
Insights from Intracranial Electrical Stimulation of the Human
Brain,&rdquo; <em>The Journal of Neuroscience</em>, 41 (10):
2076&ndash;87. doi:10.1523/jneurosci.1141-20.2020</li>

<li>Rams&oslash;y, Thomas Zo&euml;ga, &amp; Morten Overgaard, 2004,
&ldquo;Introspection and Subliminal Perception,&rdquo;
<em>Phenomenology and the Cognitive Sciences</em>, 3 (1): 1&ndash;23.
doi:10.1023/b:phen.0000041900.30172.e8</li>

<li>Rausch, Manuel, &amp; Michael Zehetleitner, 2016,
&ldquo;Visibility Is Not Equivalent to Confidence in a Low Contrast
Orientation Discrimination Task,&rdquo; <em>Frontiers in
Psychology</em>, 7 (e1004519): 47. doi:10.1093/brain/121.1.25</li>

<li>Rescorla, M., 2015, &ldquo;Bayesian Perceptual Psychology,&rdquo;
in <em>The Oxford Handbook of Philosophy of Perception</em>, edited by
Mohan Matthen, 694&ndash;716. Oxford: Oxford University Press.</li>

<li>Romo, Ranulfo, Adri&aacute;n Hern&aacute;ndez, An&oacute;tonio
Zainos, &amp; Emilio Salinas, 1998, &ldquo;Somatosensory
Discrimination Based on Cortical Microstimulation,&rdquo;
<em>Nature</em>, 392 (6674): 387&ndash;90. doi:10.1038/32891</li>

<li>Romo, Ranulfo, Adri&aacute;n Hern&aacute;ndez, Antonio Zainos,
Carlos D Brody, &amp; Luis Lemus, 2000, &ldquo;Sensing without
Touching Psychophysical Performance Based on Cortical
Microstimulation,&rdquo; <em>Neuron</em>, 26 (1): 273&ndash;78.
doi:10.1016/s0896-6273(00)81156-3</li>

<li>Rosenthal, David, 2002, &ldquo;Explaining Consciousness,&rdquo; in
<em>Philosophy of Mind: Classical and Contemporary Readings</em>,
edited by David J. Chalmers, 109&ndash;31. Oxford: Oxford University
Press.</li>

<li>&ndash;&ndash;&ndash;, 2005, <em>Consciousness and Mind</em>, New
York: Oxford University Press.</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;Exaggerated Reports: Reply to
Block,&rdquo; <em>Analysis</em>, 71 (3): 431&ndash;37.
doi:10.1093/analys/anr039</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;Consciousness and
Confidence,&rdquo; <em>Neuropsychologia</em>, 128: 255&ndash;65.
doi:10.1016/j.neuropsychologia.2018.01.018</li>

<li>Rossetti, Yves, Laure Pisella, &amp; Alain Vighetto, 2003,
&ldquo;Optic Ataxia Revisited: Visually Guided Action versus Immediate
Visuomotor Control,&rdquo; <em>Experimental Brain Research</em>, 153
(2): 171&ndash;79. doi:10.1007/s00221-003-1590-6</li>

<li>Rounis, Elisabeth, Brian Maniscalco, John C. Rothwell, Richard E.
Passingham, &amp; Hakwan Lau, 2010, &ldquo;Theta-Burst Transcranial
Magnetic Stimulation to the Prefrontal Cortex Impairs Metacognitive
Visual Awareness,&rdquo; <em>Cognitive Neuroscience</em>, 1 (3):
165&ndash;75. doi:10.1080/17588921003632529</li>

<li>Rouy, Martin, Vincent de Gardelle, Gabriel Reyes,
J&eacute;r&ocirc;me Sackur, Jean Christophe Vergnaud, Elisa Filevich,
&amp; Nathan Faivre, 2022, &ldquo;Metacognitive Improvement:
Disentangling Adaptive Training From Experimental Confounds,&rdquo;
<em>Journal of Experimental Psychology: General</em>, 151 (9):
2083&ndash;2091. doi:10.1037/xge0001185</li>

<li>Ruff, Douglas A., &amp; Marlene R. Cohen, 2014, &ldquo;Relating the
Activity of Sensory Neurons to Perception,&rdquo; in <em>The Cognitive
Neurosciences</em>, edited by Michael S. Gazzaniga &amp; George R
Mangun, 5th ed., 349&ndash;62. Cambridge, MA: MIT Press.</li>

<li>Salzman, C. Daniel, Kenneth H. Britten, &amp; William T. Newsome,
1990, &ldquo;Cortical Microstimulation Influences Perceptual
Judgements of Motion Direction,&rdquo; <em>Nature</em>, 346 (6280):
174&ndash;77. doi:10.1038/346174a0</li>

<li>Sandberg, Kristian, Bert Timmermans, Morten Overgaard, &amp; Axel
Cleeremans, 2010, &ldquo;Measuring Consciousness: Is One Measure
Better than the Other?&rdquo; <em>Consciousness and Cognition</em>, 19
(4): 1069&ndash;78. doi:10.1016/j.concog.2009.12.013</li>

<li>Schalk, Gerwin, Christoph Kapeller, Christoph Guger, Hiroshi
Ogawa, Satoru Hiroshima, Rosa Lafer-Sousa, Zeynep M. Saygin, Kyousuke
Kamada, &amp; Nancy Kanwisher, 2017, &ldquo;Facephenes and Rainbows:
Causal Evidence for Functional and Anatomical Specificity of Face and
Color Processing in the Human Brain,&rdquo; <em>Proceedings of the
National Academy of Sciences</em>, 114 (46): 12285&ndash;90.
doi:10.1073/pnas.1713447114</li>

<li>Schenk, Thomas, &amp; Robert D. McIntosh, 2010, &ldquo;Do We Have
Independent Visual Streams for Perception and Action?&rdquo;
<em>Cognitive Neuroscience</em>, 1 (1): 52&ndash;62.
doi:10.1080/17588920903388950</li>

<li>Schwitzgebel, Eric, 2008, &ldquo;The Unreliability of Naive
Introspection,&rdquo; <em>The Philosophical Review</em>, 117 (2):
245&ndash;73. doi:10.1215/00318108-2007-037</li>

<li>&ndash;&ndash;&ndash;, 2011, <em>Perplexities of
Consciousness</em>, Cambridge, MA: MIT Press.</li>

<li>Shea, Nicholas, 2014, &ldquo;Neural Signaling of Probabilistic
Vectors,&rdquo; <em>Philosophy of Science</em>, 81 (5): 902&ndash;13.
doi:10.1086/678354</li>

<li>&ndash;&ndash;&ndash;, 2018, <em>Representation in Cognitive
Science</em>, Oxford: Oxford University Press.</li>

<li>Shea, Nicholas, &amp; Tim Bayne, 2010, &ldquo;The Vegetative State
and the Science of Consciousness,&rdquo; <em>The British Journal for
the Philosophy of Science</em>, 61 (3): 459&ndash;84.
doi:10.1093/bjps/axp046</li>

<li>Shekhar, Medha, &amp; Dobromir Rahnev, 2018, &ldquo;Distinguishing
the Roles of Dorsolateral and Anterior PFC in Visual
Metacognition,&rdquo; <em>The Journal of Neuroscience</em>, 38 (22):
5078&ndash;87. doi:10.1523/jneurosci.3484-17.2018</li>

<li>Siegel, Susanna, 2022, &ldquo;How Can Perceptual Experiences
Explain Uncertainty?&rdquo; <em>Mind &amp; Language</em>, 37 (2):
134&ndash;58. doi:10.1111/mila.12348</li>

<li>Singer, Emily, 2006, &ldquo;Big Brain Thinking,&rdquo; <em>MIT
Technology Review</em>, February 10, 2006.
 [<a href="https://www.technologyreview.com/s/405296/big-brain-thinking/" target="other">Singer 2006 available online</a>]</li>
 
<li>Sitt, Jacobo D., Jean-R&eacute;mi King, Lionel Naccache, &amp;
Stanislas Dehaene, 2013, &ldquo;Ripples of Consciousness,&rdquo;
<em>Trends in Cognitive Sciences</em>, 17 (11): 552&ndash;54.
doi:10.1016/j.tics.2013.09.003</li>

<li>Smeets, Jeroen B. J., &amp; Eli Brenner, 2006, &ldquo;10 Years of
Illusions,&rdquo; <em>Journal of Experimental Psychology: Human
Perception and Performance</em>, 32 (6): 1501&ndash;4.
doi:10.1037/0096-1523.32.6.1501</li>

<li>Smithies, Declan, &amp; Daniel Stoljar (eds.), 2012,
<em>Introspection and Consciousness</em>, Oxford: Oxford University
Press.  doi:10.1093/acprof:oso/9780199744794.001.0001</li>

<li>Spener, Maja, 2015, &ldquo;Calibrating Introspection,&rdquo;
<em>Philosophical Issues</em>, 25 (1): 300&ndash;321.
doi:10.1111/phis.12062</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;Introspecting in the 20th
Century,&rdquo; in <em>Philosophy of Mind in the Twentieth and
Twenty-First Centuries</em>, edited by Amy Kind, 148&ndash;74. London:
Routledge.</li>

<li>Sperling, George, 1960, &ldquo;The Information Available in Brief
Visual Presentations,&rdquo; <em>Psychological Monographs: General and
Applied</em>, 74 (11): 1&ndash;29. doi:10.1037/h0093759</li>

<li>Stoerig, Petra, Martin H&uuml;bner, &amp; Ernst P&ouml;ppel, 1985,
&ldquo;Signal Detection Analysis of Residual Vision in a Field Defect
Due to a Post-Geniculate Lesion,&rdquo; <em>Neuropsychologia</em>, 23
(5): 589&ndash;99. doi:10.1016/0028-3932(85)90061-2</li>

<li>Tanner, Wilson P., &amp; John A. Swets, 1954, &ldquo;A
Decision-Making Theory of Visual Detection,&rdquo; <em>Psychological
Review</em>, 61 (6): 401&ndash;9. doi:10.1037/h0058700</li>

<li>Tarr, Michael J., &amp; Isabel Gauthier, 2000, &ldquo;FFA: A
Flexible Fusiform Area for Subordinate-Level Visual Processing
Automatized by Expertise,&rdquo; <em>Nature Neuroscience</em>, 3 (8):
764&ndash;69. doi:10.1038/77666</li>

<li>Tong, Frank, Ming Meng, &amp; Randolph Blake, 2006, &ldquo;Neural
Bases of Binocular Rivalry,&rdquo; <em>Trends in Cognitive
Sciences</em>10 (11): 502&ndash;11.
doi:10.1016/j.tics.2006.09.003</li>

<li>Tononi, Giulio, 2004, &ldquo;An Information Integration Theory of
Consciousness,&rdquo; <em>BMC Neuroscience</em>, 5 (42).
doi:10.1186/1471-2202-5-42</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;Consciousness as Integrated
Information: A Provisional Manifesto,&rdquo; <em>The Biological
Bulletin</em>, 215 (3): 216&ndash;42. doi:10.2307/25470707</li>

<li>Tononi, Giulio, Melanie Boly, Marcello Massimini, &amp; Christof
Koch, 2016, &ldquo;Integrated Information Theory: From Consciousness
to Its Physical Substrate,&rdquo; <em>Nature Reviews
Neuroscience</em>, 17 (7): 450&ndash;61. doi:10.1038/nrn.2016.44</li>

<li>Tse, P. U., S. Martinez-Conde, A. A. Schlegel, &amp;
S. L. Macknik, 2005, &ldquo;Visibility, Visual Awareness, and Visual
Masking of Simple Unattended Targets Are Confined to Areas in the
Occipital Cortex beyond Human V1/V2,&rdquo; <em>Proceedings of the
National Academy of Sciences of the United States of America</em>, 102
(47): 17178&ndash;83. doi:10.1073/pnas.0508010102</li>

<li>Tye, Michael, 1992, &ldquo;Visual Qualia and Visual
Content,&rdquo; n <em>The Contents of Experience</em>, edited by Tim
Crane, 158&ndash;76.</li>

<li>Ungerleider, Leslie, &amp; Mortimer Mishkin, 1982, &ldquo;Two
Cortical Systems,&rdquo; in <em>Analysis of Visual Behavior</em>,
edited by David J. Ingle, Melvyn A. Goodale, &amp; Richard J.W.
Mansfield, 549&ndash;586. Cambridge, MA: MIT Press.</li>

<li>Urbanski, Marika, Olivier A. Coubard, &amp; Cl&eacute;mence
Bourlon, 2014, &ldquo;Visualizing the Blind Brain: Brain Imaging of
Visual Field Defects from Early Recovery to Rehabilitation
Techniques,&rdquo; <em>Frontiers in Integrative Neuroscience</em>, 8:
74. doi:10.3389/fnint.2014.00074</li>

<li>Vance, Jonna, 2021, &ldquo;Precision and Perceptual
Clarity,&rdquo; <em>Australasian Journal of Philosophy</em>, 99 (2):
379&ndash;95. doi:10.1080/00048402.2020.1767663</li>

<li>Vignal, J.P., P. Chauvel, &amp; E. Halgren, 2000, &ldquo;Localised
Face Processing by the Human Prefrontal Cortex: Stimulation-Evoked
Hallucinations of Faces,&rdquo; <em>Cognitive Neuropsychology</em>, 17
(1&ndash;3): 281&ndash;91. doi:10.1080/026432900380616</li>

<li>Vignemont, F. de, &amp; P. Fourneret, 2004, &ldquo;The Sense of
Agency: A Philosophical and Empirical Review of the &lsquo;Who&rsquo;
System,&rdquo; <em>Consciousness and Cognition</em>, 13 (1):
1&ndash;19. doi:10.1016/s1053-8100(03)00022-9</li>

<li>Wallhagen, Morgan, 2007, &ldquo;Consciousness and Action: Does
Cognitive Science Support (Mild) Epiphenomenalism?&rdquo; <em>The
British Journal for the Philosophy of Science</em>, 58 (3):
539&ndash;61. doi:10.1093/bjps/axm023</li>

<li>Weiskrantz, Lawrence, 1986, <em>Blindsight: A Case Study and
Implications</em>, Oxford: Oxford University Press.</li>

<li>Westlin, Christiana, Jordan E. Theriault, Yuta Katsumi, Alfonso
Nieto-Castanon, Aaron Kucyi, Sebastian F. Ruf, Sarah M. Brown, et al.,
2023, &ldquo;Improving the Study of Brain-Behavior Relationships by
Revisiting Basic Assumptions,&rdquo; <em>Trends in Cognitive
Sciences</em>, 27 (3): 246&ndash;257.
doi:10.1016/j.tics.2022.12.015</li>

<li>Wilson, Hugh R., 2003, &ldquo;Computational Evidence for a Rivalry
Hierarchy in Vision,&rdquo; <em>Proceedings of the National Academy of
Sciences</em>, 100 (24): 14499&ndash;503.
doi:10.1073/pnas.2333622100</li>

<li>Working Party of the Royal College of Physicians, 2003, &ldquo;The
Vegetative State: Guidance on Diagnosis and Management,&rdquo;
<em>Clinical Medicine</em>, 3 (3): 249&ndash;54.
doi:10.7861/clinmedicine.3-3-249</li>

<li>Wu, Wayne, 2013, &ldquo;The Case for Zombie Agency,&rdquo;
<em>Mind</em>, 122 (485): 217&ndash;30. doi:10.1093/mind/fzt030</li>

<li>&ndash;&ndash;&ndash;, 2014a, &ldquo;Against Division:
Consciousness, Information and the Visual Streams,&rdquo; <em>Mind
&amp; Language</em>, 29 (4): 383&ndash;406.
doi:10.1111/mila.12056</li>

<li>&ndash;&ndash;&ndash;, 2014b, <em>Attention</em>, London:
Routledge</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;Attention and Perception: A
Necessary Connection?&rdquo; In <em>Current Controversies in
Philosophy of Perception</em>, edited by Bence Nanay, 148&ndash;62.
Routledge: New York</li>

<li>&ndash;&ndash;&ndash;, 2023, <em>Movements of the Mind: A Theory
of Attention, Intention and Action</em>, Oxford: Oxford University
Press.</li>

<li>Young, Michael J., Yelena G. Bodien, Joseph T. Giacino, Joseph J.
Fins, Robert D. Truog, Leigh R. Hochberg, &amp; Brian L Edlow, 2021,
&ldquo;The Neuroethics of Disorders of Consciousness: A Brief History
of Evolving Ideas,&rdquo; <em>Brain</em>, 144 (11): 3291&ndash;3310.
doi:10.1093/brain/awab290</li>

<li>Zehetleitner, Michael, &amp; Manuel Rausch, 2013, &ldquo;Being
Confident without Seeing: What Subjective Measures of Visual
Consciousness Are About,&rdquo; <em>Attention, Perception, &amp;
Psychophysics</em>, 75 (7): 1406&ndash;26.
doi:10.3758/s13414-013-0505-2</li>

<li>Zihl, J., D. von Cramon, &amp; N. Mai, 1983, &ldquo;Selective
Disturbance of Movement Vision after Bilateral Brain Damage,&rdquo;
<em>Brain</em>, 106 (2): 313&ndash;40.
doi:10.1093/brain/106.2.313</li>

<li>Zou, Jinyou, Sheng He, &amp; Peng Zhang, 2016, &ldquo;Binocular
Rivalry from Invisible Patterns,&rdquo; <em>Proceedings of the
National Academy of Sciences of the United States of America</em>, 113
(30): 8408&ndash;13. doi:10.1073/pnas.1604816113</li>
</ul>
</div> 

<div id="academic-tools">
<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=consciousness-neuroscience" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/consciousness-neuroscience/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=consciousness-neuroscience&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="https://philpapers.org/sep/consciousness-neuroscience/" target="other">Enhanced bibliography for this entry</a>
at <a href="https://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>
</div>

<div id="other-internet-resources">

<h2 id="Oth">Other Internet Resources</h2>

<h3>Manuscripts and Letters</h3>

<ul>

<li>IIT-Concerned, Stephen M. Fleming, Chris D. Frith, Mel Goodale,
Hakwan Lau, Joseph E. LeDoux, Alan L. F. Lee, et al., 2023, 
&ldquo;<a href="https://doi.org/10.31234/osf.io/zsr78" target="other">The Integrated Information Theory of Consciousness as Pseudoscience</a>,&rdquo; <em>PsyArXiv</em>.</li>

<li>Tononi, Giulio, Larissa Albantakis, Melanie Boly, Chiara Cirelli,
&amp; Christof Koch, 2022, 
&ldquo;<a href="https://doi.org/10.48550/arxiv.2206.02069" target="other">Only What Exists Can Cause: An
Intrinsic View of Free Will</a>,&rdquo; <em>ArXiv</em>.</li>

</ul>

<h3>Movies</h3>

<ul>

<li  id="S1"><a href="https://movie-usa.glencoesoftware.com/video/10.1073/pnas.1604126113/video-1" target="other">Movie S1</a>,
 from Hirshorn et al. 2016: electrical stimulation session with P2.
This movie shows P2&rsquo;s word-naming ability completely disrupted
during high stimulation, but no errors during low stimulation.</li>

<li  id="S2"><a href="https://movie-usa.glencoesoftware.com/video/10.1073/pnas.1604126113/video-2" target="other">Movie S2</a>,
 from Hirshorn et al. 2016: electrical stimulation session with P1.
This movie shows P1 misnaming letters under high stimulation, but no
errors during low stimulation.</li>

<li  id="S3"><a href="https://www.jneurosci.org/content/32/43/14915#media-1" target="other">Movie S3</a>,
 from Parvizi et al. 2013: electrical stimulation session. This movie
shows a patient experiencing distorted faces but no distortion of
other objects or during sham stimulation.</li>

</ul>

<h3>Blog Posts</h3>

<ul>

<li>Aaronson, Scott, 2014a, 
&ldquo;<a href="https://www.scottaaronson.com/blog/?p=1799" target="other">Why I Am Not An Integrated Information Theorist (or, The Unconscious Expander)</a>&rdquo;,
<em>Shtetl-Optimized</em> (blog), May 21; accessed May 21, 2016.</li>

<li>&ndash;&ndash;&ndash;, 2014b, 
&ldquo;<a href="https://www.scottaaronson.com/blog/?p=1823" target="other">Giulio Tononi and Me: A Phi-Nal Exchange</a>&rdquo;, <em>Shtetl-Optimized</em> (blog), May 30; accessed May 21, 2018.</li>

<li>De Brigard, Felipe, 2023, 
&ldquo;<a href="https://felipedebrigard.substack.com/p/a-pseudo-rose-by-any-another-name" target="other">A Pseudo-Rose by Any Another Name</a>,&rdquo; <em>Substack</em> (blog), November 9;
accessed Jan 15, 2024.</li>

<li>Lau, Hakwan, 2017, 
&ldquo;<a href="https://inconsciousnesswetrust.blogspot.com/2017/08/how-to-make-iit-and-other-theories-of.html" target="other">How to Make IIT (&amp; Other Theories of Consciousness) More Respectable</a>&rdquo;, 
  <em>In Consciousness We Trust</em> (blog), August 10; 
accessed April 3, 2024.</li>
 
<li>Fleming, Stephen M, 2023b, 
&ldquo;<a href="https://elusiveself.wordpress.com/2023/09/09/iit-vs-gnwt-and-the-meaning-of-evidence-in-consciousness-science/" target="other">IIT vs. GNWT and the Meaning of Evidence in Consciousness Science</a>,&rdquo;
 <em>The Elusive Self</em> (blog), September; accessed Jan 15, 2024.</li>

<li>Tononi, Gulio, 2014, 
 &ldquo;<a href="https://www.scottaaronson.com/tononi.docx" target="other">Why Scott Should Stare at a Blank Wall and Reconsider (or, the Conscious Grid)</a>,&rdquo; 
 in <em>Shtetl-Optimized</em> (blog), May; accessed Jan 15, 2024.</li>

</ul>

</div>

<div id="related-entries">

<h2 id="Rel">Related Entries</h2>

<p>

 <a href="../agency/">agency</a> |
 <a href="../consciousness-higher/">consciousness: higher-order theories</a> |
 <a href="../consciousness-representational/">consciousness: representational theories of</a> |
 <a href="../consciousness-unity/">consciousness: unity of</a> |
 <a href="../grounding/">grounding, metaphysical</a> |
 <a href="../information/">information</a> |
 <a href="../mental-representation/">mental representation</a> |
 <a href="../models-science/">models in science</a> |
 <a href="../qualia/">qualia</a> |
 <a href="../zombies/">zombies</a>
</p>
</div>

<div id="acknowledgments">

<h3>Acknowledgments</h3>

<p>
Thanks to Hakwan Lau and Susanna Siegel and especially Dave Chalmers
who refereed the article. Special thanks to Mark Sprevak and David
Barak for organizing discussion groups on the entry at the University
of Edinburgh and at Columbia University respectively and for their
feedback. Thanks to Doug Ruff for extensive feedback on central
sections. Among many others, thanks for comments to: Jake Berger, Ned
Block, Richard Brown, Alessandra Buccella, Denis Buehler, Tony Cheng,
Mazviita Chirimuuta, Andy Clark, Sam Clarke, Carrie Figdor, Cressida
Gaukroger, Michelle Liu, Chris Mole, John Morrison, Will Nalls, David
Papineau, David Rosenthal, Ian Phillips, Adina Roskies, Forrest
Schreick, Nick Shea, and Cecily Whiteley.</p>
</div>
<script type="text/javascript" src="local.js"></script>
<script type="text/javascript" src="../../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2024</a> by

<br />
Wayne Wu
&lt;<a href="m&#97;ilto:waynewu&#37;40andrew&#37;2ecmu&#37;2eedu"><em>waynewu<abbr title=" at ">&#64;</abbr>andrew<abbr title=" dot ">&#46;</abbr>cmu<abbr title=" dot ">&#46;</abbr>edu</em></a>&gt;<br />
<a href="http://cssh.northeastern.edu/faculty/jorge-morales/" target="other">Jorge Morales</a>
&lt;<a href="m&#97;ilto:j&#37;2emorales&#37;40northeastern&#37;2eedu"><em>j<abbr title=" dot ">&#46;</abbr>morales<abbr title=" at ">&#64;</abbr>northeastern<abbr title=" dot ">&#46;</abbr>edu</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../contents.html">Table of Contents</a></li>
            <li role="menuitem"><a href="../../new.html">What's New</a></li>
            <li role="menuitem"><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li role="menuitem"><a href="../../published.html">Chronological</a></li>
            <li role="menuitem"><a href="../../archives/">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../info.html">Editorial Information</a></li>
            <li role="menuitem"><a href="../../about.html">About the SEP</a></li>
            <li role="menuitem"><a href="../../board.html">Editorial Board</a></li>
            <li role="menuitem"><a href="../../cite.html">How to Cite the SEP</a></li>
            <li role="menuitem"><a href="../../special-characters.html">Special Characters</a></li>
            <li role="menuitem"><a href="../../tools/">Advanced Tools</a></li>
            <li role="menuitem"><a href="../../accessibility.html">Accessibility</a></li>
            <li role="menuitem"><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li role="menuitem"><a href="../../support/">Support the SEP</a></li>
            <li role="menuitem"><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li role="menuitem"><a href="../../support/donate.html">Make a Donation</a></li>
            <li role="menuitem"><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2024</a> by <a href="https://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>
</html>
